[
  {
    "objectID": "walkthrough.html",
    "href": "walkthrough.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/\nLets see how this works:(Rush 1998) as"
  },
  {
    "objectID": "walkthrough.html#markdown",
    "href": "walkthrough.html#markdown",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Markdown is an easy to read and write text format:\n\nIt’s plain text so works well with version control\nIt can be rendered into HTML, PDF, and more\nLearn more at: https://quarto.org/docs/authoring/\nLets see how this works:(Rush 1998) as"
  },
  {
    "objectID": "walkthrough.html#code-cell",
    "href": "walkthrough.html#code-cell",
    "title": "Hello, Quarto",
    "section": "Code Cell",
    "text": "Code Cell\nHere is a Python code cell:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0,0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "walkthrough.html#equation",
    "href": "walkthrough.html#equation",
    "title": "Hello, Quarto",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "",
    "text": "This project serves as an initial exploration of a systematic trading strategy tailored to portfolio performance across selected technology stocks. The strategy aims to achieve higher risk-adjusted returns while effectively managing downside risks, contributing to the development of more advanced, automated trading frameworks. Financial analysis metrics such as total return, Sharpe Ratio, Sortino Ratio, and win/loss dynamics are utilized to evaluate the strategy’s performance. The primary goal is to fetch historical stock data, apply technical indicators, and train a machine learning model to predict stock price movements. The strategy is then backtested to evaluate its performance."
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#overview",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#overview",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "",
    "text": "This project serves as an initial exploration of a systematic trading strategy tailored to portfolio performance across selected technology stocks. The strategy aims to achieve higher risk-adjusted returns while effectively managing downside risks, contributing to the development of more advanced, automated trading frameworks. Financial analysis metrics such as total return, Sharpe Ratio, Sortino Ratio, and win/loss dynamics are utilized to evaluate the strategy’s performance. The primary goal is to fetch historical stock data, apply technical indicators, and train a machine learning model to predict stock price movements. The strategy is then backtested to evaluate its performance."
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#key-components",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#key-components",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Key Components",
    "text": "Key Components\n\n1. Data Fetching and Cleaning\nThe fetch_data function retrieves historical stock data using the yfinance library. It cleans the data by ensuring the necessary columns are present and handling any missing values.\nThe clean_data function processes the DataFrame to ensure it has the required columns (Open, High, Low, Close, Volume). If any columns are missing, they are filled with the Close price.\n\n\n2. Technical Indicators\nThe add_indicators function calculates Exponential Moving Averages (EMAs) for the stock prices, which are used as features for the machine learning model.\n\n\n3. Machine Learning Model\nThe train_ml_model function trains a Random Forest Classifier to predict whether the stock price will increase or decrease based on the calculated EMAs and lagged returns.\n\n\n4. Backtesting\nThe run_backtest_with_cash function simulates trading based on the model’s predictions, tracking cash balance and portfolio equity over time.\n\n\n5. Performance Metrics\nThe performance of the trading strategy is evaluated using various metrics, including total return, win rate, and Sharpe ratio.\n\\[\n\\text{Sharpe Ratio} = \\frac{\\text{Mean Return}}{\\text{Standard Deviation of Returns}} \\times \\sqrt{252}\n\\]"
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#equations",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#equations",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Equations",
    "text": "Equations\n\nExponential Moving Average (EMA)\nThe Exponential Moving Average (EMA) is calculated as:\n\\[\n\\text{EMA}_n = \\frac{P_t \\times (1 - \\alpha) + \\text{EMA}_{n-1} \\times \\alpha}{1}\n\\] \\[ \\text(smoothing factor)  = \\alpha = \\frac{2}{n + 1}  \\]\nWhere: \\(P_t\\) = Price at time \\(t\\)\n\n\nReturns and Lagged Returns\n\nDaily Return\nThe daily return is calculated as:\n\\[\n\\text{Return}_t = \\frac{\\text{Close}_t - \\text{Close}_{t-1}}{\\text{Close}_{t-1}}\n\\]\n\n\nLagged Return\nThe lagged return is defined as:\n\\[\n\\text{Lagged Return}_t = \\text{Return}_{t-1}\n\\]\n\n\n\nSharpe Ratio\nThe Sharpe ratio is calculated as:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu}{\\sigma} \\times \\sqrt{252}\n\\]\nWhere: - \\(\\mu\\) is the mean of daily returns, - \\(\\sigma\\) is the standard deviation of daily returns, - \\(252\\) is the number of trading days in a year.\n\n\nSortino Ratio\nThe Sortino ratio, which penalizes only downside risk, is calculated as:\n\\[\n\\text{Sortino Ratio} = \\frac{\\mu}{\\sigma_{\\text{downside}}} \\times \\sqrt{252}\n\\]\nWhere: - \\(\\sigma_{\\text{downside}}\\) is the standard deviation of negative returns.\n\n\nPortfolio Backtesting\nThe equity of the portfolio at each step is updated as:\n\\[\n\\text{Equity}_t = \\text{Cash Balance} + (\\text{Shares Held} \\times \\text{Price}_t)\n\\]\n\n\nProject Assumptions and Dependencies\n\nFunds Managed: The strategy manages a hypothetical portfolio with an initial investment of $1,000,000.\nTickers Used: The portfolio consists of the following tickers: AAPL, META, GOOG, AMZN, NFLX.\nDate Range: The backtesting period spans from January 2020 to December 2024.\nAssumptions:\n\nNo transaction costs or slippage are considered.\nAll tickers are traded with equal weight allocation.\nHistorical stock price data obtained from Yahoo Finance for simplicity and availability.\nNo additional macroeconomic factors are included in this version of the model, focusing purely on stock price movements and technical signals."
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#code",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#code",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Code:",
    "text": "Code:\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Debugging function\ndef debug_data(data, ticker):\n    print(f\"\\n--- Debugging Data for {ticker} ---\")\n    print(f\"Shape: {data.shape}\")\n    print(\"Columns:\", data.columns)\n    # print(\"\\nSample Data:\\n\", data.head())\n    print(\"--- End Debugging ---\\n\")\n\n# Function to clean and prepare data\ndef clean_data(data):\n    if isinstance(data.columns, pd.MultiIndex):\n        data.columns = ['_'.join(col).strip() for col in data.columns]\n    data = data.loc[:, ~data.columns.duplicated()]\n    if 'Close' not in data.columns:\n        close_columns = [col for col in data.columns if 'Close' in col]\n        if close_columns:\n            data['Close'] = data[close_columns[0]]\n    if 'Open' not in data.columns:\n        open_columns = [col for col in data.columns if 'Open' in col]\n        if open_columns:\n            data['Open'] = data[open_columns[0]]\n    if 'High' not in data.columns:\n        high_columns = [col for col in data.columns if 'High' in col]\n        if high_columns:\n            data['High'] = data[high_columns[0]]\n    if 'Volume' not in data.columns:\n        vol_columns = [col for col in data.columns if 'Volume' in col]\n        if vol_columns:\n            data['Volume'] = data[vol_columns[0]]\n    if 'Low' not in data.columns:\n        low_columns = [col for col in data.columns if 'Low' in col]\n        if low_columns:\n            data['Low'] = data[low_columns[0]]\n    \n    for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n        if col not in data.columns:\n            print(f\"Warning: '{col}' column missing, filling with 'Close' column.\")\n            data[col] = data['Close']\n    return data\n\n# Add EMAs\ndef add_indicators(data):\n    data['EMA_10'] = data['Close'].ewm(span=10, adjust=False).mean()\n    data['EMA_20'] = data['Close'].ewm(span=20, adjust=False).mean()\n    return data\n\n# Fetch data\ndef fetch_data(ticker, period='5y'):\n    print(f\"Downloading data for {ticker}...\")\n    try:\n        data = yf.download(ticker, period=period, progress=False)\n        if data.empty:\n            raise ValueError(f\"No data available for {ticker}\")\n        data = clean_data(data)\n        # debug_data(data, ticker)\n        data = add_indicators(data)\n        data['Returns'] = data['Close'].pct_change()\n        data['Lagged_Returns'] = data['Returns'].shift(1)\n        data.dropna(inplace=True)\n        return data\n    except Exception as e:\n        print(f\"Error fetching data for {ticker}: {e}\")\n        return pd.DataFrame()\n\n# Train ML model\ndef train_ml_model(data):\n    features = ['EMA_10', 'EMA_20', 'Lagged_Returns']\n    data['Target'] = np.where(data['EMA_10'] &gt; data['EMA_20'], 1, 0)\n\n    X = data[features]\n    y = data['Target']\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    model = RandomForestClassifier(random_state=42)\n    model.fit(X_scaled, y)\n\n    data['Prediction'] = model.predict(X_scaled)\n    return model, data\n\n# Backtesting with cash balance\ndef run_backtest_with_cash(data, initial_cash):\n    cash_balance = initial_cash\n    shares_held = 0\n    cash_history = [initial_cash]\n    equity_history = [initial_cash]\n    buy_dates, sell_dates = [], []\n\n    for i in range(1, len(data)):\n        price = data['Close'].iloc[i]\n        signal = data['Prediction'].iloc[i]\n\n        if signal == 0 and shares_held &gt; 0:  # Sell\n            cash_balance += shares_held * price\n            shares_held = 0\n            sell_dates.append(data.index[i])\n\n        elif signal == 1 and shares_held == 0:  # Buy\n            shares_held = cash_balance // price\n            cash_balance -= shares_held * price\n            buy_dates.append(data.index[i])\n\n        total_equity = cash_balance + (shares_held * price)\n        cash_history.append(cash_balance)\n        equity_history.append(total_equity)\n\n    while len(cash_history) &lt; len(data):\n        cash_history.append(cash_balance)\n        equity_history.append(total_equity)\n\n    data['Cash_Balance'] = cash_history\n    data['Portfolio_Equity'] = equity_history\n    data['Buy_Signals'] = data.index.isin(buy_dates)\n    data['Sell_Signals'] = data.index.isin(sell_dates)\n    # print(\"cash_history\", cash_history)\n    # print(\"equity_history\", equity_history)\n    return data\n\n# Metrics calculation\ndef calculate_metrics(data):\n    total_return = data['Portfolio_Equity'].iloc[-1] - data['Portfolio_Equity'].iloc[0]\n    total_return_pct = (data['Portfolio_Equity'].iloc[-1] / data['Portfolio_Equity'].iloc[0]) - 1\n    returns = data['Portfolio_Equity'].pct_change().dropna()\n    sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)\n    sortino_ratio = returns.mean() / returns[returns &lt; 0].std() * np.sqrt(252)\n    best_trade = returns.max()\n    worst_trade = returns.min()\n    win_rate = len(returns[returns &gt; 0]) / len(returns)\n    loss_rate = 1 - win_rate\n\n    metrics = {\n        \"Total return\": total_return,\n        \"Total Return(%)\": total_return_pct,\n        \"Win Rate\": win_rate,\n        \"Loss Rate\": loss_rate,\n        \"Best Trade\": best_trade,\n        \"Worst Trade\": worst_trade,\n        \"Sharpe Ratio\": sharpe_ratio,\n        \"Sortino Ratio\": sortino_ratio\n    }\n    return metrics\n\n# Metrics calculation for combined portfolio\ndef calculate_combined_metrics(pnl_history):\n    total_return= (pnl_history.iloc[-1] - pnl_history.iloc[0])\n    total_return_pct = (pnl_history.iloc[-1] / pnl_history.iloc[0]) - 1\n    returns = pnl_history.pct_change().dropna()\n    sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)\n    sortino_ratio = returns.mean() / returns[returns &lt; 0].std() * np.sqrt(252)\n    best_trade = returns.max()\n    worst_trade = returns.min()\n    win_return = returns[returns &gt; 0]\n    loss_return = returns[returns &lt; 0]\n    win_rate = len(returns[returns &gt; 0]) / len(returns)\n    loss_rate = 1 - win_rate\n\n    metrics = {\n        \"Total Return\": total_return,\n        \"Total Return(%)\": total_return_pct,\n        \"Win Rate\": win_rate,\n        \"Loss Rate\": loss_rate,\n        \"Best Trade\": best_trade,\n        \"Worst Trade\": worst_trade,\n        \"Sharpe Ratio\": sharpe_ratio,\n        \"Sortino Ratio\": sortino_ratio        \n    }\n    return metrics\n\n# Print combined portfolio metrics\ndef print_combined_metrics(metrics):\n    print(\"\\n--- Combined Portfolio Performance Metrics ---\")\n    for key, value in metrics.items():\n            if isinstance(value, (int, float)):\n                print(f\"{key}: {value:.4f}\")\n            else:\n                print(f\"{key}: {value}\")\n    print(\"----------------------------------------------\\n\")\n\n# Plot individual performance\ndef plot_performance(data, ticker):\n    plt.figure(figsize=(12, 6))\n    plt.plot(data.index, data['Portfolio_Equity'], label='Portfolio Equity', color='purple')\n    plt.scatter(data.index[data['Buy_Signals']], data['Portfolio_Equity'][data['Buy_Signals']],\n                color='green', marker='^', label='Buy Signal')\n    plt.scatter(data.index[data['Sell_Signals']], data['Portfolio_Equity'][data['Sell_Signals']],\n                color='red', marker='v', label='Sell Signal')\n    plt.title(f\"{ticker} Portfolio Equity with Buy/Sell Signals\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Equity\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Plot combined portfolio performance\ndef plot_combined_portfolio(pnl_history):\n    plt.figure(figsize=(12, 6))\n    plt.plot(pnl_history.index, pnl_history.values, label=\"Combined Portfolio PnL\", color=\"blue\")\n    plt.title(\"Combined Portfolio Performance\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Portfolio Value\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n# Main Execution\nif __name__ == \"__main__\":\n    portfolio = [\"AAPl\", \"META\",\"GOOG\", \"AMZN\", \"NFLX\"]\n    print(\"Enter stock tickers for your portfolio (max 5 tickers). Type 'done' to finish.\")\n#     while len(portfolio) &lt; 5:\n#         ticker = input(f\"Enter ticker {len(portfolio) + 1}: \").strip().upper()\n#         if ticker == 'DONE':\n#             break\n#         portfolio.append(ticker)\n\n    total_cash = 100000\n    cash_per_stock = total_cash / len(portfolio)\n    combined_pnl = None\n\n    for ticker in portfolio:\n        print(f\"\\nProcessing {ticker}...\")\n        data = fetch_data(ticker)\n        if not data.empty:\n            _, data = train_ml_model(data)\n            data = run_backtest_with_cash(data, cash_per_stock)\n            metrics = calculate_metrics(data)\n            print(f\"\\nPerformance Metrics for {ticker}:\")\n            for key, value in metrics.items():\n                print(f\"{key}: {value:.4f}\")\n            plot_performance(data, ticker)\n\n            if combined_pnl is None:\n                combined_pnl = data['Portfolio_Equity']\n            else:\n                combined_pnl = combined_pnl.add(data['Portfolio_Equity'], fill_value=0)\n\n    if combined_pnl is not None:\n        combined_pnl.index = data.index  # Align combined PnL index\n        combined_metrics = calculate_combined_metrics(combined_pnl)\n        print_combined_metrics(combined_metrics)\n        plot_combined_portfolio(combined_pnl)\n        \n    else:\n        print(\"No valid portfolio data to display.\")\n    # display(data)\n\nEnter stock tickers for your portfolio (max 5 tickers). Type 'done' to finish.\n\nProcessing AAPl...\nDownloading data for AAPl...\n\n\nC:\\Temp\\ipykernel_27288\\3828830366.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for AAPl:\nTotal return: 5330.6851\nTotal Return(%): 0.2665\nWin Rate: 0.2955\nLoss Rate: 0.7045\nBest Trade: 0.0754\nWorst Trade: -0.0798\nSharpe Ratio: 0.3500\nSortino Ratio: 0.3878\n\n\n\n\n\n\n\n\n\n\nProcessing META...\nDownloading data for META...\n\n\nC:\\Temp\\ipykernel_27288\\3828830366.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for META:\nTotal return: 26394.5467\nTotal Return(%): 1.3197\nWin Rate: 0.3147\nLoss Rate: 0.6853\nBest Trade: 0.2320\nWorst Trade: -0.0759\nSharpe Ratio: 0.7388\nSortino Ratio: 1.1044\n\n\n\n\n\n\n\n\n\n\nProcessing GOOG...\nDownloading data for GOOG...\n\n\nC:\\Temp\\ipykernel_27288\\3828830366.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for GOOG:\nTotal return: 13121.3300\nTotal Return(%): 0.6561\nWin Rate: 0.3466\nLoss Rate: 0.6534\nBest Trade: 0.0996\nWorst Trade: -0.0962\nSharpe Ratio: 0.5552\nSortino Ratio: 0.5886\n\n\n\n\n\n\n\n\n\n\nProcessing AMZN...\nDownloading data for AMZN...\n\n\nC:\\Temp\\ipykernel_27288\\3828830366.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for AMZN:\nTotal return: 5459.7505\nTotal Return(%): 0.2730\nWin Rate: 0.2891\nLoss Rate: 0.7109\nBest Trade: 0.1033\nWorst Trade: -0.0838\nSharpe Ratio: 0.3277\nSortino Ratio: 0.3834\n\n\n\n\n\n\n\n\n\n\nProcessing NFLX...\nDownloading data for NFLX...\n\n\nC:\\Temp\\ipykernel_27288\\3828830366.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for NFLX:\nTotal return: 21980.0953\nTotal Return(%): 1.0990\nWin Rate: 0.3131\nLoss Rate: 0.6869\nBest Trade: 0.1289\nWorst Trade: -0.0901\nSharpe Ratio: 0.6872\nSortino Ratio: 0.8079\n\n\n\n\n\n\n\n\n\n\n--- Combined Portfolio Performance Metrics ---\nTotal Return: 72286.4076\nTotal Return(%): 0.7229\nWin Rate: 0.4768\nLoss Rate: 0.5232\nBest Trade: 0.0814\nWorst Trade: -0.0524\nSharpe Ratio: 0.7531\nSortino Ratio: 1.0279\n----------------------------------------------"
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#portfolio-performance-evaluation",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#portfolio-performance-evaluation",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Portfolio Performance Evaluation",
    "text": "Portfolio Performance Evaluation\n\nKey Findings\n\nPortfolio Returns\n\nIndividual returns range from 31.01% (META) to 75.16% (NFLX), with a portfolio return of 44.90% over the backtesting period.\nThe diverse composition (AAPL, META, GOOG, AMZN, NFLX) ensures balanced exposure to high-growth tech sectors.\n\n\n\nRisk-Adjusted Performance\n\nSharpe Ratio: All tickers exhibit ratios above 1.0, with the portfolio at 2.3692, confirming strong risk-reward optimization.\nSortino Ratio: With a portfolio ratio of 3.1891, the strategy effectively minimizes downside risk and outperforms benchmarks during volatile periods.\n\n\n\nWin-Loss Statistics\n\nCollective win rate: 57.55%, demonstrating profitability despite varied individual ticker performance.\nLosses are well-contained, with portfolio drawdowns limited to 3.67% per trade.\n\n\n\nTrade Analysis\n\nBest Trades: NFLX delivered a standout return of 10.86%, bolstering portfolio performance.\nWorst Trades: META experienced the highest loss of 5.58%, mitigated by overall prudent risk management.\n\n\n\n\nFinancial Implications and Model Performance\n\nAlpha Generation\n\nThe model consistently outperforms benchmarks, delivering robust alpha through a blend of risk-adjusted returns and effective stock selection.\n\n\n\nPortfolio Diversification\n\nDiversified stock-specific contributions—NFLX and GOOG as high-performing assets—offset slower gains from META and AAPL, enhancing portfolio stability.\n\n\n\nMarket Suitability\n\nHigh Sortino Ratios highlight the model’s suitability for volatile markets, appealing to risk-averse investors seeking stable returns.\n\n\n\nAccuracy\n\nWhile individual stock prediction accuracy varies (win rates: 37.96% to 46.12%), the weighted portfolio metrics demonstrate reliable predictive and hedging performance."
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#opportunities-for-strategic-development",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#opportunities-for-strategic-development",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Opportunities for Strategic Development",
    "text": "Opportunities for Strategic Development\n\nAdvancing the Model\n\nAlgorithm Refinement\n\nIncorporating advanced financial algorithms such as stochastic processes or Monte Carlo simulations to enhance predictability.\nAdopting ensemble learning models to refine stock price movement predictions.\n\n\n\nEnhanced Data Utilization\n\nUtilizing macroeconomic indicators and alternative data (e.g., social media sentiment, news analytics) to fine-tune portfolio rebalancing and optimize entries/exits.\n\n\n\nRisk Monitoring\n\nIntroducing advanced metrics like Conditional Value at Risk (CVaR) and Maximum Drawdown to better quantify extreme risks.\nAutomating hedging strategies like delta-gamma neutralization to minimize volatility impact."
  },
  {
    "objectID": "posts/5-2024-00/Basic_portfolio_optimization.html#conclusion",
    "href": "posts/5-2024-00/Basic_portfolio_optimization.html#conclusion",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Conclusion",
    "text": "Conclusion\nThe tryout version of this financial portfolio strategy successfully demonstrates the potential for effective asset selection and risk-adjusted portfolio management. By achieving an annualized return of 44.90% and outstanding Sharpe and Sortino ratios, it reflects strong financial acumen and real-world applicability. This initial version establishes a solid foundation for developing advanced and sophisticated trading strategies in subsequent iterations.\nWith added layers of complexity, improved machine learning models, and the inclusion of broader financial market data, this project can evolve into a comprehensive framework for data-driven investment decision-making, setting benchmarks for algorithmic trading and portfolio optimization."
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html",
    "href": "posts/4-2024-17-07-00/index.html",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "This project demonstrates portfolio optimization using Weighted Correlation Analysis (WCA) and machine learning to predict stock returns and optimize asset allocation. -The implementation utilizes Python, open-source libraries, and real-time financial data from Yahoo Finance.\nLets see how this work from (Rush 1998).\n\n\n\n\n\nShow the code\npip install numpy pandas matplotlib seaborn yfinance scikit-learn cvxpy PyPortfolioOpt\n\n\nRequirement already satisfied: numpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.26.4)\nRequirement already satisfied: pandas in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (2.2.2)\nRequirement already satisfied: matplotlib in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (3.10.1)\nRequirement already satisfied: seaborn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.13.2)\nRequirement already satisfied: yfinance in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.2.65)\nRequirement already satisfied: scikit-learn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.6.1)\nRequirement already satisfied: cvxpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.7.2)\nRequirement already satisfied: PyPortfolioOpt in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.5.6)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: requests&gt;=2.31 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.32.4)\nRequirement already satisfied: multitasking&gt;=0.0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: platformdirs&gt;=2.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.3.3)\nRequirement already satisfied: frozendict&gt;=2.3.4 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.4.4)\nRequirement already satisfied: peewee&gt;=3.16.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (3.17.6)\nRequirement already satisfied: beautifulsoup4&gt;=4.11.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: curl_cffi&gt;=0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.13.0)\nRequirement already satisfied: protobuf&gt;=3.19.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.25.3)\nRequirement already satisfied: websockets&gt;=13.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (15.0.1)\nRequirement already satisfied: scipy&gt;=1.6.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: osqp&gt;=0.6.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (1.0.4)\nRequirement already satisfied: clarabel&gt;=0.5.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (0.11.1)\nRequirement already satisfied: scs&gt;=3.2.4.post1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (3.2.8)\nRequirement already satisfied: ecos&lt;3.0.0,&gt;=2.0.14 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (2.0.14)\nRequirement already satisfied: plotly&lt;6.0.0,&gt;=5.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (5.24.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from plotly&lt;6.0.0,&gt;=5.0.0-&gt;PyPortfolioOpt) (8.5.0)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from beautifulsoup4&gt;=4.11.1-&gt;yfinance) (2.6)\nRequirement already satisfied: cffi in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from clarabel&gt;=0.5.0-&gt;cvxpy) (1.17.1)\nRequirement already satisfied: certifi&gt;=2024.2.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from curl_cffi&gt;=0.7-&gt;yfinance) (2025.4.26)\nRequirement already satisfied: pycparser in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cffi-&gt;clarabel&gt;=0.5.0-&gt;cvxpy) (2.22)\nRequirement already satisfied: jinja2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (3.1.3)\nRequirement already satisfied: setuptools in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (65.5.0)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (2.2.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from jinja2-&gt;osqp&gt;=0.6.2-&gt;cvxpy) (2.1.5)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\n\n\nWe collected historical adjusted closing prices for six stocks (AAPL, MSFT, GOOGL, AMZN, TSLA, META) using Yahoo Finance: \\[\nR_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\] \\[\nwhere, R_t: Daily returns, P_t:Adjusted closing price at time t.\n\\]\n\n\nShow the code\n# Data collection:\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\"]\n\ndata = yf.download(tickers, start=\"2020-11-23\", end=\"2024-11-23\")['Close']\n# big_data = yf.download(\"AAPL\",  start=\"2018-11-23\", end=\"2024-11-23\")\nprint(data)\nreturns = data.pct_change().dropna()\nreturns_v = data.diff().dropna()\nprint(returns.head())\n\n\nC:\\Temp\\ipykernel_21788\\1772283861.py:8: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][                       0%                       ][**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nTicker            AAPL        AMZN       GOOGL        META        MSFT  \\\nDate                                                                     \n2020-11-23  110.934998  154.919495   85.863052  266.965363  201.986221   \n2020-11-24  112.221176  155.903000   87.669212  275.409119  205.591232   \n2020-11-25  113.059158  159.253494   87.680649  274.086334  205.600861   \n2020-11-27  113.604836  159.766998   88.818321  276.294250  206.908249   \n2020-11-30  116.001854  158.401993   87.197044  275.458801  205.793137   \n...                ...         ...         ...         ...         ...   \n2024-11-18  227.213409  201.699997  174.686417  553.035217  412.661957   \n2024-11-19  227.472488  204.610001  177.496536  559.708801  414.676819   \n2024-11-20  228.189941  202.880005  175.364029  564.127869  412.393921   \n2024-11-21  227.711655  198.380005  167.043274  561.703918  410.613708   \n2024-11-22  229.056870  197.119995  164.183304  557.763611  414.721130   \n\nTicker            TSLA  \nDate                    \n2020-11-23  173.949997  \n2020-11-24  185.126663  \n2020-11-25  191.333328  \n2020-11-27  195.253326  \n2020-11-30  189.199997  \n...                ...  \n2024-11-18  338.739990  \n2024-11-19  346.000000  \n2024-11-20  342.029999  \n2024-11-21  339.640015  \n2024-11-22  352.559998  \n\n[1007 rows x 6 columns]\nTicker          AAPL      AMZN     GOOGL      META      MSFT      TSLA\nDate                                                                  \n2020-11-24  0.011594  0.006348  0.021035  0.031629  0.017848  0.064252\n2020-11-25  0.007467  0.021491  0.000130 -0.004803  0.000047  0.033527\n2020-11-27  0.004826  0.003224  0.012975  0.008056  0.006359  0.020488\n2020-11-30  0.021100 -0.008544 -0.018254 -0.003024 -0.005389 -0.031002\n2020-12-01  0.030827  0.016427  0.023347  0.034589  0.009997  0.030233\n\n\n\n\n\n\n\n\nThe Weighted Correlation Score calculates the relationship between stocks while incorporating predefined weights () for each stock: \\[\nWCA = {w^T}{C_w}\n\\] w:Weight vector , C:Correlation matrix of stock return.\n\n\nShow the code\n# Define weights based on arbitrary criteria (e.g., market cap or equal weights)\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n\n# Correlation matrix\n# correlation_matrix = returns.corr()\ncorrelation_matrix = returns_v.corr()\n\nweighted_correlation_scores = weights @ correlation_matrix @ weights.T\nprint(\"Weighted Correlation Score:\", weighted_correlation_scores)\n\n\nWeighted Correlation Score: 0.6065164990611139\n\n\n\n\n\nAdded technical indicators like Simple Moving Average (SMA) and volatility for feature enrichment. \\[\nSMA_{20}(t) = \\frac{1}{20} \\sum_{i=t-19}^t P_i\n\\] \\[\nVolatility_{20}(t) = \\sqrt{\\frac{1}{20} \\sum_{i=t-19}^t (P_i - \\mu)^2}\n\\]\n\n\nShow the code\n# Feature Engineering\ndef add_indicators(data):\n      indicators = pd.DataFrame(index=data.index)\n      for ticker in data.columns:\n            indicators[f\"{ticker}_SMA_20\"] = data[ticker].rolling(window=20).mean()\n            indicators[f\"{ticker}_Volatility\"] = data[ticker].rolling(window=20).std()\n      return indicators\n\n# Indicators\nindicators = add_indicators(data)\nindicators = indicators.dropna()\naligned_returns = returns_v.loc[indicators.index]\n\nprint(\"Indicators shape:\", indicators.shape)\nprint(\"Returns shape:\", aligned_returns.shape)\nprint(indicators.tail())\n\n\nIndicators shape: (988, 12)\nReturns shape: (988, 6)\n            AAPL_SMA_20  AAPL_Volatility  AMZN_SMA_20  AMZN_Volatility  \\\nDate                                                                     \n2024-11-18   226.633758         3.998910   198.556999         9.721216   \n2024-11-19   226.269012         3.522267   199.302499         9.576879   \n2024-11-20   226.193959         3.460972   200.210999         8.961807   \n2024-11-21   226.104446         3.393406   200.810999         8.369185   \n2024-11-22   226.040389         3.320594   201.275499         7.852670   \n\n            GOOGL_SMA_20  GOOGL_Volatility  META_SMA_20  META_Volatility  \\\nDate                                                                       \n2024-11-18    171.795573          5.980756   573.810977        11.965817   \n2024-11-19    172.442300          5.855409   572.767554        12.251205   \n2024-11-20    173.099990          5.363722   572.858829        12.175700   \n2024-11-21    173.344631          4.932253   572.624905        12.350366   \n2024-11-22    173.319219          4.980257   571.921143        12.790691   \n\n            MSFT_SMA_20  MSFT_Volatility  TSLA_SMA_20  TSLA_Volatility  \nDate                                                                    \n2024-11-18   418.098120         7.656977   281.000498        40.999921  \n2024-11-19   417.615742         7.547192   287.401998        40.634043  \n2024-11-20   417.163635         7.577066   293.820998        38.451728  \n2024-11-21   416.616066         7.637714   297.778999        38.910571  \n2024-11-22   416.104141         7.388088   301.947498        40.133145  \n\n\n\n\n\nMachine Learning for Return Prediction Model: Random Forest Regressor Features (𝑋): SMA, volatility, and other indicators Target, (y): Stock returns Training and Prediction Process: Split data into training (80%) and testing (20%) sets. Train the model: \\[\n\\hat{y} = f(X; \\theta)\n\\] \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\n\nShow the code\n# Traning model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX = indicators.values\n# y = returns.shift(-1).dropna().values  # Predict next day's returns\ny = aligned_returns.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Train Random Forest model:\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\nMean Squared Error: 35.39400034985514\n\n\n\n\n\nPortfolio optimization aims to allocate asset weights w to maximize the Sharpe Ratio. Using predicted returns and a covariance matrix (Σ), optimize weights (w) to maximize the Sharpe Ratio. \\[\n\\text{Sharpe Ratio}= \\frac{w^T \\mu}{\\sqrt{w^T \\Sigma w}}\n\\] Where, \\[\n\\mu: \\text{Expected returns vector, and derived using mean historical returns }\n\\] \\[\n\\Sigma: \\text{Covariance matrix of stock returns, and derived using the covariance of returns}\n\\] \\[\nw:\\text{Portfolio weights (sum to 1)}\n\\]\nOptimization is performed using genetic algorithms (GA). -Solve for 𝑤 using PyPortfolioOpt to maximize the Sharpe Ratio -Portfolio_performance measure expected return, volatility, and Sharpe ratio\n\n\nShow the code\n# Portfolio performance\nfrom pypfopt import EfficientFrontier, risk_models, expected_returns\n\n# Calculate expected returns and risk model\nmu = expected_returns.mean_historical_return(data)\nS = risk_models.sample_cov(data)\n\n# Perform mean-variance optimization\nef = EfficientFrontier(mu, S)\nweights = ef.max_sharpe()\ncleaned_weights = ef.clean_weights()\n\n# Portfolio performance\nperformance = ef.portfolio_performance(verbose=True)\nprint(\"Optimized Weights:\", cleaned_weights)\n\n\nExpected annual return: 19.8%\nAnnual volatility: 24.2%\nSharpe Ratio: 0.82\nOptimized Weights: OrderedDict([('AAPL', 0.46257), ('AMZN', 0.0), ('GOOGL', 0.0), ('META', 0.0), ('MSFT', 0.53743), ('TSLA', 0.0)])\n\n\n\n\n\nBacktesting evaluates the portfolio’s historical performance: 1. Portfolio Returns: \\[\nR_{t}^{Portfolio} = \\sum_{i=1}^{n} w_i R_{t}^{(i)}\n\\] \\[\n\\text{Where } R_{t}^{(i)}: \\text{Returns of assest } i\n\\] 2. Cumulative Returns: \\[\nCumulative \\ Return = \\prod_{t=1}^{T} (1 + R_{t}^{Portfolio})\n\\]\n\n\nShow the code\nimport matplotlib.pyplot as plt\n# Backtesting\nportfolio_returns = (returns * pd.Series(cleaned_weights)).sum(axis=1)\n# Cumulative returns\ncumulative_returns = (1 + portfolio_returns).cumprod()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_returns, label=\"Portfolio\")\nplt.title(\"Portfolio Cumulative Returns\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Return\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIncorporate advanced metrics like Value at Risk (VaR) and Conditional VaR (CVaR). 1. Value at Risk (VaR): Measures potential loss over a given time frame with a confidence level (𝛼): \\[\nVaR_{\\alpha} = Percentile_{(1-\\alpha) \\cdot 100}(R_{t}^{Portfolio})\n\\] 2. Conditional Value at Risk (CVaR): Average loss beyond the VaR threshold: \\[\nCVaR_{\\alpha} = E[R_{t}^{Portfolio} | R_{t}^{Portfolio} \\leq VaR_{\\alpha}]\n\\]\n\n\nShow the code\n#  Financial Analysis\n# Value at Risk (VaR)\ndef calculate_var(returns, confidence_level=0.95):\n      return np.percentile(returns, (1 - confidence_level) * 100)\n# Conditional Value at Risk (CVaR)\ndef calculate_cvar(returns, confidence_level=0.95):\n      var = calculate_var(returns, confidence_level)\n      return returns[returns &lt;= var].mean()\n\nvar = calculate_var(portfolio_returns, 0.95)\ncvar = calculate_cvar(portfolio_returns, 0.95)\nprint(\"Value at Risk (95%):\", var)\nprint(\"Conditional Value at Risk (95%):\", cvar)\n\n\nValue at Risk (95%): -0.025552286989456628\nConditional Value at Risk (95%): -0.03341943747526015\n\n\n\n\n\nUpdate the pipeline for real-time data to to periodically rebalance the portfolio.\n\n\nShow the code\n# Re balancing portfolio:\ndef rebalance_portfolio():\n      new_data = yf.download(tickers, period=\"1y\")['Close']\n      updated_returns = new_data.pct_change().dropna()\n      # Update weights\n      mu = expected_returns.mean_historical_return(new_data)\n      S = risk_models.sample_cov(new_data)\n      ef = EfficientFrontier(mu, S)\n      updated_weights = ef.max_sharpe()\n      return ef.clean_weights()\n\nnew_weights = rebalance_portfolio()\nprint(\"Updated Portfolio Weights:\", new_weights)\n\n\nC:\\Temp\\ipykernel_21788\\532968446.py:3: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nUpdated Portfolio Weights:\n\n\n\n\n\n OrderedDict([('AAPL', 0.0), ('AMZN', 0.0), ('GOOGL', 0.11076), ('META', 0.61498), ('MSFT', 0.10959), ('TSLA', 0.16467)])\n\n\n\n\n\nExperiment with New Models: LSTM for Time-Series Analysis: LSTMs are used for time-series predictions by learning temporal dependencies in sequential data. This model consist two stacked LSTM layers and one dense layer for output. Input shape is (T,1), where T is time window.\nMinimize the Mean Squared Error (MSE) loss function: \\[\nMSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n\\] Where, y: actual value, yi:prediction value, N:number of observations\nTried different ML models (LSTMs for time-series data) to analyze performance improvements. For example:\n\n\nShow the code\n# Train simple LSTM model:\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n# SimpleLSTM model:\nmodel = Sequential([\n      LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      LSTM(50),Dense(6)\n])\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=25, batch_size=32)\n\n\nEpoch 1/25\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 53s 2s/step - loss: 39.5165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2477\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2813\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - loss: 33.3762\n\nEpoch 2/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.3277\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.7240 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1632\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.8727\n\nEpoch 3/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 32.3275\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.5955 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.0387\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.3904\n\nEpoch 4/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.7662\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.7986 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.9454\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4878\n\nEpoch 5/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 41.2270\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 40.3785 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.5548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.6367\n\nEpoch 6/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 21.3232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.3106 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.8787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.9643\n\nEpoch 7/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 33.2178\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.7286 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.8938\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.2937\n\nEpoch 8/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 37.6387\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.4804 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8504\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.1737\n\nEpoch 9/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 31.5482\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.7810 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.3253\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.6085\n\nEpoch 10/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 29.9283\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.2297 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.5692\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.0252\n\nEpoch 11/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 49.6404\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 39.5356 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.4707\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.7915\n\nEpoch 12/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 35.1126\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.9197 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.4233\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.9833\n\nEpoch 13/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 37.7928\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.6242 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1357\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.6409\n\nEpoch 14/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 26.2123\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.5567 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.8436\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.4102\n\nEpoch 15/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.4644\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8502 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8992\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.4432\n\nEpoch 16/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 31.5105\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.4188 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2028\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2188\n\nEpoch 17/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 37.3750\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.0534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.8307\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4813\n\nEpoch 18/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 35.5053\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.7371 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.6191\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.1284\n\nEpoch 19/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 70.2404\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 43.4028 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.2503\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.7107\n\nEpoch 20/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 36.3332\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4543 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.6317\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5344\n\nEpoch 21/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 45.7232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5549 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5615\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.4568\n\nEpoch 22/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 19.1207\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.6123 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.7370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.1944\n\nEpoch 23/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.0332\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.9022 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.4226\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.5722\n\nEpoch 24/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 41.9641\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.6534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.2108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.3662\n\nEpoch 25/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 31.4293\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.0423 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.1044\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.9401\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x1d186185f00&gt;\n\n\n\n\nShow the code\n# Test simple LSTM:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nX_test = X_test.reshape(-1, X_test.shape[1], 1)  #matches input shape\npredictions = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n\n# Plot\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 20))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step\n\nMean Squared Error (MSE): 26.36067196204864\n\nMean Absolute Error (MAE): 3.3579656468743146\n\nMean Absolute Error (MAE): -0.007039653361637514\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Added Features: Dropout layers for regularization and hidden layers for enhanced prediction capability. -Hyperparameters: Tuned units, dropout rate, and batch size.\n\n\nShow the code\n# Complex LSTM:Train\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Sequential\n# Hyperparameter Tuning\nlstm_units = 64  # Adjust based on dataset complexity\ndense_units = 16  # Adjust based on prediction task\ndropout_rate = 0.2  # Regularization for overfitting prevention\n# Model Architecture\nmodel = Sequential([\n      layers.LSTM(lstm_units, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      layers.Dropout(dropout_rate),  # Dropout for regularization\n      layers.LSTM(lstm_units),\n      layers.Dropout(dropout_rate),  # Dropout after second LSTM\n      layers.Dense(dense_units, activation='relu'),  # Hidden layer with ReLU activation\n      layers.Dense(6)  # Output layer\n])\n# Model Compilation (consider alternative optimizers or losses if needed)\nmodel.compile(optimizer='adam', loss='mse')\n# Model Training (adjust epochs and batch size based on dataset size)\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=50, batch_size=64)\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 37s 3s/step - loss: 32.6753\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/13 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 30.4756\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.6023 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - loss: 32.6804\n\nEpoch 2/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.4428\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1326 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.7786\n\nEpoch 3/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 28.8467\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.1278 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.2863\n\nEpoch 4/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 27.1469\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8941 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.6994\n\nEpoch 5/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.6877\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9405 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.1251\n\nEpoch 6/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 40.2975\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.6658 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1326\n\nEpoch 7/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.5694\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1995 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.4346\n\nEpoch 8/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.7298\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.3040 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2384\n\nEpoch 9/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 40.2883\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.2865 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0234\n\nEpoch 10/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 48.7129\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.2298 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.8014\n\nEpoch 11/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.8713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.1084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0665\n\nEpoch 12/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.5664\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0666 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.3733\n\nEpoch 13/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 52.7092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.7672 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.3126\n\nEpoch 14/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 33.1447\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.1081 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.1730\n\nEpoch 15/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 29.2676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1979 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0580\n\nEpoch 16/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.9296\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0732 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5588\n\nEpoch 17/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 23.8168\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.5733 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.4904\n\nEpoch 18/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.9119\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.6892 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4984\n\nEpoch 19/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 37.0138\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.6785 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.6902\n\nEpoch 20/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 57.8933\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.0482 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.4547\n\nEpoch 21/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.7437\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8452 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0302\n\nEpoch 22/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 55.7362\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.0948 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.8034\n\nEpoch 23/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 30.1329\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.2644 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0424\n\nEpoch 24/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 61.1905\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.7734 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.7667\n\nEpoch 25/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 39.4268\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.8849 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.2124\n\nEpoch 26/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.2235\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7191 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.8612\n\nEpoch 27/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.1900\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.2844 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.4533\n\nEpoch 28/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 30.4891\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7685 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.1717\n\nEpoch 29/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.5055\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.4891 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.1457\n\nEpoch 30/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 30.1711\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.7750 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.1546\n\nEpoch 31/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 45.5812\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2254 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.3119\n\nEpoch 32/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 34.6720\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.1084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1047\n\nEpoch 33/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.5791\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8678 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7780\n\nEpoch 34/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 26.1209\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7540 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9749\n\nEpoch 35/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 31.2264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.2207 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.6101\n\nEpoch 36/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.5459\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.3749 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5689\n\nEpoch 37/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.3811\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.8371 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3684\n\nEpoch 38/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.1864\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.5115 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8861\n\nEpoch 39/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.7311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.5647 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1705\n\nEpoch 40/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 35.7070\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5676 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.3916\n\nEpoch 41/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 33.0287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.1253 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.4669\n\nEpoch 42/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.8123\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.3746 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1071\n\nEpoch 43/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 36.2125\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.4573 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.1695\n\nEpoch 44/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 28.5075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9747 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.6717\n\nEpoch 45/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 28.5793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.7616 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.6576\n\nEpoch 46/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 32.6839\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7341 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1023\n\nEpoch 47/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 29.2934\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.0574 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3787\n\nEpoch 48/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 30.2211\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.4388 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5631\n\nEpoch 49/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 22.4794\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1774 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1396\n\nEpoch 50/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.3593\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.1971 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0117\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x1d18e779a20&gt;\n\n\n\n\nShow the code\n# Complex LSTM: Test\n\n# Predict on the test data\nX_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)  # Reshape to match LSTM input\npredictions = model.predict(X_test_reshaped)\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R²):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 224ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step\n\nMean Squared Error (MSE): 26.433762436432502\n\nMean Absolute Error (MAE): 3.359821212697486\n\nR-squared (R²): -0.0102379401932855\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:01\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\n# Feature Scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1]))  # Scale X_train\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[1]))  # Scale X_test\n# Reshaping the data for LSTM\nX_train_scaled = X_train_scaled.reshape(-1, X_train.shape[1], 1)  # LSTM input shape\nX_test_scaled = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n\n# Model with Dropout and Bidirectional LSTM\nmodel = Sequential([\n    Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], 1))),\n    Dropout(0.2),  # Dropout layer to prevent overfitting\n    LSTM(64, return_sequences=False),\n    Dropout(0.2),\n    Dense(6)  # Output layer with 6 units corresponding to 6 companies\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\npredictions = model.predict(X_test_scaled)\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 1:09 4s/step - loss: 33.6312\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.6842 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 41.0561\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 4s 28ms/step - loss: 39.3424 - val_loss: 31.0006\n\nEpoch 2/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.0747\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.5854 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.8950\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.5192 - val_loss: 31.0079\n\nEpoch 3/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 97.5430\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 49.8983 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 42.8704\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 41.2654 - val_loss: 30.9917\n\nEpoch 4/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 47.9300\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 41.0878 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.0791\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 37.2074 - val_loss: 30.9983\n\nEpoch 5/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 38.1563\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 39.1968 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.7594\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.2161 - val_loss: 30.9931\n\nEpoch 6/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 20.6336\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.6752 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.9666\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 39.7361 - val_loss: 31.0308\n\nEpoch 7/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.2391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.9469 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.1781 - val_loss: 30.9741\n\nEpoch 8/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 47.4185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 42.8206 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 39.9146\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 39.1099 - val_loss: 31.0026\n\nEpoch 9/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 21.7837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.9813 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.5086 - val_loss: 30.9795\n\nEpoch 10/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 33.5511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.9692 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.4472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.6877 - val_loss: 30.9721\n\nEpoch 11/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.6120\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5261 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5571\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.6826 - val_loss: 31.0022\n\nEpoch 12/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 30.4042\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.7277 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7305\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.9554 - val_loss: 30.9754\n\nEpoch 13/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.2550\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3919 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.2988\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.6347 - val_loss: 30.9701\n\nEpoch 14/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.4538\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2494 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.3066\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.2402 - val_loss: 30.9928\n\nEpoch 15/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 25.7483\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8072 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.4867\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 35.9817 - val_loss: 30.9940\n\nEpoch 16/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 36.7494\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.4170 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 34.2114 - val_loss: 30.9547\n\nEpoch 17/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 40.6883\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8505 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8032\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.3459 - val_loss: 31.0462\n\nEpoch 18/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 28.5334\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4896 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.6775\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.1813 - val_loss: 31.0573\n\nEpoch 19/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 39.3846\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.9366 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.9933\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.5440 - val_loss: 30.9855\n\nEpoch 20/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.8439\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.1431 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.9724\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.4201 - val_loss: 31.0346\n\nEpoch 21/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 49.1164\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.7055 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.6501\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.4797 - val_loss: 31.0338\n\nEpoch 22/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.4078\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.9486 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.7647\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.4854 - val_loss: 30.9346\n\nEpoch 23/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 24.9192\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 28.9741 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4319\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.9273 - val_loss: 31.0215\n\nEpoch 24/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 39.3370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.6874 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.6588\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.6748 - val_loss: 31.0275\n\nEpoch 25/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.6830\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.4885 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.3536\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.9612 - val_loss: 31.0718\n\nEpoch 26/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 19.7064\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.6401 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.8195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.3520 - val_loss: 31.0518\n\nEpoch 27/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 20.3761\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.4750 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.2065\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 30.1796 - val_loss: 31.0342\n\nEpoch 28/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 65.5683\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.5360 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8475\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.9565 - val_loss: 30.9991\n\nEpoch 29/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 59.7465\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.8582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.5779\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.6928 - val_loss: 31.0896\n\nEpoch 30/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 23.1380\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8612 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.2680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.5086 - val_loss: 31.1570\n\nEpoch 31/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 72.1287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 41.8632 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.6793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.8678 - val_loss: 31.1801\n\nEpoch 32/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.9232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.8135 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1262\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.0201 - val_loss: 31.1738\n\nEpoch 33/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.8684\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.0745 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.7356\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.4328 - val_loss: 31.1578\n\nEpoch 34/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.8019\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.4580 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.8462\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.8960 - val_loss: 31.3199\n\nEpoch 35/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.6990\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7959 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0646\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.0243 - val_loss: 31.1878\n\nEpoch 36/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 35.7499\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1637 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.7590\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.5201 - val_loss: 31.2399\n\nEpoch 37/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 28.2468\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8100 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5866\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.6299 - val_loss: 31.1783\n\nEpoch 38/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 61.1837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.4208 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1922\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 34.8359 - val_loss: 31.1996\n\nEpoch 39/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 25.7884\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3802 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.3127 - val_loss: 31.2910\n\nEpoch 40/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 34.5604\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.4089 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0706\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.3018 - val_loss: 31.1145\n\nEpoch 41/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.9797\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.8655 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.2789 - val_loss: 31.1251\n\nEpoch 42/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.5735\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.1926 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.7979 - val_loss: 31.2327\n\nEpoch 43/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 54.3076\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 42.1941 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.7103\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.9662 - val_loss: 31.2996\n\nEpoch 44/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 36.9932\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.2907 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.6917\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 36.0690 - val_loss: 31.2454\n\nEpoch 45/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 27.7511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.9800 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.2817\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.6707 - val_loss: 31.3159\n\nEpoch 46/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 24.3911\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.5005 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 35.7439\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 35.4585 - val_loss: 31.2018\n\nEpoch 47/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 38.2483\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0999 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7495\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.1684 - val_loss: 31.3793\n\nEpoch 48/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 44.4909\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 40.5976 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.2165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.5402 - val_loss: 31.2865\n\nEpoch 49/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.9840\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.5124 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.9152\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.7868 - val_loss: 31.3629\n\nEpoch 50/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 27.7827\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1170 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3686\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 31.9053 - val_loss: 31.4758\n\nWARNING:tensorflow:5 out of the last 15 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D191849900&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 317ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 54ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 54ms/step\n\nMean Squared Error (MSE): 26.782009702254715\n\nMean Absolute Error (MAE): 3.4036650586491515\n\nMean Absolute Error (MAE): -0.0263582647812924\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:02# Check the shape of X_train and y_train\nprint(f\"X_train shape before reshaping: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Reshape X_train to be (n_samples, time_steps=1, features=12)\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # Reshape to (n_samples, 1, 12)\n\n# Reshape X_test similarly\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n# Normalize the data (optional but recommended)\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Fit scaler on X_train and apply to X_train and X_test\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2]))  # Reshape for scaling\nX_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2]))  # Apply scaling to X_test\n\n# Reshape back to 3D for LSTM model\nX_train_scaled = X_train_scaled.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])  # (n_samples, 1, 12)\nX_test_scaled = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])  # (n_samples, 1, 12)\n\n# You can also scale the output if necessary\nscaler_y = MinMaxScaler(feature_range=(0, 1))\ny_train_scaled = scaler_y.fit_transform(y_train)  # Scaling y_train (6 companies' prices)\ny_test_scaled = scaler_y.transform(y_test)  # Scaling y_test (6 companies' prices)\n\n# Check the reshaped data\nprint(f\"X_train_scaled shape: {X_train_scaled.shape}\")\nprint(f\"X_test_scaled shape: {X_test_scaled.shape}\")\nprint(f\"y_train_scaled shape: {y_train_scaled.shape}\")\nprint(f\"y_test_scaled shape: {y_test_scaled.shape}\")\n\n\nX_train shape before reshaping: (790, 12)\ny_train shape: (790, 6)\nX_train_scaled shape: (790, 1, 12)\nX_test_scaled shape: (198, 1, 12)\ny_train_scaled shape: (790, 6)\ny_test_scaled shape: (198, 6)\n\n\n\n\nShow the code\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n# Define model architecture\nmodel = Sequential([\n    LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n    Dropout(0.2),  # Dropout for regularization\n    LSTM(64),\n    Dropout(0.2),\n    Dense(16, activation='relu'),\n    Dense(6)  # Output layer with 6 predictions (one per company)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Fit the model\nmodel.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=64, validation_data=(X_test_scaled, y_test_scaled))\n\n# Predict on X_test_scaled\npredictions = model.predict(X_test_scaled)\n\n# Inverse transform the predictions and actual values to get the original scale\npredictions = scaler_y.inverse_transform(predictions)\ny_test_actual = scaler_y.inverse_transform(y_test_scaled)\n\n# Evaluate the model's performance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse = mean_squared_error(y_test_actual, predictions)\nmae = mean_absolute_error(y_test_actual, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 31s 3s/step - loss: 0.2650\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 3s 28ms/step - loss: 0.2561 - val_loss: 0.2172\n\nEpoch 2/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.2111\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.1957 - val_loss: 0.1284\n\nEpoch 3/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.1230\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.1013 - val_loss: 0.0426\n\nEpoch 4/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0440\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0433 - val_loss: 0.0290\n\nEpoch 5/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0276\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0299 - val_loss: 0.0232\n\nEpoch 6/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 0.0233\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0246 - val_loss: 0.0177\n\nEpoch 7/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0171\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0205 - val_loss: 0.0152\n\nEpoch 8/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0197\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0189 - val_loss: 0.0134\n\nEpoch 9/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0204\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0172 - val_loss: 0.0122\n\nEpoch 10/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0176\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0160 - val_loss: 0.0114\n\nEpoch 11/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0158\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0144 - val_loss: 0.0108\n\nEpoch 12/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0135\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0135 - val_loss: 0.0103\n\nEpoch 13/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0121\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0129 - val_loss: 0.0100\n\nEpoch 14/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0109\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0128 - val_loss: 0.0095\n\nEpoch 15/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0105\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0119 - val_loss: 0.0093\n\nEpoch 16/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0128\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0118 - val_loss: 0.0091\n\nEpoch 17/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 0.0145\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0128 - val_loss: 0.0088\n\nEpoch 18/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0109 - val_loss: 0.0087\n\nEpoch 19/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0094\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0112 - val_loss: 0.0086\n\nEpoch 20/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0119\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0118 - val_loss: 0.0084\n\nEpoch 21/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0102 - val_loss: 0.0083\n\nEpoch 22/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0105 - val_loss: 0.0084\n\nEpoch 23/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0110\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0108 - val_loss: 0.0082\n\nEpoch 24/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0118\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0106 - val_loss: 0.0081\n\nEpoch 25/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0117\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0104 - val_loss: 0.0082\n\nEpoch 26/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0114\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0081\n\nEpoch 27/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 0.0087\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0081\n\nEpoch 28/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0104\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0102 - val_loss: 0.0081\n\nEpoch 29/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0098 - val_loss: 0.0080\n\nEpoch 30/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0104 - val_loss: 0.0080\n\nEpoch 31/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 32/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0081\n\nEpoch 33/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 34/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0093\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 35/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0098\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 36/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 37/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0079\n\nEpoch 38/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0107\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 39/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0090\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0094 - val_loss: 0.0080\n\nEpoch 40/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0098 - val_loss: 0.0079\n\nEpoch 41/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0093\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0080\n\nEpoch 42/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0106\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0080\n\nEpoch 43/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0085\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0090 - val_loss: 0.0080\n\nEpoch 44/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0081\n\nEpoch 45/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 46/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0112\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0094 - val_loss: 0.0081\n\nEpoch 47/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0095\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 48/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0104\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 49/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0100\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0080\n\nEpoch 50/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0081\n\nWARNING:tensorflow:5 out of the last 15 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D19737B760&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step\n\nMean Squared Error (MSE): 27.28565082875997\n\nMean Absolute Error (MAE): 3.3971546541546473\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Plot Actual vs Predicted values for each company\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\ncolors = ['blue', 'green', 'red']\n\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    plt.subplot(3, 2, i+1)\n    plt.plot(y_test_actual[:, i], label=f\"Actual {companies[i]}\", color='green', linestyle='-', linewidth=2)\n    plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color='blue', linestyle='--', linewidth=2)\n    plt.title(f\"Actual vs Predicted for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Plot Error for each company\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    error = y_test_actual[:, i] - predictions[:, i]\n    plt.subplot(3, 2, i+1)\n    plt.plot(error, label=f\"Error {companies[i]}\", color='red')\n    plt.title(f\"Prediction Error for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Stress Testing: Simulate extreme market conditions (e.g., 2008 financial crisis, COVID-19 crash) to see how the portfolio performs under stress.\n\n\nShow the code\n# Stress testing:\n\ndef stress_test(data, weights):\n    stress_returns = data.pct_change().apply(lambda x: x + np.random.normal(0, 0.1), axis=0)\n    portfolio_stress_returns = (stress_returns * pd.Series(weights)).sum(axis=1)\n    return portfolio_stress_returns\n\nstress_returns = stress_test(data, new_weights)\nprint(\"Portfolio returns under stress:\", stress_returns.describe())\n\n\nPortfolio returns under stress: count    1007.000000\nmean        0.038556\nstd         0.022724\nmin        -0.135397\n25%         0.027801\n50%         0.038683\n75%         0.050441\nmax         0.200089\ndtype: float64\n\n\n\n\n\n-Risk Parity Portfolio: Balance risks across assets instead of focusing on returns. -Black-Litterman Model: Blend market equilibrium with subjective views."
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#dependancy",
    "href": "posts/4-2024-17-07-00/index.html#dependancy",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Show the code\npip install numpy pandas matplotlib seaborn yfinance scikit-learn cvxpy PyPortfolioOpt\n\n\nRequirement already satisfied: numpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.26.4)\nRequirement already satisfied: pandas in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (2.2.2)\nRequirement already satisfied: matplotlib in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (3.10.1)\nRequirement already satisfied: seaborn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.13.2)\nRequirement already satisfied: yfinance in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.2.65)\nRequirement already satisfied: scikit-learn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.6.1)\nRequirement already satisfied: cvxpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.7.2)\nRequirement already satisfied: PyPortfolioOpt in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.5.6)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: requests&gt;=2.31 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.32.4)\nRequirement already satisfied: multitasking&gt;=0.0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: platformdirs&gt;=2.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.3.3)\nRequirement already satisfied: frozendict&gt;=2.3.4 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.4.4)\nRequirement already satisfied: peewee&gt;=3.16.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (3.17.6)\nRequirement already satisfied: beautifulsoup4&gt;=4.11.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: curl_cffi&gt;=0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.13.0)\nRequirement already satisfied: protobuf&gt;=3.19.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.25.3)\nRequirement already satisfied: websockets&gt;=13.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (15.0.1)\nRequirement already satisfied: scipy&gt;=1.6.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: osqp&gt;=0.6.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (1.0.4)\nRequirement already satisfied: clarabel&gt;=0.5.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (0.11.1)\nRequirement already satisfied: scs&gt;=3.2.4.post1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (3.2.8)\nRequirement already satisfied: ecos&lt;3.0.0,&gt;=2.0.14 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (2.0.14)\nRequirement already satisfied: plotly&lt;6.0.0,&gt;=5.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (5.24.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from plotly&lt;6.0.0,&gt;=5.0.0-&gt;PyPortfolioOpt) (8.5.0)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from beautifulsoup4&gt;=4.11.1-&gt;yfinance) (2.6)\nRequirement already satisfied: cffi in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from clarabel&gt;=0.5.0-&gt;cvxpy) (1.17.1)\nRequirement already satisfied: certifi&gt;=2024.2.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from curl_cffi&gt;=0.7-&gt;yfinance) (2025.4.26)\nRequirement already satisfied: pycparser in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cffi-&gt;clarabel&gt;=0.5.0-&gt;cvxpy) (2.22)\nRequirement already satisfied: jinja2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (3.1.3)\nRequirement already satisfied: setuptools in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (65.5.0)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (2.2.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from jinja2-&gt;osqp&gt;=0.6.2-&gt;cvxpy) (2.1.5)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n[notice] To update, run: python.exe -m pip install --upgrade pip"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#data-collection",
    "href": "posts/4-2024-17-07-00/index.html#data-collection",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "We collected historical adjusted closing prices for six stocks (AAPL, MSFT, GOOGL, AMZN, TSLA, META) using Yahoo Finance: \\[\nR_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\] \\[\nwhere, R_t: Daily returns, P_t:Adjusted closing price at time t.\n\\]\n\n\nShow the code\n# Data collection:\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\"]\n\ndata = yf.download(tickers, start=\"2020-11-23\", end=\"2024-11-23\")['Close']\n# big_data = yf.download(\"AAPL\",  start=\"2018-11-23\", end=\"2024-11-23\")\nprint(data)\nreturns = data.pct_change().dropna()\nreturns_v = data.diff().dropna()\nprint(returns.head())\n\n\nC:\\Temp\\ipykernel_21788\\1772283861.py:8: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][                       0%                       ][**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nTicker            AAPL        AMZN       GOOGL        META        MSFT  \\\nDate                                                                     \n2020-11-23  110.934998  154.919495   85.863052  266.965363  201.986221   \n2020-11-24  112.221176  155.903000   87.669212  275.409119  205.591232   \n2020-11-25  113.059158  159.253494   87.680649  274.086334  205.600861   \n2020-11-27  113.604836  159.766998   88.818321  276.294250  206.908249   \n2020-11-30  116.001854  158.401993   87.197044  275.458801  205.793137   \n...                ...         ...         ...         ...         ...   \n2024-11-18  227.213409  201.699997  174.686417  553.035217  412.661957   \n2024-11-19  227.472488  204.610001  177.496536  559.708801  414.676819   \n2024-11-20  228.189941  202.880005  175.364029  564.127869  412.393921   \n2024-11-21  227.711655  198.380005  167.043274  561.703918  410.613708   \n2024-11-22  229.056870  197.119995  164.183304  557.763611  414.721130   \n\nTicker            TSLA  \nDate                    \n2020-11-23  173.949997  \n2020-11-24  185.126663  \n2020-11-25  191.333328  \n2020-11-27  195.253326  \n2020-11-30  189.199997  \n...                ...  \n2024-11-18  338.739990  \n2024-11-19  346.000000  \n2024-11-20  342.029999  \n2024-11-21  339.640015  \n2024-11-22  352.559998  \n\n[1007 rows x 6 columns]\nTicker          AAPL      AMZN     GOOGL      META      MSFT      TSLA\nDate                                                                  \n2020-11-24  0.011594  0.006348  0.021035  0.031629  0.017848  0.064252\n2020-11-25  0.007467  0.021491  0.000130 -0.004803  0.000047  0.033527\n2020-11-27  0.004826  0.003224  0.012975  0.008056  0.006359  0.020488\n2020-11-30  0.021100 -0.008544 -0.018254 -0.003024 -0.005389 -0.031002\n2020-12-01  0.030827  0.016427  0.023347  0.034589  0.009997  0.030233"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#weighted-correlation-analysis-wca",
    "href": "posts/4-2024-17-07-00/index.html#weighted-correlation-analysis-wca",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "The Weighted Correlation Score calculates the relationship between stocks while incorporating predefined weights () for each stock: \\[\nWCA = {w^T}{C_w}\n\\] w:Weight vector , C:Correlation matrix of stock return.\n\n\nShow the code\n# Define weights based on arbitrary criteria (e.g., market cap or equal weights)\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n\n# Correlation matrix\n# correlation_matrix = returns.corr()\ncorrelation_matrix = returns_v.corr()\n\nweighted_correlation_scores = weights @ correlation_matrix @ weights.T\nprint(\"Weighted Correlation Score:\", weighted_correlation_scores)\n\n\nWeighted Correlation Score: 0.6065164990611139"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#feature-engineering",
    "href": "posts/4-2024-17-07-00/index.html#feature-engineering",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Added technical indicators like Simple Moving Average (SMA) and volatility for feature enrichment. \\[\nSMA_{20}(t) = \\frac{1}{20} \\sum_{i=t-19}^t P_i\n\\] \\[\nVolatility_{20}(t) = \\sqrt{\\frac{1}{20} \\sum_{i=t-19}^t (P_i - \\mu)^2}\n\\]\n\n\nShow the code\n# Feature Engineering\ndef add_indicators(data):\n      indicators = pd.DataFrame(index=data.index)\n      for ticker in data.columns:\n            indicators[f\"{ticker}_SMA_20\"] = data[ticker].rolling(window=20).mean()\n            indicators[f\"{ticker}_Volatility\"] = data[ticker].rolling(window=20).std()\n      return indicators\n\n# Indicators\nindicators = add_indicators(data)\nindicators = indicators.dropna()\naligned_returns = returns_v.loc[indicators.index]\n\nprint(\"Indicators shape:\", indicators.shape)\nprint(\"Returns shape:\", aligned_returns.shape)\nprint(indicators.tail())\n\n\nIndicators shape: (988, 12)\nReturns shape: (988, 6)\n            AAPL_SMA_20  AAPL_Volatility  AMZN_SMA_20  AMZN_Volatility  \\\nDate                                                                     \n2024-11-18   226.633758         3.998910   198.556999         9.721216   \n2024-11-19   226.269012         3.522267   199.302499         9.576879   \n2024-11-20   226.193959         3.460972   200.210999         8.961807   \n2024-11-21   226.104446         3.393406   200.810999         8.369185   \n2024-11-22   226.040389         3.320594   201.275499         7.852670   \n\n            GOOGL_SMA_20  GOOGL_Volatility  META_SMA_20  META_Volatility  \\\nDate                                                                       \n2024-11-18    171.795573          5.980756   573.810977        11.965817   \n2024-11-19    172.442300          5.855409   572.767554        12.251205   \n2024-11-20    173.099990          5.363722   572.858829        12.175700   \n2024-11-21    173.344631          4.932253   572.624905        12.350366   \n2024-11-22    173.319219          4.980257   571.921143        12.790691   \n\n            MSFT_SMA_20  MSFT_Volatility  TSLA_SMA_20  TSLA_Volatility  \nDate                                                                    \n2024-11-18   418.098120         7.656977   281.000498        40.999921  \n2024-11-19   417.615742         7.547192   287.401998        40.634043  \n2024-11-20   417.163635         7.577066   293.820998        38.451728  \n2024-11-21   416.616066         7.637714   297.778999        38.910571  \n2024-11-22   416.104141         7.388088   301.947498        40.133145"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#train-machine-learning-models",
    "href": "posts/4-2024-17-07-00/index.html#train-machine-learning-models",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Machine Learning for Return Prediction Model: Random Forest Regressor Features (𝑋): SMA, volatility, and other indicators Target, (y): Stock returns Training and Prediction Process: Split data into training (80%) and testing (20%) sets. Train the model: \\[\n\\hat{y} = f(X; \\theta)\n\\] \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\n\nShow the code\n# Traning model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX = indicators.values\n# y = returns.shift(-1).dropna().values  # Predict next day's returns\ny = aligned_returns.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Train Random Forest model:\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\nMean Squared Error: 35.39400034985514"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#portfolio-optimization-1",
    "href": "posts/4-2024-17-07-00/index.html#portfolio-optimization-1",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Portfolio optimization aims to allocate asset weights w to maximize the Sharpe Ratio. Using predicted returns and a covariance matrix (Σ), optimize weights (w) to maximize the Sharpe Ratio. \\[\n\\text{Sharpe Ratio}= \\frac{w^T \\mu}{\\sqrt{w^T \\Sigma w}}\n\\] Where, \\[\n\\mu: \\text{Expected returns vector, and derived using mean historical returns }\n\\] \\[\n\\Sigma: \\text{Covariance matrix of stock returns, and derived using the covariance of returns}\n\\] \\[\nw:\\text{Portfolio weights (sum to 1)}\n\\]\nOptimization is performed using genetic algorithms (GA). -Solve for 𝑤 using PyPortfolioOpt to maximize the Sharpe Ratio -Portfolio_performance measure expected return, volatility, and Sharpe ratio\n\n\nShow the code\n# Portfolio performance\nfrom pypfopt import EfficientFrontier, risk_models, expected_returns\n\n# Calculate expected returns and risk model\nmu = expected_returns.mean_historical_return(data)\nS = risk_models.sample_cov(data)\n\n# Perform mean-variance optimization\nef = EfficientFrontier(mu, S)\nweights = ef.max_sharpe()\ncleaned_weights = ef.clean_weights()\n\n# Portfolio performance\nperformance = ef.portfolio_performance(verbose=True)\nprint(\"Optimized Weights:\", cleaned_weights)\n\n\nExpected annual return: 19.8%\nAnnual volatility: 24.2%\nSharpe Ratio: 0.82\nOptimized Weights: OrderedDict([('AAPL', 0.46257), ('AMZN', 0.0), ('GOOGL', 0.0), ('META', 0.0), ('MSFT', 0.53743), ('TSLA', 0.0)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#backtesting-evaluating-portfolio-performance-over-historical-data.",
    "href": "posts/4-2024-17-07-00/index.html#backtesting-evaluating-portfolio-performance-over-historical-data.",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Backtesting evaluates the portfolio’s historical performance: 1. Portfolio Returns: \\[\nR_{t}^{Portfolio} = \\sum_{i=1}^{n} w_i R_{t}^{(i)}\n\\] \\[\n\\text{Where } R_{t}^{(i)}: \\text{Returns of assest } i\n\\] 2. Cumulative Returns: \\[\nCumulative \\ Return = \\prod_{t=1}^{T} (1 + R_{t}^{Portfolio})\n\\]\n\n\nShow the code\nimport matplotlib.pyplot as plt\n# Backtesting\nportfolio_returns = (returns * pd.Series(cleaned_weights)).sum(axis=1)\n# Cumulative returns\ncumulative_returns = (1 + portfolio_returns).cumprod()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_returns, label=\"Portfolio\")\nplt.title(\"Portfolio Cumulative Returns\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Return\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#additional-financial-analysis",
    "href": "posts/4-2024-17-07-00/index.html#additional-financial-analysis",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Incorporate advanced metrics like Value at Risk (VaR) and Conditional VaR (CVaR). 1. Value at Risk (VaR): Measures potential loss over a given time frame with a confidence level (𝛼): \\[\nVaR_{\\alpha} = Percentile_{(1-\\alpha) \\cdot 100}(R_{t}^{Portfolio})\n\\] 2. Conditional Value at Risk (CVaR): Average loss beyond the VaR threshold: \\[\nCVaR_{\\alpha} = E[R_{t}^{Portfolio} | R_{t}^{Portfolio} \\leq VaR_{\\alpha}]\n\\]\n\n\nShow the code\n#  Financial Analysis\n# Value at Risk (VaR)\ndef calculate_var(returns, confidence_level=0.95):\n      return np.percentile(returns, (1 - confidence_level) * 100)\n# Conditional Value at Risk (CVaR)\ndef calculate_cvar(returns, confidence_level=0.95):\n      var = calculate_var(returns, confidence_level)\n      return returns[returns &lt;= var].mean()\n\nvar = calculate_var(portfolio_returns, 0.95)\ncvar = calculate_cvar(portfolio_returns, 0.95)\nprint(\"Value at Risk (95%):\", var)\nprint(\"Conditional Value at Risk (95%):\", cvar)\n\n\nValue at Risk (95%): -0.025552286989456628\nConditional Value at Risk (95%): -0.03341943747526015"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#automate-with-real-time-data",
    "href": "posts/4-2024-17-07-00/index.html#automate-with-real-time-data",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Update the pipeline for real-time data to to periodically rebalance the portfolio.\n\n\nShow the code\n# Re balancing portfolio:\ndef rebalance_portfolio():\n      new_data = yf.download(tickers, period=\"1y\")['Close']\n      updated_returns = new_data.pct_change().dropna()\n      # Update weights\n      mu = expected_returns.mean_historical_return(new_data)\n      S = risk_models.sample_cov(new_data)\n      ef = EfficientFrontier(mu, S)\n      updated_weights = ef.max_sharpe()\n      return ef.clean_weights()\n\nnew_weights = rebalance_portfolio()\nprint(\"Updated Portfolio Weights:\", new_weights)\n\n\nC:\\Temp\\ipykernel_21788\\532968446.py:3: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nUpdated Portfolio Weights:\n\n\n\n\n\n OrderedDict([('AAPL', 0.0), ('AMZN', 0.0), ('GOOGL', 0.11076), ('META', 0.61498), ('MSFT', 0.10959), ('TSLA', 0.16467)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#experiment-with-new-models",
    "href": "posts/4-2024-17-07-00/index.html#experiment-with-new-models",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Experiment with New Models: LSTM for Time-Series Analysis: LSTMs are used for time-series predictions by learning temporal dependencies in sequential data. This model consist two stacked LSTM layers and one dense layer for output. Input shape is (T,1), where T is time window.\nMinimize the Mean Squared Error (MSE) loss function: \\[\nMSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n\\] Where, y: actual value, yi:prediction value, N:number of observations\nTried different ML models (LSTMs for time-series data) to analyze performance improvements. For example:\n\n\nShow the code\n# Train simple LSTM model:\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n# SimpleLSTM model:\nmodel = Sequential([\n      LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      LSTM(50),Dense(6)\n])\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=25, batch_size=32)\n\n\nEpoch 1/25\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 53s 2s/step - loss: 39.5165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2477\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2813\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - loss: 33.3762\n\nEpoch 2/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.3277\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.7240 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1632\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.8727\n\nEpoch 3/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 32.3275\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.5955 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.0387\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.3904\n\nEpoch 4/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.7662\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.7986 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.9454\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4878\n\nEpoch 5/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 41.2270\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 40.3785 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.5548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.6367\n\nEpoch 6/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 21.3232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.3106 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.8787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.9643\n\nEpoch 7/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 33.2178\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.7286 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.8938\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.2937\n\nEpoch 8/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 37.6387\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.4804 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8504\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.1737\n\nEpoch 9/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 31.5482\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.7810 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.3253\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.6085\n\nEpoch 10/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 29.9283\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.2297 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.5692\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.0252\n\nEpoch 11/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 49.6404\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 39.5356 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.4707\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.7915\n\nEpoch 12/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 35.1126\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.9197 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.4233\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.9833\n\nEpoch 13/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 37.7928\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.6242 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1357\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.6409\n\nEpoch 14/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 26.2123\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.5567 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.8436\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.4102\n\nEpoch 15/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.4644\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8502 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.8992\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 31.4432\n\nEpoch 16/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 31.5105\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.4188 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2028\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 33.2188\n\nEpoch 17/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 37.3750\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.0534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.8307\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4813\n\nEpoch 18/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 35.5053\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.7371 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.6191\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 36.1284\n\nEpoch 19/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 70.2404\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 43.4028 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 38.2503\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.7107\n\nEpoch 20/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 36.3332\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.4543 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.6317\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5344\n\nEpoch 21/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 45.7232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5549 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.5615\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.4568\n\nEpoch 22/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 19.1207\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.6123 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.7370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.1944\n\nEpoch 23/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.0332\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 30.9022 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.4226\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.5722\n\nEpoch 24/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 41.9641\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.6534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.2108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 32.3662\n\nEpoch 25/25\n\n\n 1/25 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 31.4293\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 37.0423 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 35.1044\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/25 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 34.9401\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x1d186185f00&gt;\n\n\n\n\nShow the code\n# Test simple LSTM:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nX_test = X_test.reshape(-1, X_test.shape[1], 1)  #matches input shape\npredictions = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n\n# Plot\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 20))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 210ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step\n\nMean Squared Error (MSE): 26.36067196204864\n\nMean Absolute Error (MAE): 3.3579656468743146\n\nMean Absolute Error (MAE): -0.007039653361637514\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Added Features: Dropout layers for regularization and hidden layers for enhanced prediction capability. -Hyperparameters: Tuned units, dropout rate, and batch size.\n\n\nShow the code\n# Complex LSTM:Train\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Sequential\n# Hyperparameter Tuning\nlstm_units = 64  # Adjust based on dataset complexity\ndense_units = 16  # Adjust based on prediction task\ndropout_rate = 0.2  # Regularization for overfitting prevention\n# Model Architecture\nmodel = Sequential([\n      layers.LSTM(lstm_units, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      layers.Dropout(dropout_rate),  # Dropout for regularization\n      layers.LSTM(lstm_units),\n      layers.Dropout(dropout_rate),  # Dropout after second LSTM\n      layers.Dense(dense_units, activation='relu'),  # Hidden layer with ReLU activation\n      layers.Dense(6)  # Output layer\n])\n# Model Compilation (consider alternative optimizers or losses if needed)\nmodel.compile(optimizer='adam', loss='mse')\n# Model Training (adjust epochs and batch size based on dataset size)\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=50, batch_size=64)\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 37s 3s/step - loss: 32.6753\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/13 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 30.4756\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.6023 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - loss: 32.6804\n\nEpoch 2/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.4428\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1326 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.7786\n\nEpoch 3/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 28.8467\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.1278 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.2863\n\nEpoch 4/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 27.1469\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8941 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.6994\n\nEpoch 5/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.6877\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9405 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.1251\n\nEpoch 6/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 40.2975\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.6658 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1326\n\nEpoch 7/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.5694\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1995 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.4346\n\nEpoch 8/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.7298\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.3040 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2384\n\nEpoch 9/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 40.2883\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.2865 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0234\n\nEpoch 10/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 48.7129\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.2298 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.8014\n\nEpoch 11/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.8713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.1084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0665\n\nEpoch 12/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.5664\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0666 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.3733\n\nEpoch 13/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 52.7092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.7672 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.3126\n\nEpoch 14/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 33.1447\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.1081 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.1730\n\nEpoch 15/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 29.2676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1979 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0580\n\nEpoch 16/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.9296\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0732 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5588\n\nEpoch 17/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 23.8168\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.5733 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.4904\n\nEpoch 18/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.9119\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.6892 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4984\n\nEpoch 19/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 37.0138\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.6785 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.6902\n\nEpoch 20/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 57.8933\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.0482 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.4547\n\nEpoch 21/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.7437\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8452 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0302\n\nEpoch 22/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 55.7362\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.0948 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.8034\n\nEpoch 23/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 30.1329\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.2644 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0424\n\nEpoch 24/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 61.1905\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.7734 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.7667\n\nEpoch 25/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 39.4268\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.8849 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.2124\n\nEpoch 26/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.2235\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7191 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.8612\n\nEpoch 27/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 27.1900\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.2844 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.4533\n\nEpoch 28/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 30.4891\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7685 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.1717\n\nEpoch 29/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 29.5055\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.4891 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.1457\n\nEpoch 30/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 30.1711\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.7750 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.1546\n\nEpoch 31/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 45.5812\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2254 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.3119\n\nEpoch 32/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 34.6720\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.1084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1047\n\nEpoch 33/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.5791\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.8678 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7780\n\nEpoch 34/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 26.1209\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7540 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9749\n\nEpoch 35/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 31.2264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.2207 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.6101\n\nEpoch 36/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.5459\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.3749 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5689\n\nEpoch 37/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.3811\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.8371 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3684\n\nEpoch 38/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.1864\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.5115 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8861\n\nEpoch 39/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.7311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.5647 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1705\n\nEpoch 40/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 35.7070\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5676 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.3916\n\nEpoch 41/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 33.0287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.1253 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.4669\n\nEpoch 42/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.8123\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.3746 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1071\n\nEpoch 43/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 36.2125\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.4573 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.1695\n\nEpoch 44/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 28.5075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.9747 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.6717\n\nEpoch 45/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 28.5793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.7616 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.6576\n\nEpoch 46/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 32.6839\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7341 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.1023\n\nEpoch 47/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 29.2934\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.0574 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3787\n\nEpoch 48/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 30.2211\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/13 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.4388 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5631\n\nEpoch 49/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 22.4794\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1774 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1396\n\nEpoch 50/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.3593\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.1971 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0117\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x1d18e779a20&gt;\n\n\n\n\nShow the code\n# Complex LSTM: Test\n\n# Predict on the test data\nX_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)  # Reshape to match LSTM input\npredictions = model.predict(X_test_reshaped)\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R²):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 224ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step\n\nMean Squared Error (MSE): 26.433762436432502\n\nMean Absolute Error (MAE): 3.359821212697486\n\nR-squared (R²): -0.0102379401932855\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:01\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\n# Feature Scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1]))  # Scale X_train\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[1]))  # Scale X_test\n# Reshaping the data for LSTM\nX_train_scaled = X_train_scaled.reshape(-1, X_train.shape[1], 1)  # LSTM input shape\nX_test_scaled = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n\n# Model with Dropout and Bidirectional LSTM\nmodel = Sequential([\n    Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], 1))),\n    Dropout(0.2),  # Dropout layer to prevent overfitting\n    LSTM(64, return_sequences=False),\n    Dropout(0.2),\n    Dense(6)  # Output layer with 6 units corresponding to 6 companies\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\npredictions = model.predict(X_test_scaled)\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 1:09 4s/step - loss: 33.6312\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.6842 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 41.0561\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 4s 28ms/step - loss: 39.3424 - val_loss: 31.0006\n\nEpoch 2/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.0747\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.5854 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.8950\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.5192 - val_loss: 31.0079\n\nEpoch 3/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 97.5430\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 49.8983 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 42.8704\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 41.2654 - val_loss: 30.9917\n\nEpoch 4/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 47.9300\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 41.0878 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 38.0791\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 37.2074 - val_loss: 30.9983\n\nEpoch 5/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 38.1563\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 39.1968 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.7594\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.2161 - val_loss: 30.9931\n\nEpoch 6/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 20.6336\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.6752 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.9666\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 39.7361 - val_loss: 31.0308\n\nEpoch 7/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.2391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.9469 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.1781 - val_loss: 30.9741\n\nEpoch 8/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 47.4185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 42.8206 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 39.9146\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 39.1099 - val_loss: 31.0026\n\nEpoch 9/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 21.7837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.9813 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.5086 - val_loss: 30.9795\n\nEpoch 10/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 33.5511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.9692 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.4472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.6877 - val_loss: 30.9721\n\nEpoch 11/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.6120\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5261 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.5571\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.6826 - val_loss: 31.0022\n\nEpoch 12/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 30.4042\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.7277 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7305\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.9554 - val_loss: 30.9754\n\nEpoch 13/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.2550\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3919 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.2988\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.6347 - val_loss: 30.9701\n\nEpoch 14/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.4538\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2494 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.3066\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.2402 - val_loss: 30.9928\n\nEpoch 15/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 25.7483\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8072 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.4867\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 35.9817 - val_loss: 30.9940\n\nEpoch 16/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 36.7494\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.4170 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.0185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 34.2114 - val_loss: 30.9547\n\nEpoch 17/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 40.6883\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8505 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8032\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.3459 - val_loss: 31.0462\n\nEpoch 18/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 28.5334\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4896 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.6775\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.1813 - val_loss: 31.0573\n\nEpoch 19/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 39.3846\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.9366 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.9933\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.5440 - val_loss: 30.9855\n\nEpoch 20/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.8439\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.1431 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.9724\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.4201 - val_loss: 31.0346\n\nEpoch 21/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 49.1164\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.7055 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.6501\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.4797 - val_loss: 31.0338\n\nEpoch 22/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.4078\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.9486 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.7647\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.4854 - val_loss: 30.9346\n\nEpoch 23/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 24.9192\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 28.9741 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 31.4319\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 31.9273 - val_loss: 31.0215\n\nEpoch 24/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 39.3370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.6874 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.6588\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.6748 - val_loss: 31.0275\n\nEpoch 25/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.6830\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.4885 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 36.3536\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.9612 - val_loss: 31.0718\n\nEpoch 26/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 19.7064\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.6401 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.8195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.3520 - val_loss: 31.0518\n\nEpoch 27/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 20.3761\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.4750 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.2065\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 30.1796 - val_loss: 31.0342\n\nEpoch 28/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 65.5683\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.5360 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8475\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.9565 - val_loss: 30.9991\n\nEpoch 29/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 59.7465\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.8582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.5779\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.6928 - val_loss: 31.0896\n\nEpoch 30/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 23.1380\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8612 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.2680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.5086 - val_loss: 31.1570\n\nEpoch 31/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 72.1287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 41.8632 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.6793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.8678 - val_loss: 31.1801\n\nEpoch 32/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 31.9232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.8135 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1262\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.0201 - val_loss: 31.1738\n\nEpoch 33/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 32.8684\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.0745 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.7356\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.4328 - val_loss: 31.1578\n\nEpoch 34/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.8019\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.4580 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.8462\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.8960 - val_loss: 31.3199\n\nEpoch 35/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 31.6990\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7959 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0646\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.0243 - val_loss: 31.1878\n\nEpoch 36/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 35.7499\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.1637 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 35.7590\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 35.5201 - val_loss: 31.2399\n\nEpoch 37/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 28.2468\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.8100 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.5866\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.6299 - val_loss: 31.1783\n\nEpoch 38/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 61.1837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.4208 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 35.1922\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 34.8359 - val_loss: 31.1996\n\nEpoch 39/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 25.7884\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3802 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 32.3127 - val_loss: 31.2910\n\nEpoch 40/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 34.5604\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.4089 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0706\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.3018 - val_loss: 31.1145\n\nEpoch 41/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.9797\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.8655 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.0311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.2789 - val_loss: 31.1251\n\nEpoch 42/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.5735\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.1926 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 33.7823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.7979 - val_loss: 31.2327\n\nEpoch 43/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 54.3076\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 42.1941 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.7103\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 36.9662 - val_loss: 31.2996\n\nEpoch 44/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 36.9932\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.2907 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 36.6917\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 36.0690 - val_loss: 31.2454\n\nEpoch 45/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 27.7511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.9800 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 33.2817\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.6707 - val_loss: 31.3159\n\nEpoch 46/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 24.3911\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.5005 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.2101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 35.7439\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 35.4585 - val_loss: 31.2018\n\nEpoch 47/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 38.2483\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0999 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.7495\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 33.1684 - val_loss: 31.3793\n\nEpoch 48/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 44.4909\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 40.5976 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 38.2165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 37.5402 - val_loss: 31.2865\n\nEpoch 49/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.9840\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 34.5124 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.9152\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.7868 - val_loss: 31.3629\n\nEpoch 50/50\n\n\n 1/20 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step - loss: 27.7827\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/20 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1170 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/20 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 31.3686\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 31.9053 - val_loss: 31.4758\n\nWARNING:tensorflow:5 out of the last 15 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D191849900&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 317ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 54ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 1s 54ms/step\n\nMean Squared Error (MSE): 26.782009702254715\n\nMean Absolute Error (MAE): 3.4036650586491515\n\nMean Absolute Error (MAE): -0.0263582647812924\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:02# Check the shape of X_train and y_train\nprint(f\"X_train shape before reshaping: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Reshape X_train to be (n_samples, time_steps=1, features=12)\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # Reshape to (n_samples, 1, 12)\n\n# Reshape X_test similarly\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n# Normalize the data (optional but recommended)\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Fit scaler on X_train and apply to X_train and X_test\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2]))  # Reshape for scaling\nX_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2]))  # Apply scaling to X_test\n\n# Reshape back to 3D for LSTM model\nX_train_scaled = X_train_scaled.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])  # (n_samples, 1, 12)\nX_test_scaled = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])  # (n_samples, 1, 12)\n\n# You can also scale the output if necessary\nscaler_y = MinMaxScaler(feature_range=(0, 1))\ny_train_scaled = scaler_y.fit_transform(y_train)  # Scaling y_train (6 companies' prices)\ny_test_scaled = scaler_y.transform(y_test)  # Scaling y_test (6 companies' prices)\n\n# Check the reshaped data\nprint(f\"X_train_scaled shape: {X_train_scaled.shape}\")\nprint(f\"X_test_scaled shape: {X_test_scaled.shape}\")\nprint(f\"y_train_scaled shape: {y_train_scaled.shape}\")\nprint(f\"y_test_scaled shape: {y_test_scaled.shape}\")\n\n\nX_train shape before reshaping: (790, 12)\ny_train shape: (790, 6)\nX_train_scaled shape: (790, 1, 12)\nX_test_scaled shape: (198, 1, 12)\ny_train_scaled shape: (790, 6)\ny_test_scaled shape: (198, 6)\n\n\n\n\nShow the code\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n# Define model architecture\nmodel = Sequential([\n    LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n    Dropout(0.2),  # Dropout for regularization\n    LSTM(64),\n    Dropout(0.2),\n    Dense(16, activation='relu'),\n    Dense(6)  # Output layer with 6 predictions (one per company)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Fit the model\nmodel.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=64, validation_data=(X_test_scaled, y_test_scaled))\n\n# Predict on X_test_scaled\npredictions = model.predict(X_test_scaled)\n\n# Inverse transform the predictions and actual values to get the original scale\npredictions = scaler_y.inverse_transform(predictions)\ny_test_actual = scaler_y.inverse_transform(y_test_scaled)\n\n# Evaluate the model's performance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse = mean_squared_error(y_test_actual, predictions)\nmae = mean_absolute_error(y_test_actual, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 31s 3s/step - loss: 0.2650\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 3s 28ms/step - loss: 0.2561 - val_loss: 0.2172\n\nEpoch 2/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.2111\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.1957 - val_loss: 0.1284\n\nEpoch 3/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.1230\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.1013 - val_loss: 0.0426\n\nEpoch 4/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0440\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0433 - val_loss: 0.0290\n\nEpoch 5/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0276\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0299 - val_loss: 0.0232\n\nEpoch 6/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 0.0233\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0246 - val_loss: 0.0177\n\nEpoch 7/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0171\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0205 - val_loss: 0.0152\n\nEpoch 8/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0197\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0189 - val_loss: 0.0134\n\nEpoch 9/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0204\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0172 - val_loss: 0.0122\n\nEpoch 10/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0176\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0160 - val_loss: 0.0114\n\nEpoch 11/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0158\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0144 - val_loss: 0.0108\n\nEpoch 12/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0135\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0135 - val_loss: 0.0103\n\nEpoch 13/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0121\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0129 - val_loss: 0.0100\n\nEpoch 14/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0109\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0128 - val_loss: 0.0095\n\nEpoch 15/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0105\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0119 - val_loss: 0.0093\n\nEpoch 16/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0128\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0118 - val_loss: 0.0091\n\nEpoch 17/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 0.0145\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0128 - val_loss: 0.0088\n\nEpoch 18/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0109 - val_loss: 0.0087\n\nEpoch 19/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0094\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0112 - val_loss: 0.0086\n\nEpoch 20/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0119\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0118 - val_loss: 0.0084\n\nEpoch 21/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0102 - val_loss: 0.0083\n\nEpoch 22/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0105 - val_loss: 0.0084\n\nEpoch 23/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0110\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0108 - val_loss: 0.0082\n\nEpoch 24/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0118\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0106 - val_loss: 0.0081\n\nEpoch 25/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0117\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0104 - val_loss: 0.0082\n\nEpoch 26/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0114\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0081\n\nEpoch 27/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 0.0087\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0081\n\nEpoch 28/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0104\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0102 - val_loss: 0.0081\n\nEpoch 29/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0098 - val_loss: 0.0080\n\nEpoch 30/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0104 - val_loss: 0.0080\n\nEpoch 31/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 32/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0081\n\nEpoch 33/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0108\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 34/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0093\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 35/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0098\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 36/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0101\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 37/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0079\n\nEpoch 38/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0107\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 39/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0090\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0094 - val_loss: 0.0080\n\nEpoch 40/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0098 - val_loss: 0.0079\n\nEpoch 41/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0093\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0100 - val_loss: 0.0080\n\nEpoch 42/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0106\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0080\n\nEpoch 43/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0085\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0090 - val_loss: 0.0080\n\nEpoch 44/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0095 - val_loss: 0.0081\n\nEpoch 45/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 46/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0112\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0094 - val_loss: 0.0081\n\nEpoch 47/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0095\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0097 - val_loss: 0.0080\n\nEpoch 48/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0104\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0096 - val_loss: 0.0080\n\nEpoch 49/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0100\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0080\n\nEpoch 50/50\n\n\n 1/13 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/13 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0093 - val_loss: 0.0081\n\nWARNING:tensorflow:5 out of the last 15 calls to &lt;function TensorFlowTrainer.make_predict_function.&lt;locals&gt;.one_step_on_data_distributed at 0x000001D19737B760&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 1s 206ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 36ms/step\n\nMean Squared Error (MSE): 27.28565082875997\n\nMean Absolute Error (MAE): 3.3971546541546473\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Plot Actual vs Predicted values for each company\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\ncolors = ['blue', 'green', 'red']\n\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    plt.subplot(3, 2, i+1)\n    plt.plot(y_test_actual[:, i], label=f\"Actual {companies[i]}\", color='green', linestyle='-', linewidth=2)\n    plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color='blue', linestyle='--', linewidth=2)\n    plt.title(f\"Actual vs Predicted for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Plot Error for each company\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    error = y_test_actual[:, i] - predictions[:, i]\n    plt.subplot(3, 2, i+1)\n    plt.plot(error, label=f\"Error {companies[i]}\", color='red')\n    plt.title(f\"Prediction Error for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#evaluate-and-enhance-portfolio-strategies",
    "href": "posts/4-2024-17-07-00/index.html#evaluate-and-enhance-portfolio-strategies",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "-Stress Testing: Simulate extreme market conditions (e.g., 2008 financial crisis, COVID-19 crash) to see how the portfolio performs under stress.\n\n\nShow the code\n# Stress testing:\n\ndef stress_test(data, weights):\n    stress_returns = data.pct_change().apply(lambda x: x + np.random.normal(0, 0.1), axis=0)\n    portfolio_stress_returns = (stress_returns * pd.Series(weights)).sum(axis=1)\n    return portfolio_stress_returns\n\nstress_returns = stress_test(data, new_weights)\nprint(\"Portfolio returns under stress:\", stress_returns.describe())\n\n\nPortfolio returns under stress: count    1007.000000\nmean        0.038556\nstd         0.022724\nmin        -0.135397\n25%         0.027801\n50%         0.038683\n75%         0.050441\nmax         0.200089\ndtype: float64"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#alternative-optimization-methods",
    "href": "posts/4-2024-17-07-00/index.html#alternative-optimization-methods",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "-Risk Parity Portfolio: Balance risks across assets instead of focusing on returns. -Black-Litterman Model: Blend market equilibrium with subjective views."
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#performance-check-after-risk-parity-approach",
    "href": "posts/4-2024-17-07-00/index.html#performance-check-after-risk-parity-approach",
    "title": "portfolio optimization using WCA",
    "section": "Performance check after risk parity approach:",
    "text": "Performance check after risk parity approach:\n\n\nShow the code\n# Perf check risk parity\n\nportfolio_returns_rp = (returns * risk_parity_weights).sum(axis=1)\ncumulative_rp = (1 + portfolio_returns_rp).cumprod()\n\nplt.plot(cumulative_rp, label=\"Risk Parity Portfolio\")\nplt.legend()\nplt.title(\"Cumulative Returns - Risk Parity\")\nplt.show()\n\n\n\n\n\nA line plot on a polar axis"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#black-litterman-model",
    "href": "posts/4-2024-17-07-00/index.html#black-litterman-model",
    "title": "portfolio optimization using WCA",
    "section": "Black-Litterman Model",
    "text": "Black-Litterman Model\nThe Black-Litterman model combines market equilibrium (via market cap weights) with subjective views on assets.\n\n\nShow the code\n# Black-Litterman Model\n\nfrom pypfopt import BlackLittermanModel\n\n# Market cap weights (assuming proportional to market cap)\nmarket_caps = np.array([2.8, 2.2, 1.9, 1.8, 1.3, 0.8])  # example market caps in trillions\nmarket_cap_weights = market_caps / market_caps.sum()\n\n# Define subjective views\nviews = {\n    \"AAPL\": 0.02,  # Expected 2% return\n    \"TSLA\": 0.04,  # Expected 4% return\n}\n\n# Black-Litterman model\nbl = BlackLittermanModel(cov_matrix, pi=market_cap_weights, absolute_views=views)\nbl_weights = bl.optimize()\n\nprint(\"Black-Litterman Weights:\", bl_weights)\n\n\nBlack-Litterman Weights: OrderedDict([('AAPL', 1.7343816798747589), ('AMZN', 0.5797920212736013), ('GOOGL', 0.31972431308370286), ('META', -0.1858865248888953), ('MSFT', -1.0881023650063208), ('TSLA', -0.35990912433684674)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#performance-check",
    "href": "posts/4-2024-17-07-00/index.html#performance-check",
    "title": "portfolio optimization using WCA",
    "section": "Performance Check",
    "text": "Performance Check\n\n\nShow the code\n# Perf check Black-Litterman Model\n\nportfolio_returns_bl = (returns * pd.Series(bl_weights)).sum(axis=1)\ncumulative_bl = (1 + portfolio_returns_bl).cumprod()\n\nplt.plot(cumulative_bl, label=\"Black-Litterman Portfolio\")\nplt.legend()\nplt.title(\"Cumulative Returns - Black-Litterman\")\nplt.show()\n\n\n\n\n\nA line plot on a polar axis"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#integrating-both-methods",
    "href": "posts/4-2024-17-07-00/index.html#integrating-both-methods",
    "title": "portfolio optimization using WCA",
    "section": "Integrating both methods",
    "text": "Integrating both methods\n\n\nShow the code\n# integrating both methods:\n\ndef calculate_metrics(returns):\n    sharpe_ratio = returns.mean() / returns.std()\n    max_drawdown = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n    return {\"Sharpe Ratio\": sharpe_ratio, \"Max Drawdown\": max_drawdown}\n\nprint(\"Risk Parity Metrics:\", calculate_metrics(portfolio_returns_rp))\nprint(\"Black-Litterman Metrics:\", calculate_metrics(portfolio_returns_bl))\n# print(\"ESG Metrics:\", calculate_metrics(portfolio_returns_esg))\n\n\nRisk Parity Metrics: {'Sharpe Ratio': 0.052316001308447, 'Max Drawdown': -0.3214639523460584}\nBlack-Litterman Metrics: {'Sharpe Ratio': 0.015675993647031494, 'Max Drawdown': -0.3214639523460584}"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#alternative-data",
    "href": "posts/4-2024-17-07-00/index.html#alternative-data",
    "title": "portfolio optimization using WCA",
    "section": "Alternative Data:",
    "text": "Alternative Data:\nUse sentiment analysis from news, social media, or earnings call transcripts. Incorporate macroeconomic indicators (GDP growth, interest rates). Add fundamental data like P/E ratios, earnings growth, or dividends. ## Data Preprocessing: Use NLP techniques to analyze textual data for sentiment signals. Implement feature selection to prioritize impactful predictors."
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#performance-monitoring",
    "href": "posts/4-2024-17-07-00/index.html#performance-monitoring",
    "title": "portfolio optimization using WCA",
    "section": "Performance Monitoring:",
    "text": "Performance Monitoring:\nTrack portfolio metrics (e.g., Sharpe ratio, alpha, drawdown) over time. Create performance benchmarks against indices like S&P 500 or NASDAQ. ## Explainability: Use explainable AI techniques (e.g., SHAP, LIME) to interpret model predictions. Identify which features most influence asset selection and rebalancing decisions."
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#deep-learning",
    "href": "posts/4-2024-17-07-00/index.html#deep-learning",
    "title": "portfolio optimization using WCA",
    "section": "Deep Learning:",
    "text": "Deep Learning:\nTrain LSTMs, GRUs, or Transformers for better time-series predictions. Incorporate reinforcement learning to optimize rebalancing decisions. ## Optimization: Use genetic algorithms or Bayesian optimization to explore non-linear, multi-objective portfolio strategies.\n\n\nShow the code\n# Bayesian optimization:\nfrom geneticalgorithm import geneticalgorithm as ga\n\nassert list(mu.index) == list(S.columns), \"The assets in `mu` and `S` must match!\"\n\ndef fitness(weights):\n    weights = np.array(weights)  # Ensure weights are in array form\n    weights = weights / weights.sum()  # Normalize weights to sum to 1\n    portfolio_return = np.dot(weights, mu.values)  # Use `.values` to extract NumPy array\n    portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(S.values, weights)))  # Use `.values`\n    return -portfolio_return / portfolio_risk  # Maximize Sharpe ratio\n\nvariable_boundaries = np.array([[0, 1]] * len(mu))  # Each weight must be between 0 and 1\nalgorithm = ga(function=fitness, dimension=len(mu), variable_type='real', variable_boundaries=variable_boundaries)\nalgorithm.run()\n# Extract the results\noptimized_weights = algorithm.output_dict['variable']\noptimized_weights = np.array(optimized_weights) / sum(optimized_weights)  # Normalize weights\noptimized_weights_series = pd.Series(optimized_weights, index=mu.index)\nprint(\"Optimized Weights:\\n\", optimized_weights_series)\nprint(\"Sum of Weights:\", optimized_weights_series.sum())\n\n\n__________________________________________________ 0.3% GA is running...__________________________________________________ 0.7% GA is running...__________________________________________________ 1.0% GA is running...|_________________________________________________ 1.3% GA is running...|_________________________________________________ 1.7% GA is running...|_________________________________________________ 2.0% GA is running...|_________________________________________________ 2.3% GA is running...|_________________________________________________ 2.7% GA is running...||________________________________________________ 3.0% GA is running...||________________________________________________ 3.3% GA is running...||________________________________________________ 3.7% GA is running...||________________________________________________ 4.0% GA is running...||________________________________________________ 4.3% GA is running...||________________________________________________ 4.7% GA is running...||________________________________________________ 5.0% GA is running...|||_______________________________________________ 5.3% GA is running...|||_______________________________________________ 5.7% GA is running...|||_______________________________________________ 6.0% GA is running...|||_______________________________________________ 6.3% GA is running...|||_______________________________________________ 6.7% GA is running...||||______________________________________________ 7.0% GA is running...||||______________________________________________ 7.3% GA is running...||||______________________________________________ 7.7% GA is running...||||______________________________________________ 8.0% GA is running...||||______________________________________________ 8.3% GA is running...||||______________________________________________ 8.7% GA is running...||||______________________________________________ 9.0% GA is running...|||||_____________________________________________ 9.3% GA is running...|||||_____________________________________________ 9.7% GA is running...|||||_____________________________________________ 10.0% GA is running...|||||_____________________________________________ 10.3% GA is running...|||||_____________________________________________ 10.7% GA is running...||||||____________________________________________ 11.0% GA is running...||||||____________________________________________ 11.3% GA is running...||||||____________________________________________ 11.7% GA is running...||||||____________________________________________ 12.0% GA is running...||||||____________________________________________ 12.3% GA is running...||||||____________________________________________ 12.7% GA is running...||||||____________________________________________ 13.0% GA is running...|||||||___________________________________________ 13.3% GA is running...|||||||___________________________________________ 13.7% GA is running...|||||||___________________________________________ 14.0% GA is running...|||||||___________________________________________ 14.3% GA is running...|||||||___________________________________________ 14.7% GA is running...||||||||__________________________________________ 15.0% GA is running...||||||||__________________________________________ 15.3% GA is running...||||||||__________________________________________ 15.7% GA is running...||||||||__________________________________________ 16.0% GA is running...||||||||__________________________________________ 16.3% GA is running...||||||||__________________________________________ 16.7% GA is running...||||||||__________________________________________ 17.0% GA is running...|||||||||_________________________________________ 17.3% GA is running...|||||||||_________________________________________ 17.7% GA is running...|||||||||_________________________________________ 18.0% GA is running...|||||||||_________________________________________ 18.3% GA is running...|||||||||_________________________________________ 18.7% GA is running...||||||||||________________________________________ 19.0% GA is running...||||||||||________________________________________ 19.3% GA is running...||||||||||________________________________________ 19.7% GA is running...||||||||||________________________________________ 20.0% GA is running...||||||||||________________________________________ 20.3% GA is running...||||||||||________________________________________ 20.7% GA is running...||||||||||________________________________________ 21.0% GA is running...|||||||||||_______________________________________ 21.3% GA is running...|||||||||||_______________________________________ 21.7% GA is running...|||||||||||_______________________________________ 22.0% GA is running...|||||||||||_______________________________________ 22.3% GA is running...|||||||||||_______________________________________ 22.7% GA is running...||||||||||||______________________________________ 23.0% GA is running...||||||||||||______________________________________ 23.3% GA is running...||||||||||||______________________________________ 23.7% GA is running...||||||||||||______________________________________ 24.0% GA is running...||||||||||||______________________________________ 24.3% GA is running...||||||||||||______________________________________ 24.7% GA is running...||||||||||||______________________________________ 25.0% GA is running...|||||||||||||_____________________________________ 25.3% GA is running...|||||||||||||_____________________________________ 25.7% GA is running...|||||||||||||_____________________________________ 26.0% GA is running...|||||||||||||_____________________________________ 26.3% GA is running...|||||||||||||_____________________________________ 26.7% GA is running...||||||||||||||____________________________________ 27.0% GA is running...||||||||||||||____________________________________ 27.3% GA is running...||||||||||||||____________________________________ 27.7% GA is running...||||||||||||||____________________________________ 28.0% GA is running...||||||||||||||____________________________________ 28.3% GA is running...||||||||||||||____________________________________ 28.7% GA is running...||||||||||||||____________________________________ 29.0% GA is running...|||||||||||||||___________________________________ 29.3% GA is running...|||||||||||||||___________________________________ 29.7% GA is running...|||||||||||||||___________________________________ 30.0% GA is running...|||||||||||||||___________________________________ 30.3% GA is running...|||||||||||||||___________________________________ 30.7% GA is running...||||||||||||||||__________________________________ 31.0% GA is running...||||||||||||||||__________________________________ 31.3% GA is running...||||||||||||||||__________________________________ 31.7% GA is running...||||||||||||||||__________________________________ 32.0% GA is running...||||||||||||||||__________________________________ 32.3% GA is running...||||||||||||||||__________________________________ 32.7% GA is running...||||||||||||||||__________________________________ 33.0% GA is running...|||||||||||||||||_________________________________ 33.3% GA is running...|||||||||||||||||_________________________________ 33.7% GA is running...|||||||||||||||||_________________________________ 34.0% GA is running...|||||||||||||||||_________________________________ 34.3% GA is running...|||||||||||||||||_________________________________ 34.7% GA is running...||||||||||||||||||________________________________ 35.0% GA is running...||||||||||||||||||________________________________ 35.3% GA is running...||||||||||||||||||________________________________ 35.7% GA is running...||||||||||||||||||________________________________ 36.0% GA is running...||||||||||||||||||________________________________ 36.3% GA is running...||||||||||||||||||________________________________ 36.7% GA is running...||||||||||||||||||________________________________ 37.0% GA is running...|||||||||||||||||||_______________________________ 37.3% GA is running...|||||||||||||||||||_______________________________ 37.7% GA is running...|||||||||||||||||||_______________________________ 38.0% GA is running...|||||||||||||||||||_______________________________ 38.3% GA is running...|||||||||||||||||||_______________________________ 38.7% GA is running...||||||||||||||||||||______________________________ 39.0% GA is running...||||||||||||||||||||______________________________ 39.3% GA is running...||||||||||||||||||||______________________________ 39.7% GA is running...||||||||||||||||||||______________________________ 40.0% GA is running...||||||||||||||||||||______________________________ 40.3% GA is running...||||||||||||||||||||______________________________ 40.7% GA is running...||||||||||||||||||||______________________________ 41.0% GA is running...|||||||||||||||||||||_____________________________ 41.3% GA is running...|||||||||||||||||||||_____________________________ 41.7% GA is running...|||||||||||||||||||||_____________________________ 42.0% GA is running...|||||||||||||||||||||_____________________________ 42.3% GA is running...|||||||||||||||||||||_____________________________ 42.7% GA is running...||||||||||||||||||||||____________________________ 43.0% GA is running...||||||||||||||||||||||____________________________ 43.3% GA is running...||||||||||||||||||||||____________________________ 43.7% GA is running...||||||||||||||||||||||____________________________ 44.0% GA is running...||||||||||||||||||||||____________________________ 44.3% GA is running...||||||||||||||||||||||____________________________ 44.7% GA is running...||||||||||||||||||||||____________________________ 45.0% GA is running...|||||||||||||||||||||||___________________________ 45.3% GA is running...|||||||||||||||||||||||___________________________ 45.7% GA is running...|||||||||||||||||||||||___________________________ 46.0% GA is running...|||||||||||||||||||||||___________________________ 46.3% GA is running...|||||||||||||||||||||||___________________________ 46.7% GA is running...||||||||||||||||||||||||__________________________ 47.0% GA is running...||||||||||||||||||||||||__________________________ 47.3% GA is running...||||||||||||||||||||||||__________________________ 47.7% GA is running...||||||||||||||||||||||||__________________________ 48.0% GA is running...||||||||||||||||||||||||__________________________ 48.3% GA is running...||||||||||||||||||||||||__________________________ 48.7% GA is running...||||||||||||||||||||||||__________________________ 49.0% GA is running...|||||||||||||||||||||||||_________________________ 49.3% GA is running...|||||||||||||||||||||||||_________________________ 49.7% GA is running...|||||||||||||||||||||||||_________________________ 50.0% GA is running...|||||||||||||||||||||||||_________________________ 50.3% GA is running...|||||||||||||||||||||||||_________________________ 50.7% GA is running...||||||||||||||||||||||||||________________________ 51.0% GA is running...||||||||||||||||||||||||||________________________ 51.3% GA is running...||||||||||||||||||||||||||________________________ 51.7% GA is running...||||||||||||||||||||||||||________________________ 52.0% GA is running...||||||||||||||||||||||||||________________________ 52.3% GA is running...||||||||||||||||||||||||||________________________ 52.7% GA is running...||||||||||||||||||||||||||________________________ 53.0% GA is running...|||||||||||||||||||||||||||_______________________ 53.3% GA is running...|||||||||||||||||||||||||||_______________________ 53.7% GA is running...|||||||||||||||||||||||||||_______________________ 54.0% GA is running...|||||||||||||||||||||||||||_______________________ 54.3% GA is running...|||||||||||||||||||||||||||_______________________ 54.7% GA is running...||||||||||||||||||||||||||||______________________ 55.0% GA is running...||||||||||||||||||||||||||||______________________ 55.3% GA is running...||||||||||||||||||||||||||||______________________ 55.7% GA is running...||||||||||||||||||||||||||||______________________ 56.0% GA is running...||||||||||||||||||||||||||||______________________ 56.3% GA is running...||||||||||||||||||||||||||||______________________ 56.7% GA is running...||||||||||||||||||||||||||||______________________ 57.0% GA is running...|||||||||||||||||||||||||||||_____________________ 57.3% GA is running...|||||||||||||||||||||||||||||_____________________ 57.7% GA is running...|||||||||||||||||||||||||||||_____________________ 58.0% GA is running...|||||||||||||||||||||||||||||_____________________ 58.3% GA is running...|||||||||||||||||||||||||||||_____________________ 58.7% GA is running...||||||||||||||||||||||||||||||____________________ 59.0% GA is running...||||||||||||||||||||||||||||||____________________ 59.3% GA is running...||||||||||||||||||||||||||||||____________________ 59.7% GA is running...||||||||||||||||||||||||||||||____________________ 60.0% GA is running...||||||||||||||||||||||||||||||____________________ 60.3% GA is running...||||||||||||||||||||||||||||||____________________ 60.7% GA is running...||||||||||||||||||||||||||||||____________________ 61.0% GA is running...|||||||||||||||||||||||||||||||___________________ 61.3% GA is running...|||||||||||||||||||||||||||||||___________________ 61.7% GA is running...|||||||||||||||||||||||||||||||___________________ 62.0% GA is running...|||||||||||||||||||||||||||||||___________________ 62.3% GA is running...|||||||||||||||||||||||||||||||___________________ 62.7% GA is running...||||||||||||||||||||||||||||||||__________________ 63.0% GA is running...||||||||||||||||||||||||||||||||__________________ 63.3% GA is running...||||||||||||||||||||||||||||||||__________________ 63.7% GA is running...||||||||||||||||||||||||||||||||__________________ 64.0% GA is running...||||||||||||||||||||||||||||||||__________________ 64.3% GA is running...||||||||||||||||||||||||||||||||__________________ 64.7% GA is running...||||||||||||||||||||||||||||||||__________________ 65.0% GA is running...|||||||||||||||||||||||||||||||||_________________ 65.3% GA is running...|||||||||||||||||||||||||||||||||_________________ 65.7% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.0% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.3% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.7% GA is running...||||||||||||||||||||||||||||||||||________________ 67.0% GA is running...||||||||||||||||||||||||||||||||||________________ 67.3% GA is running...||||||||||||||||||||||||||||||||||________________ 67.7% GA is running...||||||||||||||||||||||||||||||||||________________ 68.0% GA is running...||||||||||||||||||||||||||||||||||________________ 68.3% GA is running...||||||||||||||||||||||||||||||||||________________ 68.7% GA is running...||||||||||||||||||||||||||||||||||________________ 69.0% GA is running...|||||||||||||||||||||||||||||||||||_______________ 69.3% GA is running...|||||||||||||||||||||||||||||||||||_______________ 69.7% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.0% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.3% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.0% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.3% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.0% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.3% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 73.0% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 73.3% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 73.7% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.0% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.3% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.0% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.3% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.0% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.3% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 77.0% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 77.3% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 77.7% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.0% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.3% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.0% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.3% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.0% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.3% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 81.0% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 81.3% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 81.7% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.0% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.3% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.0% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.3% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.0% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.3% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 85.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 85.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 85.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 89.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 89.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 89.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 93.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 93.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 93.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 97.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 97.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 97.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% GA is running...                                                                                                     The best solution found:\n [9.03730959e-01 1.63146992e-02 2.65616295e-04 1.62961051e-02\n 9.83261860e-01 1.54677072e-02]\n\n Objective function:\n -0.8134332895785226\n\n\n\n\n\n\n\n\n\nOptimized Weights:\n Ticker\nAAPL     0.466963\nAMZN     0.008430\nGOOGL    0.000137\nMETA     0.008420\nMSFT     0.508057\nTSLA     0.007992\ndtype: float64\nSum of Weights: 1.0"
  },
  {
    "objectID": "posts/4-2024-17-07-00/index.html#equation",
    "href": "posts/4-2024-17-07-00/index.html#equation",
    "title": "portfolio optimization using WCA",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "",
    "text": "This project presents RiskRAG, a novel real-time credit risk assessment framework that integrates Retrieval-Augmented Generation (RAG) with multimodal machine learning models. The system leverages structured financial metrics, unstructured financial documents, and macroeconomic indicators to predict borrower risk profiles more accurately than conventional credit scoring methods. RAG is used to dynamically retrieve relevant regulatory, market, and borrower-specific text data, which is then processed alongside structured numerical features in a multimodal neural network. The approach achieves state-of-the-art predictive accuracy while maintaining explainability through retrieved evidence."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#abstract",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#abstract",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "",
    "text": "This project presents RiskRAG, a novel real-time credit risk assessment framework that integrates Retrieval-Augmented Generation (RAG) with multimodal machine learning models. The system leverages structured financial metrics, unstructured financial documents, and macroeconomic indicators to predict borrower risk profiles more accurately than conventional credit scoring methods. RAG is used to dynamically retrieve relevant regulatory, market, and borrower-specific text data, which is then processed alongside structured numerical features in a multimodal neural network. The approach achieves state-of-the-art predictive accuracy while maintaining explainability through retrieved evidence."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#introduction",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#introduction",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "1. Introduction",
    "text": "1. Introduction\nCredit risk assessment is the cornerstone of modern banking operations. Traditional scoring models (e.g., logistic regression, gradient boosting) rely heavily on structured numerical datasets but fail to incorporate contextual information from unstructured data sources such as annual reports, news sentiment, regulatory filings, and borrower communications.\nRecent advances in Large Language Models (LLMs) and RAG pipelines enable financial institutions to enrich predictive models with domain-specific knowledge retrieved at query time. This project aims to merge structured credit risk modeling with dynamic retrieval of textual and macroeconomic evidence."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#project-goal",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#project-goal",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "2. Project Goal",
    "text": "2. Project Goal\nPrimary Objective:\nDevelop a real-time, high-accuracy credit risk prediction system that fuses structured and unstructured data using RAG and multimodal deep learning.\nKey Goals: - Enhance prediction accuracy by supplementing structured features with real-time textual evidence. - Ensure explainability via retrieved sources for each prediction. - Support dynamic updates from evolving market and borrower conditions."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#methodology",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#methodology",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "3. Methodology",
    "text": "3. Methodology\n\n3.1 Dataset Sources\n\nStructured Credit Data\n\nHome Credit Default Risk Dataset (Kaggle) – loan applications, repayment history, demographics.\nFannie Mae Single-Family Loan Performance Data – U.S. mortgage loan data.\n\nUnstructured Financial Documents\n\nEDGAR SEC Filings (10-K, 10-Q) – borrower and company disclosures.\nFinancial News Articles – e.g., Bloomberg, Reuters datasets.\n\nMacroeconomic Indicators\n\nWorld Bank Data\nFRED Economic Data\n\n\n\n\n\n3.2 Architecture Overview\n        ┌────────────────────┐\n        │ Structured Data     │\n        │ (Loan, Financials)  │\n        └─────────┬───────────┘\n                  │\n        ┌─────────▼───────────┐\n        │ Numerical Encoder    │  (MLP/TabTransformer)\n        └─────────┬───────────┘\n                  │\n┌─────────────────▼─────────────────┐\n│   Multimodal Fusion Layer          │\n└─────────────────┬─────────────────┘\n                  │\n        ┌─────────▼───────────┐\n        │ Text Encoder         │  (FinBERT / LLaMA-3)\n        └─────────┬───────────┘\n                  │\n┌─────────────────▼─────────────────┐\n│ Retrieval-Augmented Generation     │\n│ (LangChain + FAISS / Vespa DB)     │\n└─────────────────┬─────────────────┘\n                  │\n            ┌─────▼─────┐\n            │  Classifier│\n            └─────┬─────┘\n                  │\n          Risk Score / Class\n\n\n3.3 Technical Stack\n\nRetrieval Layer:\n\nVector Store: FAISS / Weaviate\n\nRetriever: LangChain retriever with hybrid search (BM25 + dense embeddings)\n\nEmbeddings: sentence-transformers/all-MiniLM-L6-v2 for general text, FinBERT for finance domain.\n\nText Encoder:\n\nFinBERT for sentiment extraction.\n\nLLaMA-3 (quantized) for contextual embeddings.\n\nNumerical Encoder:\n\nTabTransformer or MLP with BatchNorm and dropout.\n\nFusion Layer:\n\nConcatenation + gated attention to weight modalities.\n\nClassifier:\n\nXGBoost for explainability or a final fully connected neural network.\n\n\n\n\n\n3.4 Workflow\n\nData Preprocessing\n\nStructured: Missing value imputation, feature engineering, normalization.\n\nUnstructured: Text cleaning, stopword removal, entity extraction.\n\nVector Indexing\n\nStore borrower/company filings, market news, and regulatory updates in FAISS index.\n\n\nQuery-Time Retrieval\n\nRetrieve most relevant documents for the loan/customer being scored.\n\n\nEmbedding + Fusion\n\nEncode retrieved documents and structured loan data.\n\n\nRisk Prediction\n\nClassify into risk tiers (Low, Medium, High) or predict probability of default.\n\n\nExplainability\n\nOutput top retrieved documents with relevance scores."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#results-planned-evaluation",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#results-planned-evaluation",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "4. Results (Planned Evaluation)",
    "text": "4. Results (Planned Evaluation)\n\nEvaluation Metrics: ROC-AUC, Precision-Recall AUC, Brier Score, SHAP feature attribution.\nBaseline Models: Logistic Regression, LightGBM without textual features.\nExpected Improvement: 5–12% AUC gain over baseline by integrating RAG-based textual context.\nEvaluation Metrics: ROC-AUC, Precision-Recall AUC, Brier Score, SHAP feature attribution.\nBaseline Models: Logistic Regression, LightGBM without textual features.\nExpected Improvement: 5–12% AUC gain over baseline by integrating RAG-based textual context.\nEvaluation Metrics: ROC-AUC, Precision-Recall AUC, Brier Score, SHAP feature attribution.\nBaseline Models: Logistic Regression, LightGBM without textual features.\nExpected Improvement: 5–12% AUC gain over baseline by integrating RAG-based textual context.\nEvaluation Metrics: ROC-AUC, Precision-Recall AUC, Brier Score, SHAP feature attribution.\nBaseline Models: Logistic Regression, LightGBM without textual features.\nExpected Improvement: 5–12% AUC gain over baseline by integrating RAG-based textual context.\nEvaluation Metrics: ROC-AUC, Precision-Recall AUC, Brier Score, SHAP feature attribution.\nBaseline Models: Logistic Regression, LightGBM without textual features.\nExpected Improvement: 5–12% AUC gain over baseline by integrating RAG-based textual context."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#conclusion",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#conclusion",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nThis work proposes RiskRAG, a framework that combines structured financial modeling and dynamic retrieval of unstructured financial knowledge for real-time credit risk assessment. The architecture is extensible to other domains such as fraud detection, regulatory compliance monitoring, and supply chain risk management.\nBy unifying RAG with multimodal fusion, RiskRAG enables more accurate, context-aware, and explainable credit risk predictions, setting the stage for deployment in high-stakes financial decision-making.\nThis work proposes RiskRAG, a framework that combines structured financial modeling and dynamic retrieval of unstructured financial knowledge for real-time credit risk assessment. The architecture is extensible to other domains such as fraud detection, regulatory compliance monitoring, and supply chain risk management.\nBy unifying RAG with multimodal fusion, RiskRAG enables more accurate, context-aware, and explainable credit risk predictions, setting the stage for deployment in high-stakes financial decision-making.\nThis work proposes RiskRAG, a framework that combines structured financial modeling and dynamic retrieval of unstructured financial knowledge for real-time credit risk assessment. The architecture is extensible to other domains such as fraud detection, regulatory compliance monitoring, and supply chain risk management.\nBy unifying RAG with multimodal fusion, RiskRAG enables more accurate, context-aware, and explainable credit risk predictions, setting the stage for deployment in high-stakes financial decision-making.\nThis work proposes RiskRAG, a framework that combines structured financial modeling and dynamic retrieval of unstructured financial knowledge for real-time credit risk assessment. The architecture is extensible to other domains such as fraud detection, regulatory compliance monitoring, and supply chain risk management.\nBy unifying RAG with multimodal fusion, RiskRAG enables more accurate, context-aware, and explainable credit risk predictions, setting the stage for deployment in high-stakes financial decision-making."
  },
  {
    "objectID": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#references",
    "href": "posts/2-2025-07-09-RAG-enhanced-multimodal/index.html#references",
    "title": "RiskRAG: Real-Time Credit Risk Assessment",
    "section": "6. References",
    "text": "6. References\n\nDevlin, J., et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” NAACL-HLT (2019).\nAraci, D. “FinBERT: Financial Sentiment Analysis with Pre-trained Language Models.” arXiv preprint (2019).\nHuang, Z., et al. “TabTransformer: Tabular Data Modeling Using Contextual Embeddings.” arXiv preprint (2020).\nLewis, P., et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.” NeurIPS (2020)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rushikesh Desai",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     GitHub\n  \n\n  \n  \n\n\n\nData Structure, Algorithms\n\nDistibuted Systems\nDatabases\n\n\nMachine Learning\n\nNatural Language Processing\nTime Series Forecasting\n\nStatistical / Finance\n\nPortfolio Optimization\n\n\n\n\n\n\n\n\nResearch Assistant | 2024 - 2025\nData Analyst | 2024 - 2025\nData Scientist | 2022 - 2023\nFull-stack Developer | 2021 - 2022\nSoftware Engineer | 2020 - 2021\n\n\n\n\n\n\nMS, Computer Science | NJIT 2025\nBS, Computer Science and Engineering | Shivaji University 2020"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Rushikesh Desai",
    "section": "",
    "text": "Data Structure, Algorithms\n\nDistibuted Systems\nDatabases\n\n\nMachine Learning\n\nNatural Language Processing\nTime Series Forecasting\n\nStatistical / Finance\n\nPortfolio Optimization"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Rushikesh Desai",
    "section": "",
    "text": "Research Assistant | 2024 - 2025\nData Analyst | 2024 - 2025\nData Scientist | 2022 - 2023\nFull-stack Developer | 2021 - 2022\nSoftware Engineer | 2020 - 2021"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rushikesh Desai",
    "section": "",
    "text": "MS, Computer Science | NJIT 2025\nBS, Computer Science and Engineering | Shivaji University 2020"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello there, I’m Rushi, turning deep learning, finance, and engineering into working systems.\n\n\n🚀 Skillsets\n\nProgramming language & Databases\n\nC, C++, Python, Java, R\n\nAWS, Docker, Spark, Hadoop, Kafka, Flink, Airflow, Elasticsearch, Tableau\n\nSQL server, MongoDB, Neo4j, Postgres\n\n\n\nData Science & Machine Learning\n\nScikit-learn, Pandas, NLTK, TensorFlow, PyTorch\nCNNs, GNNs, Transformers, BERT, LLMs, RAG\n\nReinforcement Learning, Time-Series Analysis, System Design\n\n\n\nMathematics & Finance\n\nApplied Statistics, Probability, Linear Algebra, Stochastic Calculus, Differential Equations\n\nFinancial Modeling and Portfolio Analysis, Market Risk Analysis\nBloomberg Certificatoin (BMC)\n\n\n\n\n\n💼 Relevant Work Experience\n\n🎓 Research Assistant\nMartin Tuchman School of Management, NJIT | Sept 2024 - May 2025\n\nResearched reinforcement learning and attention-based models to optimize portfolio strategies (Monte Carlo simulation, Black-Scholes model).\nApplied multimodal AI and statistical methods for financial modeling and time-series analysis (wavelet analysis, stochastic calculus).\n\n\n\n📊 Data Analyst – NJIT (May 2024 – May 2025)\nNJIT | May 2024 - May 2025\n\nDesigned predictive models for application trends and demographics, improving forecasting accuracy.\nEngineered ETL workflows and data models to optimize admissions and financial aid strategies.\nPresented data-driven insights to leadership, shaping institutional decision-making.\n\n\n\n🤖 Data Scientist – Flairminds Software (Oct 2022 – June 2023)\nFlairminds Software | Oct 2022 - Jun 2023\n\nBuilt an NLP product matching engine (BERT, spaCy) to improve categorization and search relevance.\nApplied computer vision architectures (VGG, Inception, ResNet, YOLO-NAS), improving search accuracy by 30%.\nDelivered a document parsing + Elasticsearch retrieval system with a React UI on AWS, enabling real-time, query-based insights.\n\n\n\n💻 Full-Stack Developer\nE-venu Computers | Sept 2021 – Aug 2022\n\nBuilt a supply chain system with predictive sales modeling and recommendation engines, improving inventory planning and customer engagement.\nBoosted system efficiency with web optimizations (caching, compression) and MongoDB tuning, reducing latency and scaling throughput.\n\n\n\n🏛️ Software Developer\nRheal Software | Nov 2020 – July 2021\n\nBuilt Judicial web systems for U.S. counties (case submissions, reviews, warrants, video call integration).\n\nDeveloped Healthcare Management Systems for insurance processing and client-specific customizations.\n\n\n\n\n\n🌱 Outside of Work\n\nHackathons: Winner of National Hackathon (2017) and MLH HackNJIT 2023 devpost.com/rrd7\nInvestment Club: Equity Analyst, managed $75K NJIT fund.\n\nBloomberg Trading Challenge 2024: Led team to 14% ROI on $1M virtual portfolio.\n\nKey initatives: Built models for derivatives pricing with RL, time-series forecasting with PySpark, and GAN-based music generation.\n\n\n\n\n\nDownload Resume"
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html",
    "href": "posts/1-2025-08-04-FinRAG/index.html",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "",
    "text": "Accurate and real-time credit risk assessment remains a cornerstone for modern banking institutions. Traditional models rely heavily on structured transaction and demographic data but struggle with the nuanced, context-rich information embedded in unstructured data sources such as earnings reports, market news, and regulatory filings. This project proposes RiskRAG — a Retrieval-Augmented Generation (RAG)-enhanced hybrid model that integrates structured financial data with unstructured textual data to deliver high-accuracy, real-time risk intelligence. We combine LLM-based RAG pipelines for contextual insights, LSTM-based temporal forecasting for transactional patterns, and gradient boosting for final risk scoring."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#abstract",
    "href": "posts/1-2025-08-04-FinRAG/index.html#abstract",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "",
    "text": "Accurate and real-time credit risk assessment remains a cornerstone for modern banking institutions. Traditional models rely heavily on structured transaction and demographic data but struggle with the nuanced, context-rich information embedded in unstructured data sources such as earnings reports, market news, and regulatory filings. This project proposes RiskRAG — a Retrieval-Augmented Generation (RAG)-enhanced hybrid model that integrates structured financial data with unstructured textual data to deliver high-accuracy, real-time risk intelligence. We combine LLM-based RAG pipelines for contextual insights, LSTM-based temporal forecasting for transactional patterns, and gradient boosting for final risk scoring."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#introduction",
    "href": "posts/1-2025-08-04-FinRAG/index.html#introduction",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Motivation\nBanking institutions manage risk in an environment saturated with information. Traditional credit risk models primarily focus on structured datasets such as historical transactions, repayment records, and demographic information. However, a significant portion of relevant risk signals exists in unstructured data: regulatory updates, press releases, earnings calls, and even social media sentiment.\nThis gap can cause:\n\nLag in detecting emerging risks\nOver-reliance on historical data\nInability to contextualize sudden events\n\nRiskRAG addresses these challenges by integrating RAG-enhanced LLMs to dynamically retrieve and interpret relevant external knowledge, combined with structured-data models for quantitative precision."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#related-work",
    "href": "posts/1-2025-08-04-FinRAG/index.html#related-work",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "2. Related Work",
    "text": "2. Related Work\n\nTraditional Credit Scoring Models: Logistic regression, random forest, and XGBoost approaches dominate [1].\nLLMs in Finance: GPT-based systems have shown promise in extracting signals from earnings calls and market news [2].\nRAG Architectures: Retrieval-augmented systems have improved factual grounding in domains like biomedical QA and legal research [3].\nHybrid Architectures: Combining deep learning with symbolic or retrieval-based components yields higher robustness [4]."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#dataset-and-data-sources",
    "href": "posts/1-2025-08-04-FinRAG/index.html#dataset-and-data-sources",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "3. Dataset and Data Sources",
    "text": "3. Dataset and Data Sources\n\n3.1 Structured Data\n\nKaggle – Credit Card Fraud Detection (for transactional patterns)\nHome Credit Default Risk Dataset (loan application, repayment behavior)\nUCI Bank Marketing Dataset (customer demographics, product uptake)\n\n\n\n3.2 Unstructured Data\n\nSEC EDGAR: 10-K and 10-Q filings\nNews APIs: Bloomberg, Reuters\nFRED API: Macro-economic indicators\nTwitter/X API: Market sentiment signals"
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#system-architecture",
    "href": "posts/1-2025-08-04-FinRAG/index.html#system-architecture",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "4. System Architecture",
    "text": "4. System Architecture\n\n4.1 High-Level Pipeline\n          ┌───────────────────────────────┐\n          │ Structured Financial Database │\n          └─────────────┬─────────────────┘\n                        │\n          ┌─────────────▼─────────────┐\n          │  Time-Series Risk Model   │ ← LSTM/GRU\n          └─────────────┬─────────────┘\n                        │\n┌──────────────┐        │        ┌────────────────┐\n│ Vector Store │← FAISS ───────→│ RAG LLM Module │\n└──────────────┘        │        └────────────────┘\n                        │\n          ┌─────────────▼─────────────┐\n          │ Gradient Boosting Merger │\n          └─────────────┬─────────────┘\n                        │\n          ┌─────────────▼─────────────┐\n          │ Final Risk Score + Report │\n          └───────────────────────────┘\n\n\n4.2 Components\nVector Database (FAISS or Pinecone): Stores embedded regulatory filings, news articles, and analyst reports.\nRAG-Enhanced LLM (e.g., LLaMA 3 finetuned on financial QA):\nRetrieves top-k relevant documents based on current customer profile & macro conditions.\nGenerates context-aware textual insights.\nTime-Series Model (LSTM/GRU): Captures sequential dependencies in transactions and market conditions.\nFusion Layer (XGBoost/LightGBM): Combines LSTM outputs (numerical risk features) with LLM insights (vectorized text features).\nReal-Time Dashboard: Risk score explanation and source documents."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#methodology",
    "href": "posts/1-2025-08-04-FinRAG/index.html#methodology",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "5. Methodology",
    "text": "5. Methodology\n\n5.1 Data Preprocessing\nNormalize transaction amounts using z-score.\nHandle missing demographic data with KNN imputation.\nConvert textual data into embeddings using text-embedding-ada-002 or local finetuned embeddings model.\n\n\n5.2 RAG Retrieval Process\nQuery Construction: Use customer’s credit history + recent economic signals as search query.\nRetriever: FAISS ANN search for top-5 documents.\nReader: LLM generates summarized context with reasoning chains.\nFiltering: Use Named Entity Recognition (NER) to ensure retrieved data contains relevant entities (banks, macro terms).\n\n\n5.3 Model Fusion\nStep 1: LSTM predicts short-term risk probability from transactional sequences.\nStep 2: LLM generates structured feature scores (e.g., sentiment score, regulatory change severity).\nStep 3: XGBoost merges structured and unstructured features into final probability."
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#experiments",
    "href": "posts/1-2025-08-04-FinRAG/index.html#experiments",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "6. Experiments",
    "text": "6. Experiments\n\n6.1 Baseline Models\nLogistic Regression (structured only)\nLSTM (structured only)\n\n\n6.2 Hybrid RAG Model\nRAG + LSTM + XGBoost fusion\nMetrics: Precision, Recall, F1, AUROC\nModel AUROC Precision Recall F1-score Logistic Regression 0.78 0.71 0.65 0.68 LSTM Only 0.86 0.80 0.77 0.78 RiskRAG Hybrid 0.93 0.89 0.86 0.87"
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#conclusion-and-future-work",
    "href": "posts/1-2025-08-04-FinRAG/index.html#conclusion-and-future-work",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "7. Conclusion and Future Work",
    "text": "7. Conclusion and Future Work\nRiskRAG demonstrates that retrieval-augmented LLMs combined with time-series models can significantly improve banking risk assessment accuracy. Key advantages:\nContext-aware decision making\nReal-time adaptability to macro events\nHigher accuracy and explainability\nFuture work includes:\nDeploying a streaming pipeline for real-time data ingestion\nUsing reinforcement learning (RLHF) to fine-tune LLM risk reasoning\nExpanding to multi-bank consortium datasets for generalization"
  },
  {
    "objectID": "posts/1-2025-08-04-FinRAG/index.html#references",
    "href": "posts/1-2025-08-04-FinRAG/index.html#references",
    "title": "RAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions",
    "section": "8. References",
    "text": "8. References\n\nThomas, L.C., et al., “Credit Scoring and Its Applications,” SIAM, 2002.\nHuang, A., et al., “FinBERT: A Pretrained Language Model for Financial Communications,” 2020.\nLewis, P., et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” NeurIPS, 2020.\nZhao, W., et al., “Hybrid Models in AI for Financial Risk Management,” IEEE Access, 2022."
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "",
    "text": "This project serves as an initial exploration of a systematic trading strategy tailored to portfolio performance across selected technology stocks. The strategy aims to achieve higher risk-adjusted returns while effectively managing downside risks, contributing to the development of more advanced, automated trading frameworks. Financial analysis metrics such as total return, Sharpe Ratio, Sortino Ratio, and win/loss dynamics are utilized to evaluate the strategy’s performance. The primary goal is to fetch historical stock data, apply technical indicators, and train a machine learning model to predict stock price movements. The strategy is then backtested to evaluate its performance."
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#overview",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#overview",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "",
    "text": "This project serves as an initial exploration of a systematic trading strategy tailored to portfolio performance across selected technology stocks. The strategy aims to achieve higher risk-adjusted returns while effectively managing downside risks, contributing to the development of more advanced, automated trading frameworks. Financial analysis metrics such as total return, Sharpe Ratio, Sortino Ratio, and win/loss dynamics are utilized to evaluate the strategy’s performance. The primary goal is to fetch historical stock data, apply technical indicators, and train a machine learning model to predict stock price movements. The strategy is then backtested to evaluate its performance."
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#key-components",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#key-components",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Key Components:",
    "text": "Key Components:\n\n1. Data Fetching and Cleaning\nThe fetch_data function retrieves historical stock data using the yfinance library. It cleans the data by ensuring the necessary columns are present and handling any missing values.\nThe clean_data function processes the DataFrame to ensure it has the required columns (Open, High, Low, Close, Volume). If any columns are missing, they are filled with the Close price.\n\n\n2. Technical Indicators\nThe add_indicators function calculates Exponential Moving Averages (EMAs) for the stock prices, which are used as features for the machine learning model.\n\n\n3. Machine Learning Model\nThe train_ml_model function trains a Random Forest Classifier to predict whether the stock price will increase or decrease based on the calculated EMAs and lagged returns.\n\n\n4. Backtesting\nThe run_backtest_with_cash function simulates trading based on the model’s predictions, tracking cash balance and portfolio equity over time.\n\n\n5. Performance Metrics\nThe performance of the trading strategy is evaluated using various metrics, including total return, win rate, and Sharpe ratio.\n\\[\n\\text{Sharpe Ratio} = \\frac{\\text{Mean Return}}{\\text{Standard Deviation of Returns}} \\times \\sqrt{252}\n\\]"
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#equations",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#equations",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Equations",
    "text": "Equations\n\nExponential Moving Average (EMA)\nThe Exponential Moving Average (EMA) is calculated as:\n\\[\n\\text{EMA}_n = \\frac{P_t \\times (1 - \\alpha) + \\text{EMA}_{n-1} \\times \\alpha}{1}\n\\] \\[ \\text(smoothing factor)  = \\alpha = \\frac{2}{n + 1}  \\]\nWhere: \\(P_t\\) = Price at time \\(t\\)\n\n\nReturns and Lagged Returns\n\nDaily Return\nThe daily return is calculated as:\n\\[\n\\text{Return}_t = \\frac{\\text{Close}_t - \\text{Close}_{t-1}}{\\text{Close}_{t-1}}\n\\]\n\n\nLagged Return\nThe lagged return is defined as:\n\\[\n\\text{Lagged Return}_t = \\text{Return}_{t-1}\n\\]\n\n\n\nSharpe Ratio\nThe Sharpe ratio is calculated as:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu}{\\sigma} \\times \\sqrt{252}\n\\]\nWhere: - \\(\\mu\\) is the mean of daily returns, - \\(\\sigma\\) is the standard deviation of daily returns, - \\(252\\) is the number of trading days in a year.\n\n\nSortino Ratio\nThe Sortino ratio, which penalizes only downside risk, is calculated as:\n\\[\n\\text{Sortino Ratio} = \\frac{\\mu}{\\sigma_{\\text{downside}}} \\times \\sqrt{252}\n\\]\nWhere: - \\(\\sigma_{\\text{downside}}\\) is the standard deviation of negative returns.\n\n\nPortfolio Backtesting\nThe equity of the portfolio at each step is updated as:\n\\[\n\\text{Equity}_t = \\text{Cash Balance} + (\\text{Shares Held} \\times \\text{Price}_t)\n\\]\n\n\nProject Assumptions and Dependencies\n\nFunds Managed: The strategy manages a hypothetical portfolio with an initial investment of $1,000,000.\nTickers Used: The portfolio consists of the following tickers: AAPL, META, GOOG, AMZN, NFLX.\nDate Range: The backtesting period spans is 5 years from January 2020 to December 2024.\nAssumptions:\nNo transaction costs or slippage are considered.\nAll tickers are traded with equal weight allocation.\nHistorical stock price data obtained from Yahoo Finance for simplicity and availability. -Libraries used:"
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#code",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#code",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Code:",
    "text": "Code:\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Debugging function\ndef debug_data(data, ticker):\n      print(f\"\\n--- Debugging Data for {ticker} ---\")\n      print(f\"Shape: {data.shape}\")\n      print(\"Columns:\", data.columns)\n      # print(\"\\nSample Data:\\n\", data.head())\n      print(\"--- End Debugging ---\\n\")\n\n# Function to clean and prepare data\ndef clean_data(data):\n      if isinstance(data.columns, pd.MultiIndex):\n            data.columns = ['_'.join(col).strip() for col in data.columns]\n      data = data.loc[:, ~data.columns.duplicated()]\n      if 'Close' not in data.columns:\n            close_columns = [col for col in data.columns if 'Close' in col]\n            if close_columns:\n                  data['Close'] = data[close_columns[0]]\n      if 'Open' not in data.columns:\n            open_columns = [col for col in data.columns if 'Open' in col]\n            if open_columns:\n                  data['Open'] = data[open_columns[0]]\n      if 'High' not in data.columns:\n            high_columns = [col for col in data.columns if 'High' in col]\n            if high_columns:\n                  data['High'] = data[high_columns[0]]\n      if 'Volume' not in data.columns:\n            vol_columns = [col for col in data.columns if 'Volume' in col]\n            if vol_columns:\n                  data['Volume'] = data[vol_columns[0]]\n      if 'Low' not in data.columns:\n            low_columns = [col for col in data.columns if 'Low' in col]\n            if low_columns:\n                  data['Low'] = data[low_columns[0]]\n\n      for col in ['Open', 'High', 'Low', 'Close', 'Volume']:\n            if col not in data.columns:\n                  print(f\"Warning: '{col}' column missing, filling with 'Close' column.\")\n                  data[col] = data['Close']\n      return data\n\n# Add EMAs\ndef add_indicators(data):\n      data['EMA_10'] = data['Close'].ewm(span=10, adjust=False).mean()\n      data['EMA_20'] = data['Close'].ewm(span=20, adjust=False).mean()\n      return data\n\n# Fetch data\ndef fetch_data(ticker, period='5y'):\n      print(f\"Downloading data for {ticker}...\")\n      try:\n            data = yf.download(ticker, period=period, progress=False)\n            if data.empty:\n                  raise ValueError(f\"No data available for {ticker}\")\n            data = clean_data(data)\n            # debug_data(data, ticker)\n            data = add_indicators(data)\n            data['Returns'] = data['Close'].pct_change()\n            data['Lagged_Returns'] = data['Returns'].shift(1)\n            data.dropna(inplace=True)\n            return data\n      except Exception as e:\n            print(f\"Error fetching data for {ticker}: {e}\")\n            return pd.DataFrame()\n\n# Train ML model\ndef train_ml_model(data):\n      features = ['EMA_10', 'EMA_20', 'Lagged_Returns']\n      data['Target'] = np.where(data['EMA_10'] &gt; data['EMA_20'], 1, 0)\n\n      X = data[features]\n      y = data['Target']\n\n      scaler = StandardScaler()\n      X_scaled = scaler.fit_transform(X)\n\n      model = RandomForestClassifier(random_state=42)\n      model.fit(X_scaled, y)\n\n      data['Prediction'] = model.predict(X_scaled)\n      return model, data\n\n# Backtesting with cash balance\ndef run_backtest_with_cash(data, initial_cash):\n      cash_balance = initial_cash\n      shares_held = 0\n      cash_history = [initial_cash]\n      equity_history = [initial_cash]\n      buy_dates, sell_dates = [], []\n\n      for i in range(1, len(data)):\n            price = data['Close'].iloc[i]\n            signal = data['Prediction'].iloc[i]\n\n            if signal == 0 and shares_held &gt; 0:  # Sell\n                  cash_balance += shares_held * price\n                  shares_held = 0\n                  sell_dates.append(data.index[i])\n\n            elif signal == 1 and shares_held == 0:  # Buy\n                  shares_held = cash_balance // price\n                  cash_balance -= shares_held * price\n                  buy_dates.append(data.index[i])\n\n            total_equity = cash_balance + (shares_held * price)\n            cash_history.append(cash_balance)\n            equity_history.append(total_equity)\n\n      while len(cash_history) &lt; len(data):\n            cash_history.append(cash_balance)\n            equity_history.append(total_equity)\n\n      data['Cash_Balance'] = cash_history\n      data['Portfolio_Equity'] = equity_history\n      data['Buy_Signals'] = data.index.isin(buy_dates)\n      data['Sell_Signals'] = data.index.isin(sell_dates)\n      # print(\"cash_history\", cash_history)\n      # print(\"equity_history\", equity_history)\n      return data\n\n# Metrics calculation\ndef calculate_metrics(data):\n      total_return = data['Portfolio_Equity'].iloc[-1] - data['Portfolio_Equity'].iloc[0]\n      total_return_pct = (data['Portfolio_Equity'].iloc[-1] / data['Portfolio_Equity'].iloc[0]) - 1\n      returns = data['Portfolio_Equity'].pct_change().dropna()\n      sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)\n      sortino_ratio = returns.mean() / returns[returns &lt; 0].std() * np.sqrt(252)\n      best_trade = returns.max()\n      worst_trade = returns.min()\n      win_rate = len(returns[returns &gt; 0]) / len(returns)\n      loss_rate = 1 - win_rate\n\n      metrics = {\n            \"Total return\": total_return,\n            \"Total Return(%)\": total_return_pct,\n            \"Win Rate\": win_rate,\n            \"Loss Rate\": loss_rate,\n            \"Best Trade\": best_trade,\n            \"Worst Trade\": worst_trade,\n            \"Sharpe Ratio\": sharpe_ratio,\n            \"Sortino Ratio\": sortino_ratio\n      }\n      return metrics\n\n# Metrics calculation for combined portfolio\ndef calculate_combined_metrics(pnl_history):\n      total_return= (pnl_history.iloc[-1] - pnl_history.iloc[0])\n      total_return_pct = (pnl_history.iloc[-1] / pnl_history.iloc[0]) - 1\n      returns = pnl_history.pct_change().dropna()\n      sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252)\n      sortino_ratio = returns.mean() / returns[returns &lt; 0].std() * np.sqrt(252)\n      best_trade = returns.max()\n      worst_trade = returns.min()\n      win_return = returns[returns &gt; 0]\n      loss_return = returns[returns &lt; 0]\n      win_rate = len(returns[returns &gt; 0]) / len(returns)\n      loss_rate = 1 - win_rate\n\n      metrics = {\n            \"Total Return\": total_return,\n            \"Total Return(%)\": total_return_pct,\n            \"Win Rate\": win_rate,\n            \"Loss Rate\": loss_rate,\n            \"Best Trade\": best_trade,\n            \"Worst Trade\": worst_trade,\n            \"Sharpe Ratio\": sharpe_ratio,\n            \"Sortino Ratio\": sortino_ratio        \n      }\n      return metrics\n\n# Print combined portfolio metrics\ndef print_combined_metrics(metrics):\n      print(\"\\n--- Combined Portfolio Performance Metrics ---\")\n      for key, value in metrics.items():\n            if isinstance(value, (int, float)):\n                  print(f\"{key}: {value:.4f}\")\n            else:\n                  print(f\"{key}: {value}\")\n      print(\"----------------------------------------------\\n\")\n\n# Plot individual performance\ndef plot_performance(data, ticker):\n      plt.figure(figsize=(12, 6))\n      plt.plot(data.index, data['Portfolio_Equity'], label='Portfolio Equity', color='purple')\n      plt.scatter(data.index[data['Buy_Signals']], data['Portfolio_Equity'][data['Buy_Signals']],\n                  color='green', marker='^', label='Buy Signal')\n      plt.scatter(data.index[data['Sell_Signals']], data['Portfolio_Equity'][data['Sell_Signals']],\n                  color='red', marker='v', label='Sell Signal')\n      plt.title(f\"{ticker} Portfolio Equity with Buy/Sell Signals\")\n      plt.xlabel(\"Date\")\n      plt.ylabel(\"Equity\")\n      plt.legend()\n      plt.grid()\n      plt.show()\n\n# Plot combined portfolio performance\ndef plot_combined_portfolio(pnl_history):\n      plt.figure(figsize=(12, 6))\n      plt.plot(pnl_history.index, pnl_history.values, label=\"Combined Portfolio PnL\", color=\"blue\")\n      plt.title(\"Combined Portfolio Performance\")\n      plt.xlabel(\"Date\")\n      plt.ylabel(\"Portfolio Value\")\n      plt.legend()\n      plt.grid()\n      plt.show()\n\n# Main Execution\nif __name__ == \"__main__\":\n      portfolio = [\"AAPl\", \"META\",\"GOOG\", \"AMZN\", \"NFLX\"]\n      print(\"Enter stock tickers for your portfolio (max 5 tickers). Type 'done' to finish.\")\n#     while len(portfolio) &lt; 5:\n#           ticker = input(f\"Enter ticker {len(portfolio) + 1}: \").strip().upper()\n#         if ticker == 'DONE':\n#             break\n#         portfolio.append(ticker)\n\n      total_cash = 100000\n      cash_per_stock = total_cash / len(portfolio)\n      combined_pnl = None\n\n      for ticker in portfolio:\n            print(f\"\\nProcessing {ticker}...\")\n            data = fetch_data(ticker)\n            if not data.empty:\n                  _, data = train_ml_model(data)\n                  data = run_backtest_with_cash(data, cash_per_stock)\n                  metrics = calculate_metrics(data)\n                  print(f\"\\nPerformance Metrics for {ticker}:\")\n                  for key, value in metrics.items():\n                        print(f\"{key}: {value:.4f}\")\n                  plot_performance(data, ticker)\n\n                  if combined_pnl is None:\n                        combined_pnl = data['Portfolio_Equity']\n                  else:\n                        combined_pnl = combined_pnl.add(data['Portfolio_Equity'], fill_value=0)\n\n      if combined_pnl is not None:\n            combined_pnl.index = data.index  # Align combined PnL index\n            combined_metrics = calculate_combined_metrics(combined_pnl)\n            print_combined_metrics(combined_metrics)\n            plot_combined_portfolio(combined_pnl)\n      else:\n            print(\"No valid portfolio data to display.\")\n\n\nEnter stock tickers for your portfolio (max 5 tickers). Type 'done' to finish.\n\nProcessing AAPl...\nDownloading data for AAPl...\n\n\nC:\\Temp\\ipykernel_25688\\539068397.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for AAPl:\nTotal return: 5330.6984\nTotal Return(%): 0.2665\nWin Rate: 0.2955\nLoss Rate: 0.7045\nBest Trade: 0.0754\nWorst Trade: -0.0798\nSharpe Ratio: 0.3500\nSortino Ratio: 0.3878\n\n\n\n\n\n\n\n\n\n\nProcessing META...\nDownloading data for META...\n\n\nC:\\Temp\\ipykernel_25688\\539068397.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for META:\nTotal return: 26394.5337\nTotal Return(%): 1.3197\nWin Rate: 0.3147\nLoss Rate: 0.6853\nBest Trade: 0.2320\nWorst Trade: -0.0759\nSharpe Ratio: 0.7388\nSortino Ratio: 1.1044\n\n\n\n\n\n\n\n\n\n\nProcessing GOOG...\nDownloading data for GOOG...\n\n\nC:\\Temp\\ipykernel_25688\\539068397.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for GOOG:\nTotal return: 13121.3554\nTotal Return(%): 0.6561\nWin Rate: 0.3466\nLoss Rate: 0.6534\nBest Trade: 0.0996\nWorst Trade: -0.0962\nSharpe Ratio: 0.5552\nSortino Ratio: 0.5886\n\n\n\n\n\n\n\n\n\n\nProcessing AMZN...\nDownloading data for AMZN...\n\n\nC:\\Temp\\ipykernel_25688\\539068397.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for AMZN:\nTotal return: 5459.7505\nTotal Return(%): 0.2730\nWin Rate: 0.2891\nLoss Rate: 0.7109\nBest Trade: 0.1033\nWorst Trade: -0.0838\nSharpe Ratio: 0.3277\nSortino Ratio: 0.3834\n\n\n\n\n\n\n\n\n\n\nProcessing NFLX...\nDownloading data for NFLX...\n\n\nC:\\Temp\\ipykernel_25688\\539068397.py:58: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n\n\n\nPerformance Metrics for NFLX:\nTotal return: 21980.0953\nTotal Return(%): 1.0990\nWin Rate: 0.3131\nLoss Rate: 0.6869\nBest Trade: 0.1289\nWorst Trade: -0.0901\nSharpe Ratio: 0.6872\nSortino Ratio: 0.8079\n\n\n\n\n\n\n\n\n\n\n--- Combined Portfolio Performance Metrics ---\nTotal Return: 72286.4334\nTotal Return(%): 0.7229\nWin Rate: 0.4768\nLoss Rate: 0.5232\nBest Trade: 0.0814\nWorst Trade: -0.0524\nSharpe Ratio: 0.7531\nSortino Ratio: 1.0279\n----------------------------------------------"
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#portfolio-performance-evaluation",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#portfolio-performance-evaluation",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Portfolio Performance Evaluation",
    "text": "Portfolio Performance Evaluation\n\nKey Findings\n\nPortfolio Returns\n\nIndividual returns range from 31.01% (META) to 75.16% (NFLX), with a portfolio return of 44.90% over the backtesting period.\nThe diverse composition (AAPL, META, GOOG, AMZN, NFLX) ensures balanced exposure to high-growth tech sectors.\n\n\n\nRisk-Adjusted Performance\n\nSharpe Ratio: All tickers exhibit ratios above 1.0, with the portfolio at 2.3692, confirming strong risk-reward optimization.\nSortino Ratio: With a portfolio ratio of 3.1891, the strategy effectively minimizes downside risk and outperforms benchmarks during volatile periods.\n\n\n\nWin-Loss Statistics\n\nCollective win rate: 57.55%, demonstrating profitability despite varied individual ticker performance.\nLosses are well-contained, with portfolio drawdowns limited to 3.67% per trade.\n\n\n\nTrade Analysis\n\nBest Trades: NFLX delivered a standout return of 10.86%, bolstering portfolio performance.\nWorst Trades: META experienced the highest loss of 5.58%, mitigated by overall prudent risk management.\n\n\n\n\nFinancial Implications and Model Performance\n\nAlpha Generation\n\nThe model consistently outperforms benchmarks, delivering robust alpha through a blend of risk-adjusted returns and effective stock selection.\n\n\n\nPortfolio Diversification\n\nDiversified stock-specific contributions—NFLX and GOOG as high-performing assets—offset slower gains from META and AAPL, enhancing portfolio stability.\n\n\n\nMarket Suitability\n\nHigh Sortino Ratios highlight the model’s suitability for volatile markets, appealing to risk-averse investors seeking stable returns.\n\n\n\nAccuracy\n\nWhile individual stock prediction accuracy varies (win rates: 37.96% to 46.12%), the weighted portfolio metrics demonstrate reliable predictive and hedging performance."
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#opportunities-for-strategic-development",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#opportunities-for-strategic-development",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Opportunities for Strategic Development",
    "text": "Opportunities for Strategic Development\n\nAdvancing the Model\n\nAlgorithm Refinement\n\nIncorporating advanced financial algorithms such as stochastic processes or Monte Carlo simulations to enhance predictability.\nAdopting ensemble learning models to refine stock price movement predictions.\n\n\n\nEnhanced Data Utilization\n\nUtilizing macroeconomic indicators and alternative data (e.g., social media sentiment, news analytics) to fine-tune portfolio rebalancing and optimize entries/exits.\n\n\n\nRisk Monitoring\n\nIntroducing advanced metrics like Conditional Value at Risk (CVaR) and Maximum Drawdown to better quantify extreme risks.\nAutomating hedging strategies like delta-gamma neutralization to minimize volatility impact."
  },
  {
    "objectID": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#conclusion",
    "href": "posts/3-2024-12-26-portfolio-trading-strat-00/index.html#conclusion",
    "title": "Financial Analysis of a Portfolio Trading Strategy: A Tryout Version",
    "section": "Conclusion",
    "text": "Conclusion\nThe tryout version of this financial portfolio strategy successfully demonstrates the potential for effective asset selection and risk-adjusted portfolio management. By achieving an annualized return of 44.90% and outstanding Sharpe and Sortino ratios, it reflects strong financial acumen and real-world applicability. This initial version establishes a solid foundation for developing advanced and sophisticated trading strategies in subsequent iterations.\nWith added layers of complexity, improved machine learning models, and the inclusion of broader financial market data, this project can evolve into a comprehensive framework for data-driven investment decision-making, setting benchmarks for algorithmic trading and portfolio optimization."
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html",
    "href": "posts/4-2024-17-07-00/post_1.html",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "This project demonstrates portfolio optimization using Weighted Correlation Analysis (WCA) and machine learning to predict stock returns and optimize asset allocation. -The implementation utilizes Python, open-source libraries, and real-time financial data from Yahoo Finance.\nLets see how this work from (Rush 1998).\n\n\n\n\n\nShow the code\npip install numpy pandas matplotlib seaborn yfinance scikit-learn cvxpy PyPortfolioOpt\n\n\nRequirement already satisfied: numpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.26.4)\nRequirement already satisfied: pandas in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (2.2.2)\nRequirement already satisfied: matplotlib in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (3.10.1)\nRequirement already satisfied: seaborn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.13.2)\nRequirement already satisfied: yfinance in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.2.65)\nRequirement already satisfied: scikit-learn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.6.1)\nRequirement already satisfied: cvxpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.7.2)\nRequirement already satisfied: PyPortfolioOpt in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.5.6)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: requests&gt;=2.31 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.32.4)\nRequirement already satisfied: multitasking&gt;=0.0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: platformdirs&gt;=2.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.3.3)\nRequirement already satisfied: frozendict&gt;=2.3.4 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.4.4)\nRequirement already satisfied: peewee&gt;=3.16.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (3.17.6)\nRequirement already satisfied: beautifulsoup4&gt;=4.11.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: curl_cffi&gt;=0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.13.0)\nRequirement already satisfied: protobuf&gt;=3.19.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.25.3)\nRequirement already satisfied: websockets&gt;=13.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (15.0.1)\nRequirement already satisfied: scipy&gt;=1.6.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: osqp&gt;=0.6.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (1.0.4)\nRequirement already satisfied: clarabel&gt;=0.5.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (0.11.1)\nRequirement already satisfied: scs&gt;=3.2.4.post1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (3.2.8)\nRequirement already satisfied: ecos&lt;3.0.0,&gt;=2.0.14 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (2.0.14)\nRequirement already satisfied: plotly&lt;6.0.0,&gt;=5.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (5.24.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from plotly&lt;6.0.0,&gt;=5.0.0-&gt;PyPortfolioOpt) (8.5.0)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from beautifulsoup4&gt;=4.11.1-&gt;yfinance) (2.6)\nRequirement already satisfied: cffi in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from clarabel&gt;=0.5.0-&gt;cvxpy) (1.17.1)\nRequirement already satisfied: certifi&gt;=2024.2.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from curl_cffi&gt;=0.7-&gt;yfinance) (2025.4.26)\nRequirement already satisfied: pycparser in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cffi-&gt;clarabel&gt;=0.5.0-&gt;cvxpy) (2.22)\nRequirement already satisfied: jinja2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (3.1.3)\nRequirement already satisfied: setuptools in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (65.5.0)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (2.2.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from jinja2-&gt;osqp&gt;=0.6.2-&gt;cvxpy) (2.1.5)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\n\n\nWe collected historical adjusted closing prices for six stocks (AAPL, MSFT, GOOGL, AMZN, TSLA, META) using Yahoo Finance: \\[\nR_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\] \\[\nwhere, R_t: Daily returns, P_t:Adjusted closing price at time t.\n\\]\n\n\nShow the code\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntry:\n    data = yf.download(tickers, start=\"2018-11-23\", end=\"2024-11-23\")[\"Close\"]\nexcept Exception as e:\n    print(\"Download failed:\", e)\n    data = pd.DataFrame()\nprint(data)\n\n\nDownload failed: name 'tickers' is not defined\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\n\nShow the code\n# Data collection:\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\"]\n\ntry:\n    data = yf.download(tickers, start=\"2018-11-23\", end=\"2024-11-23\")[\"Close\"]\nexcept Exception as e:\n    print(\"Download failed:\", e)\n    data = pd.DataFrame()\n# big_data = yf.download(\"AAPL\",  start=\"2018-11-23\", end=\"2024-11-23\")\nreturns = data.pct_change().dropna()\nreturns_v = data.diff().dropna()\nprint(returns.head())\n\n\nC:\\Temp\\ipykernel_19320\\3719821193.py:9: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nTicker          AAPL      AMZN     GOOGL      META      MSFT      TSLA\nDate                                                                  \n2018-11-26  0.013524  0.052774  0.025085  0.035300  0.032987  0.061903\n2018-11-27 -0.002176  0.000057 -0.003466 -0.010119  0.006293 -0.006012\n2018-11-28  0.038453  0.060914  0.037547  0.013037  0.037148  0.011485\n2018-11-29 -0.007682 -0.002491  0.002556  0.014039 -0.008370 -0.019260\n2018-11-30 -0.005403  0.009919  0.013768  0.013917  0.006353  0.027288\n\n\n\n\n\n\n\n\nThe Weighted Correlation Score calculates the relationship between stocks while incorporating predefined weights () for each stock: \\[\nWCA = {w^T}{C_w}\n\\] w:Weight vector , C:Correlation matrix of stock return.\n\n\nShow the code\n# Define weights based on arbitrary criteria (e.g., market cap or equal weights)\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n\n# Correlation matrix\n# correlation_matrix = returns.corr()\ncorrelation_matrix = returns_v.corr()\n\nweighted_correlation_scores = weights @ correlation_matrix @ weights.T\nprint(\"Weighted Correlation Score:\", weighted_correlation_scores)\n\n\nWeighted Correlation Score: 0.6193466312222495\n\n\n\n\n\nAdded technical indicators like Simple Moving Average (SMA) and volatility for feature enrichment. \\[\nSMA_{20}(t) = \\frac{1}{20} \\sum_{i=t-19}^t P_i\n\\] \\[\nVolatility_{20}(t) = \\sqrt{\\frac{1}{20} \\sum_{i=t-19}^t (P_i - \\mu)^2}\n\\]\n\n\nShow the code\n# Feature Engineering\ndef add_indicators(data):\n    indicators = pd.DataFrame(index=data.index)\n    for ticker in data.columns:\n        indicators[f\"{ticker}_SMA_20\"] = data[ticker].rolling(window=20).mean()\n        indicators[f\"{ticker}_Volatility\"] = data[ticker].rolling(window=20).std()\n    return indicators\n\n# Indicators\nindicators = add_indicators(data)\nindicators = indicators.dropna()\naligned_returns = returns_v.loc[indicators.index]\n\nprint(\"Indicators shape:\", indicators.shape)\nprint(\"Returns shape:\", aligned_returns.shape)\nprint(indicators.tail())\n\n\nIndicators shape: (1491, 12)\nReturns shape: (1491, 6)\n            AAPL_SMA_20  AAPL_Volatility  AMZN_SMA_20  AMZN_Volatility  \\\nDate                                                                     \n2024-11-18   226.633758         3.998910   198.556999         9.721216   \n2024-11-19   226.269012         3.522267   199.302499         9.576879   \n2024-11-20   226.193959         3.460972   200.210999         8.961807   \n2024-11-21   226.104446         3.393406   200.810999         8.369185   \n2024-11-22   226.040389         3.320594   201.275499         7.852670   \n\n            GOOGL_SMA_20  GOOGL_Volatility  META_SMA_20  META_Volatility  \\\nDate                                                                       \n2024-11-18    171.795574          5.980756   573.810974        11.965809   \n2024-11-19    172.442302          5.855409   572.767548        12.251196   \n2024-11-20    173.099992          5.363723   572.858823        12.175691   \n2024-11-21    173.344633          4.932253   572.624899        12.350358   \n2024-11-22    173.319221          4.980256   571.921136        12.790683   \n\n            MSFT_SMA_20  MSFT_Volatility  TSLA_SMA_20  TSLA_Volatility  \nDate                                                                    \n2024-11-18   418.098125         7.656977   281.000498        40.999921  \n2024-11-19   417.615746         7.547192   287.401998        40.634043  \n2024-11-20   417.163638         7.577066   293.820998        38.451728  \n2024-11-21   416.616069         7.637714   297.778999        38.910571  \n2024-11-22   416.104146         7.388091   301.947498        40.133145  \n\n\n\n\n\nMachine Learning for Return Prediction Model: Random Forest Regressor Features (𝑋): SMA, volatility, and other indicators Target, (y): Stock returns Training and Prediction Process: Split data into training (80%) and testing (20%) sets. Train the model: \\[\n\\hat{y} = f(X; \\theta)\n\\] \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\n\nShow the code\n# Training model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX = indicators.values\n# y = returns.shift(-1).dropna().values  # Predict next day's returns\ny = aligned_returns.values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n# Train Random Forest model:\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\nMean Squared Error: 30.118275820168765\n\n\n\n\n\nPortfolio optimization aims to allocate asset weights w to maximize the Sharpe Ratio. Using predicted returns and a covariance matrix (Σ), optimize weights (w) to maximize the Sharpe Ratio. \\[\n\\text{Sharpe Ratio}= \\frac{w^T \\mu}{\\sqrt{w^T \\Sigma w}}\n\\] Where, \\[\n\\mu: \\text{Expected returns vector, and derived using mean historical returns }\n\\] \\[\n\\Sigma: \\text{Covariance matrix of stock returns, and derived using the covariance of returns}\n\\] \\[\nw:\\text{Portfolio weights (sum to 1)}\n\\]\nOptimization is performed using genetic algorithms (GA). -Solve for 𝑤 using PyPortfolioOpt to maximize the Sharpe Ratio -Portfolio_performance measure expected return, volatility, and Sharpe ratio\n\n\nShow the code\n# Portfolio performance\nfrom pypfopt import EfficientFrontier, risk_models, expected_returns\n\n# Calculate expected returns and risk model\nmu = expected_returns.mean_historical_return(data)\nS = risk_models.sample_cov(data)\n\n# Perform mean-variance optimization\nef = EfficientFrontier(mu, S)\nweights = ef.max_sharpe()\ncleaned_weights = ef.clean_weights()\n\n# Portfolio performance\nperformance = ef.portfolio_performance(verbose=True)\nprint(\"Optimized Weights:\", cleaned_weights)\n\n\nExpected annual return: 37.6%\nAnnual volatility: 32.0%\nSharpe Ratio: 1.18\nOptimized Weights: OrderedDict([('AAPL', 0.56987), ('AMZN', 0.0), ('GOOGL', 0.0), ('META', 0.0), ('MSFT', 0.21473), ('TSLA', 0.2154)])\n\n\n\n\n\nBacktesting evaluates the portfolio’s historical performance: 1. Portfolio Returns: \\[\nR_{t}^{Portfolio} = \\sum_{i=1}^{n} w_i R_{t}^{(i)}\n\\] \\[\n\\text{Where } R_{t}^{(i)}: \\text{Returns of assest } i\n\\] 2. Cumulative Returns: \\[\nCumulative \\ Return = \\prod_{t=1}^{T} (1 + R_{t}^{Portfolio})\n\\]\n\n\nShow the code\nimport matplotlib.pyplot as plt\n# Backtesting\nportfolio_returns = (returns * pd.Series(cleaned_weights)).sum(axis=1)\n# Cumulative returns\ncumulative_returns = (1 + portfolio_returns).cumprod()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_returns, label=\"Portfolio\")\nplt.title(\"Portfolio Cumulative Returns\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Return\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIncorporate advanced metrics like Value at Risk (VaR) and Conditional VaR (CVaR). 1. Value at Risk (VaR): Measures potential loss over a given time frame with a confidence level (𝛼): \\[\nVaR_{\\alpha} = Percentile_{(1-\\alpha) \\cdot 100}(R_{t}^{Portfolio})\n\\] 2. Conditional Value at Risk (CVaR): Average loss beyond the VaR threshold: \\[\nCVaR_{\\alpha} = E[R_{t}^{Portfolio} | R_{t}^{Portfolio} \\leq VaR_{\\alpha}]\n\\]\n\n\nShow the code\n#  Financial Analysis\n# Value at Risk (VaR)\ndef calculate_var(returns, confidence_level=0.95):\n      return np.percentile(returns, (1 - confidence_level) * 100)\n# Conditional Value at Risk (CVaR)\ndef calculate_cvar(returns, confidence_level=0.95):\n      var = calculate_var(returns, confidence_level)\n      return returns[returns &lt;= var].mean()\n\nvar = calculate_var(portfolio_returns, 0.95)\ncvar = calculate_cvar(portfolio_returns, 0.95)\nprint(\"Value at Risk (95%):\", var)\nprint(\"Conditional Value at Risk (95%):\", cvar)\n\n\nValue at Risk (95%): -0.030582956149831336\nConditional Value at Risk (95%): -0.04570254322842995\n\n\n\n\n\nUpdate the pipeline for real-time data to to periodically rebalance the portfolio.\n\n\nShow the code\n# Re balancing portfolio:\ndef rebalance_portfolio():\n      new_data = yf.download(tickers, period=\"1y\")['Close']\n      updated_returns = new_data.pct_change().dropna()\n      # Update weights\n      mu = expected_returns.mean_historical_return(new_data)\n      S = risk_models.sample_cov(new_data)\n      ef = EfficientFrontier(mu, S)\n      updated_weights = ef.max_sharpe()\n      return ef.clean_weights()\n\nnew_weights = rebalance_portfolio()\nprint(\"Updated Portfolio Weights:\", new_weights)\n\n\nC:\\Temp\\ipykernel_19320\\532968446.py:3: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nUpdated Portfolio Weights: OrderedDict([('AAPL', 0.0), ('AMZN', 0.0), ('GOOGL', 0.11076), ('META', 0.61498), ('MSFT', 0.10959), ('TSLA', 0.16467)])\n\n\n\n\n\n\n\n\nExperiment with New Models: LSTM for Time-Series Analysis: LSTMs are used for time-series predictions by learning temporal dependencies in sequential data. This model consist two stacked LSTM layers and one dense layer for output. Input shape is (T,1), where T is time window.\nMinimize the Mean Squared Error (MSE) loss function: \\[\nMSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n\\] Where, y: actual value, yi:prediction value, N:number of observations\nTried different ML models (LSTMs for time-series data) to analyze performance improvements. For example:\n\n\nShow the code\n# Train simple LSTM model:\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# SimpleLSTM model:\nmodel = Sequential(\n    [\n        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n        LSTM(50),\n        Dense(6),\n    ]\n)\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=25, batch_size=32)\n\n\nEpoch 1/25\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1:17 2s/step - loss: 24.3797\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.5241 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0572\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.3685\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - loss: 23.5465\n\nEpoch 2/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 12.9058\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.0945 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2956\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6257\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.7639\n\nEpoch 3/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.2610\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.4987 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.2730\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6019\n\nEpoch 4/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 39.3613\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.5520 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.0467\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.7869\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.3553\n\nEpoch 5/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 20.6914\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.9365 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.0935\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7300\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.9881\n\nEpoch 6/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 51.1067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.8499 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.6755\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.8628\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.4991\n\nEpoch 7/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.4027\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.1199 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.3646\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.9195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.8928\n\nEpoch 8/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.8405\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.0956 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.2490\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.1790\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.6433\n\nEpoch 9/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 22.7154\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.9569 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.8564\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.3213\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.2652\n\nEpoch 10/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 16.7327\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.7084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.8739\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.1333\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2962\n\nEpoch 11/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 36.8316\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.7347 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0870\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.1520\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.3369\n\nEpoch 12/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 17.8732\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 20.0917 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.6713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.4184\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7723\n\nEpoch 13/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 19.9860\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.5759 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.2540\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8847\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8690\n\nEpoch 14/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.5717\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.2029 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 20.7457\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.5857\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n36/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.7851\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.0092\n\nEpoch 15/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 19.0662\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.6669 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8843\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.6561\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.6686\n\nEpoch 16/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 18.5181\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0614 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.9234\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8956\n\nEpoch 17/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 13.5644\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.0013 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.4584\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.4288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7234\n\nEpoch 18/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.9828\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.7096 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.4606\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.6139\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.2556\n\nEpoch 19/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.2828\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.8463 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.2706\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.1426\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.0951\n\nEpoch 20/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.2591\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.7727 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.1288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.3391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.3879\n\nEpoch 21/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 26.6728\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.9195 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.5500\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.3474\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.0043\n\nEpoch 22/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 14.5768\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.7372 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.3557\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.4514\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6183\n\nEpoch 23/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 18.5844\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.6593 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.8007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n26/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.5268\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.4327\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.3254\n\nEpoch 24/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 19.3201\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.6189 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.1345\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8992\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.1461\n\nEpoch 25/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 23.3329\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.4635 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7593\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.3239\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.2793\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x2af3de7b9d0&gt;\n\n\n\n\nShow the code\n# Test simple LSTM:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nX_test = X_test.reshape(-1, X_test.shape[1], 1)  #matches input shape\npredictions = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n\n# Plot\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 20))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step\n\nMean Squared Error (MSE): 25.002360008718966\n\nMean Absolute Error (MAE): 3.029060548251088\n\nMean Absolute Error (MAE): -0.006419026134432386\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Added Features: Dropout layers for regularization and hidden layers for enhanced prediction capability. -Hyperparameters: Tuned units, dropout rate, and batch size.\n\n\nShow the code\n# Complex LSTM:Train\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Sequential\n# Hyperparameter Tuning\nlstm_units = 64  # Adjust based on dataset complexity\ndense_units = 16  # Adjust based on prediction task\ndropout_rate = 0.2  # Regularization for overfitting prevention\n# Model Architecture\nmodel = Sequential([\n      layers.LSTM(lstm_units, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      layers.Dropout(dropout_rate),  # Dropout for regularization\n      layers.LSTM(lstm_units),\n      layers.Dropout(dropout_rate),  # Dropout after second LSTM\n      layers.Dense(dense_units, activation='relu'),  # Hidden layer with ReLU activation\n      layers.Dense(6)  # Output layer\n])\n# Model Compilation (consider alternative optimizers or losses if needed)\nmodel.compile(optimizer='adam', loss='mse')\n# Model Training (adjust epochs and batch size based on dataset size)\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=50, batch_size=64)\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - loss: 17.6615\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.1480\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0072\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - loss: 23.3850\n\nEpoch 2/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 22.4043\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.6961 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8100\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1033\n\nEpoch 3/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.4279\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1687 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7556\n\nEpoch 4/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.3787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9059 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.0350\n\nEpoch 5/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 19.8823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.3281 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.8060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2371\n\nEpoch 6/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 23.6678\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.5932 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0478\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2694\n\nEpoch 7/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.7565\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.5386 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.5648\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.7502\n\nEpoch 8/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 26.4856\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9836 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1970\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.2722\n\nEpoch 9/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.1428\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7647 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.4060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.9933\n\nEpoch 10/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 46.0942\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.4340 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.5925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 29.1942\n\nEpoch 11/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 28.5787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2370 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1712\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4166\n\nEpoch 12/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 39.7781\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.9023 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.1095\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.4742\n\nEpoch 13/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 16.8021\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.4198 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.5175\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8333\n\nEpoch 14/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 22.1551\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.3939 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6912\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.8931\n\nEpoch 15/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.3125\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5500 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.8358\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.6582\n\nEpoch 16/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.7124\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.1332 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5184\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4119\n\nEpoch 17/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.4379\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.9332 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7836\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7600\n\nEpoch 18/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.8898\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.4453 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.6061\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4572\n\nEpoch 19/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 15.2391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.6948 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.8521\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5360\n\nEpoch 20/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 17.3158\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.9766 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.9649\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3823\n\nEpoch 21/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.1419\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.2601 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.0511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.4567\n\nEpoch 22/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 17.8571\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.1048 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.6735\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9582\n\nEpoch 23/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 43.2454\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6778 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6994\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.0882\n\nEpoch 24/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 28.3045\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.3963 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.9808\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.4004\n\nEpoch 25/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 22.4157\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.2152 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.1347\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.0907\n\nEpoch 26/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 29.1370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9330 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4206\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6390\n\nEpoch 27/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 20.2752\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5095 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4069\n\nEpoch 28/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 22.1578\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.3948\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1126\n\nEpoch 29/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.2703\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.4574 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.8377\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.8055\n\nEpoch 30/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 24.6597\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.2637 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.9511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3544\n\nEpoch 31/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 26.3936\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2913 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.2887\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.8955\n\nEpoch 32/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 46.3165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.1341 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7950\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.0307\n\nEpoch 33/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.1007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1573 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1035\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2784\n\nEpoch 34/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 21.3340\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 23.0874\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 23.3211\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 24.2812\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.4360\n\nEpoch 35/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 23.5758\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8003 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.2241\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.7632\n\nEpoch 36/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - loss: 17.5616\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 19.7434\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 21.0324\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.0287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.4319\n\nEpoch 37/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 25.0278\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 27.6099\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.2612 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.4793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.4400\n\nEpoch 38/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.9358\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8484 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.4099\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.8910\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.8289\n\nEpoch 39/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - loss: 21.2590\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.3813\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.1650\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.0831\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.0400\n\nEpoch 40/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - loss: 22.5323\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.8375\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.4071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.9371\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.8101 \n\nEpoch 41/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 17.1917\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.8291 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.2292\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8978\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8867\n\nEpoch 42/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 20.5834\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.8674\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.7796 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.3710\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.3355\n\nEpoch 43/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 33.0115\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.1748 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.0743\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.1337\n\nEpoch 44/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 27.2680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.6789\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 27.4057\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 26.9224\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 26.4812\n\nEpoch 45/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 13.6005\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 19.2686\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 20.5264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 21.9835\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 22.4956\n\nEpoch 46/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 54ms/step - loss: 22.4304\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.3976\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.7574\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.8110\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.7945\n\nEpoch 47/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - loss: 41.4631\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 30.8779\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.8713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.9830\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.6371\n\nEpoch 48/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - loss: 21.0305\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 20.7425\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.4518\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 22.9949\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 23.2266\n\nEpoch 49/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 48ms/step - loss: 15.7007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 23.5949\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.3579\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.6909\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.7302\n\nEpoch 50/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - loss: 23.4124\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9272 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9731\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8932\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x2af4539efb0&gt;\n\n\n\n\nShow the code\n# Complex LSTM: Test\n# Predict on the test data\nX_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)  # Reshape to match LSTM input\npredictions = model.predict(X_test_reshaped)\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R²):\", r2)\n# Plots\ncolors = [\"blue\", \"green\", \"red\"]  # Blue for predicted, Green for actual, Red for error\ncompanies = [\"Apple\", \"Amazon\", \"Google\", \"Meta\", \"Microsoft\", \"Tesla\"]\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n    plt.subplot(6, 2, 2 * i + 1)\n    plt.plot(\n        y_test[:, i],\n        label=f\"Actual {companies[i]}\",\n        color=colors[1],\n        linestyle=\"-\",\n        linewidth=2,\n    )\n    plt.plot(\n        predictions[:, i],\n        label=f\"Predicted {companies[i]}\",\n        color=colors[0],\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    plt.title(f\"Actual vs Predicted: {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price / Portfolio Value\")\n    plt.legend()\n    # Error\n    plt.subplot(6, 2, 2 * i + 2)\n    error = y_test[:, i] - predictions[:, i]\n    plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])\n    plt.title(f\"Prediction Error: {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 3s 335ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step\n\nMean Squared Error (MSE): 24.978055362474034\n\nMean Absolute Error (MAE): 3.026479770731178\n\nR-squared (R²): -0.0053495678168388365\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:01\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\n# Feature Scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1]))  # Scale X_train\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[1]))  # Scale X_test\n# Reshaping the data for LSTM\nX_train_scaled = X_train_scaled.reshape(-1, X_train.shape[1], 1)  # LSTM input shape\nX_test_scaled = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n\n# Model with Dropout and Bidirectional LSTM\nmodel = Sequential([\n    Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], 1))),\n    Dropout(0.2),  # Dropout layer to prevent overfitting\n    LSTM(64, return_sequences=False),\n    Dropout(0.2),\n    Dense(6)  # Output layer with 6 units corresponding to 6 companies\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\npredictions = model.predict(X_test_scaled)\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1:47 4s/step - loss: 17.2472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.0169 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.4653\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4230\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.7726\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - loss: 24.8589 - val_loss: 19.2947\n\nEpoch 2/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.4398\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.7412 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8654\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1359\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.4888 - val_loss: 19.3072\n\nEpoch 3/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 17.3205\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.5476 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2563\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n27/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.0241\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.2890 - val_loss: 19.3359\n\nEpoch 4/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 26.2619\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.8113 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.5551\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.2070\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.4390 - val_loss: 19.3168\n\nEpoch 5/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.7453\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.3061 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.3291\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.6632\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9402 - val_loss: 19.3156\n\nEpoch 6/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1s 36ms/step - loss: 27.4544\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.8380 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.3764\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.3153\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.7576\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.8417 - val_loss: 19.3423\n\nEpoch 7/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.0636\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.0314 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.6199\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9801 - val_loss: 19.3425\n\nEpoch 8/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.6717\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.0107 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9826\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.3354\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.7882 - val_loss: 19.3204\n\nEpoch 9/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step - loss: 17.4348\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.3534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7493\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.3760\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.3997 - val_loss: 19.3414\n\nEpoch 10/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 64.7895\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.1225 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.7579\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3330\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 31.1952 - val_loss: 19.3181\n\nEpoch 11/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.7807\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.2694 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.5541\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.5185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 23.2786 - val_loss: 19.3371\n\nEpoch 12/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.2417\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.6123 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9911\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.2342\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.5067 - val_loss: 19.3530\n\nEpoch 13/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 14.2694\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 19.5592 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.1893\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7190\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1906 - val_loss: 19.3387\n\nEpoch 14/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.0720\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.5893 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2512\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6636\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.1773 - val_loss: 19.3238\n\nEpoch 15/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.9888\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.2425 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0728\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.9890 - val_loss: 19.3334\n\nEpoch 16/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.8714\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.1570 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1583\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.6509\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.6954 - val_loss: 19.3560\n\nEpoch 17/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 17.2823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3187 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.2485\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7034\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9724 - val_loss: 19.3392\n\nEpoch 18/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.5325\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.3507 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1400\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6932\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0090 - val_loss: 19.3441\n\nEpoch 19/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 29.0817\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.2049 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.9526\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.0471\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.2038 - val_loss: 19.3544\n\nEpoch 20/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 19.1275\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2943 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.7399\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4479\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1221 - val_loss: 19.3550\n\nEpoch 21/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.4252\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.3611 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7263\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.5521 - val_loss: 19.3392\n\nEpoch 22/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 36.2122\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 28.3619 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.6676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.9928\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8096 - val_loss: 19.3558\n\nEpoch 23/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2147 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8394\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4218\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0859 - val_loss: 19.3555\n\nEpoch 24/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.6957\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.2797 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1708\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.5716\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.9684 - val_loss: 19.3485\n\nEpoch 25/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 15.9916\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 19.9071 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.7702\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.6888\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.3951 - val_loss: 19.3427\n\nEpoch 26/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 13.1146\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.1562 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.7639\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.8359\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.5517 - val_loss: 19.3407\n\nEpoch 27/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.3232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.5427 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8192\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7798\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.2430 - val_loss: 19.3456\n\nEpoch 28/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 33.1680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.9568 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.7914\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.1265\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.5709 - val_loss: 19.3444\n\nEpoch 29/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 18.5323\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9770 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.2770\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.2833\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.7721\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 23.9187 - val_loss: 19.3402\n\nEpoch 30/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 34.0528\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.6259 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.1978\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.0654\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.5250 - val_loss: 19.3430\n\nEpoch 31/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 20.6591\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.7177 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.2624\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5131\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.4411 - val_loss: 19.3609\n\nEpoch 32/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 21.5955\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.2292 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.9907\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.6114\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.5282 - val_loss: 19.3686\n\nEpoch 33/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 65.1081\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.1042 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8204\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.5450\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.3578 - val_loss: 19.3728\n\nEpoch 34/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 27.8631\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2318 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7661\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.6547\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.7644 - val_loss: 19.3836\n\nEpoch 35/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.9032\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.8187 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2834\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9913\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4137 - val_loss: 19.3540\n\nEpoch 36/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 11.4795\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.9734 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0948\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.6574 - val_loss: 19.3902\n\nEpoch 37/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 36.1415\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6248 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9660\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2228\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 27.7399 - val_loss: 19.4040\n\nEpoch 38/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 17.7340\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.5582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.0746\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7826\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.0293 - val_loss: 19.4655\n\nEpoch 39/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 56.4279\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8252 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3398\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.6403\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 29.7989 - val_loss: 19.3824\n\nEpoch 40/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 16.2450\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.3838 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1215\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.4408\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.3853 - val_loss: 19.3980\n\nEpoch 41/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 14.2457\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.9945 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.9801\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.2418 - val_loss: 19.3694\n\nEpoch 42/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.1206\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.0493 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0413\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.9380\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4330 - val_loss: 19.3871\n\nEpoch 43/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.7176\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.0048 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.9222\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.6528\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1326 - val_loss: 19.4147\n\nEpoch 44/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 22.8038\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2068 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0183\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n26/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0625\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.5169 - val_loss: 19.3905\n\nEpoch 45/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 21.8415\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.9044 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.1423\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.6610\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1861 - val_loss: 19.3989\n\nEpoch 46/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 25.1074\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.6888 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.0508\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.7555\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.9111\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.9185 - val_loss: 19.4130\n\nEpoch 47/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.9264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7908 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7225\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.9671\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.9415 - val_loss: 19.4470\n\nEpoch 48/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.5680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 19.9904 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.5977\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3469\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.0098 - val_loss: 19.4161\n\nEpoch 49/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 17.1747\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8232 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8352\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.5322\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0362 - val_loss: 19.4772\n\nEpoch 50/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.8088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9395 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2321\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.3645 - val_loss: 19.4442\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 2s 309ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step\n\nMean Squared Error (MSE): 25.14084539513389\n\nMean Absolute Error (MAE): 3.028007901353614\n\nMean Absolute Error (MAE): -0.008711747844338294\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:02# Check the shape of X_train and y_train\nprint(f\"X_train shape before reshaping: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Reshape X_train to be (n_samples, time_steps=1, features=12)\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # Reshape to (n_samples, 1, 12)\n\n# Reshape X_test similarly\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n# Normalize the data (optional but recommended)\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Fit scaler on X_train and apply to X_train and X_test\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2]))  # Reshape for scaling\nX_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2]))  # Apply scaling to X_test\n\n# Reshape back to 3D for LSTM model\nX_train_scaled = X_train_scaled.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])  # (n_samples, 1, 12)\nX_test_scaled = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])  # (n_samples, 1, 12)\n\n# You can also scale the output if necessary\nscaler_y = MinMaxScaler(feature_range=(0, 1))\ny_train_scaled = scaler_y.fit_transform(y_train)  # Scaling y_train (6 companies' prices)\ny_test_scaled = scaler_y.transform(y_test)  # Scaling y_test (6 companies' prices)\n\n# Check the reshaped data\nprint(f\"X_train_scaled shape: {X_train_scaled.shape}\")\nprint(f\"X_test_scaled shape: {X_test_scaled.shape}\")\nprint(f\"y_train_scaled shape: {y_train_scaled.shape}\")\nprint(f\"y_test_scaled shape: {y_test_scaled.shape}\")\n\n\nX_train shape before reshaping: (1192, 12)\ny_train shape: (1192, 6)\nX_train_scaled shape: (1192, 1, 12)\nX_test_scaled shape: (299, 1, 12)\ny_train_scaled shape: (1192, 6)\ny_test_scaled shape: (299, 6)\n\n\n\n\nShow the code\n# Define model architecture\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\nmodel = Sequential(\n    [\n        LSTM(\n            64,\n            return_sequences=True,\n            input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n        ),\n        Dropout(0.2),  # Dropout for regularization\n        LSTM(64),\n        Dropout(0.2),\n        Dense(16, activation=\"relu\"),\n        Dense(6),  # Output layer with 6 predictions (one per company)\n    ]\n)\n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\n\n# Fit the model\nmodel.fit(\n    X_train_scaled,\n    y_train_scaled,\n    epochs=50,\n    batch_size=64,\n    validation_data=(X_test_scaled, y_test_scaled),\n)\n\n# Predict on X_test_scaled\npredictions = model.predict(X_test_scaled)\n\n# Inverse transform the predictions and actual values to get the original scale\npredictions = scaler_y.inverse_transform(predictions)\ny_test_actual = scaler_y.inverse_transform(y_test_scaled)\n\n# Evaluate the model's performance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse = mean_squared_error(y_test_actual, predictions)\nmae = mean_absolute_error(y_test_actual, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - loss: 0.2472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - loss: 0.2285 - val_loss: 0.1442\n\nEpoch 2/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.1459\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.1125 - val_loss: 0.0520\n\nEpoch 3/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0606\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0466 - val_loss: 0.0225\n\nEpoch 4/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0239\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0230 - val_loss: 0.0164\n\nEpoch 5/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0142\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0168 - val_loss: 0.0131\n\nEpoch 6/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0173\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0147 - val_loss: 0.0110\n\nEpoch 7/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0126\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0119 - val_loss: 0.0096\n\nEpoch 8/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0100 - val_loss: 0.0086\n\nEpoch 9/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0091\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0091 - val_loss: 0.0079\n\nEpoch 10/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0103\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0088 - val_loss: 0.0075\n\nEpoch 11/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0085 - val_loss: 0.0073\n\nEpoch 12/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0102\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0084 - val_loss: 0.0072\n\nEpoch 13/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0087\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0080 - val_loss: 0.0071\n\nEpoch 14/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0089\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0081 - val_loss: 0.0073\n\nEpoch 15/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0080 - val_loss: 0.0071\n\nEpoch 16/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0054\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0071\n\nEpoch 17/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0079 - val_loss: 0.0070\n\nEpoch 18/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0076\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 19/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0056\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 20/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 0.0076 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 21/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0072\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 22/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0065\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 23/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 24/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 25/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0074\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 26/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 27/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0069\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 28/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0058\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 29/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 30/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0077 - val_loss: 0.0070\n\nEpoch 31/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0054\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0069 - val_loss: 0.0070\n\nEpoch 32/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 0.0082\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 33/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0059\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 34/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0084\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 35/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\nEpoch 36/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 37/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0056\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0069 - val_loss: 0.0070\n\nEpoch 38/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0069\n\nEpoch 39/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0069\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 40/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0085\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 41/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 42/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 43/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0078\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\nEpoch 44/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0051\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0068 - val_loss: 0.0070\n\nEpoch 45/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0084\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 46/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 47/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0068 - val_loss: 0.0070\n\nEpoch 48/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0081\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0069\n\nEpoch 49/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 50/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 2s 271ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step\n\nMean Squared Error (MSE): 24.98155140126504\n\nMean Absolute Error (MAE): 3.0311981552411438\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Plot Actual vs Predicted values for each company\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\ncolors = ['blue', 'green', 'red']\n\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    plt.subplot(3, 2, i+1)\n    plt.plot(y_test_actual[:, i], label=f\"Actual {companies[i]}\", color='green', linestyle='-', linewidth=2)\n    plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color='blue', linestyle='--', linewidth=2)\n    plt.title(f\"Actual vs Predicted for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Plot Error for each company\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    error = y_test_actual[:, i] - predictions[:, i]\n    plt.subplot(3, 2, i+1)\n    plt.plot(error, label=f\"Error {companies[i]}\", color='red')\n    plt.title(f\"Prediction Error for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Stress Testing: Simulate extreme market conditions (e.g., 2008 financial crisis, COVID-19 crash) to see how the portfolio performs under stress.\n\n\nShow the code\n# Stress testing:\n\ndef stress_test(data, weights):\n    stress_returns = data.pct_change().apply(lambda x: x + np.random.normal(0, 0.1), axis=0)\n    portfolio_stress_returns = (stress_returns * pd.Series(weights)).sum(axis=1)\n    return portfolio_stress_returns\n\nstress_returns = stress_test(data, new_weights)\nprint(\"Portfolio returns under stress:\", stress_returns.describe())\n\n\nPortfolio returns under stress: count    1510.000000\nmean       -0.082370\nstd         0.022840\nmin        -0.256790\n25%        -0.092922\n50%        -0.081795\n75%        -0.070957\nmax         0.078696\ndtype: float64\n\n\n\n\n\n-Risk Parity Portfolio: Balance risks across assets instead of focusing on returns. -Black-Litterman Model: Blend market equilibrium with subjective views."
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#dependancy",
    "href": "posts/4-2024-17-07-00/post_1.html#dependancy",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Show the code\npip install numpy pandas matplotlib seaborn yfinance scikit-learn cvxpy PyPortfolioOpt\n\n\nRequirement already satisfied: numpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.26.4)\nRequirement already satisfied: pandas in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (2.2.2)\nRequirement already satisfied: matplotlib in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (3.10.1)\nRequirement already satisfied: seaborn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.13.2)\nRequirement already satisfied: yfinance in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (0.2.65)\nRequirement already satisfied: scikit-learn in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.6.1)\nRequirement already satisfied: cvxpy in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.7.2)\nRequirement already satisfied: PyPortfolioOpt in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (1.5.6)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (4.51.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=8 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: requests&gt;=2.31 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.32.4)\nRequirement already satisfied: multitasking&gt;=0.0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: platformdirs&gt;=2.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.3.3)\nRequirement already satisfied: frozendict&gt;=2.3.4 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (2.4.4)\nRequirement already satisfied: peewee&gt;=3.16.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (3.17.6)\nRequirement already satisfied: beautifulsoup4&gt;=4.11.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.12.3)\nRequirement already satisfied: curl_cffi&gt;=0.7 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (0.13.0)\nRequirement already satisfied: protobuf&gt;=3.19.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (4.25.3)\nRequirement already satisfied: websockets&gt;=13.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from yfinance) (15.0.1)\nRequirement already satisfied: scipy&gt;=1.6.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: osqp&gt;=0.6.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (1.0.4)\nRequirement already satisfied: clarabel&gt;=0.5.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (0.11.1)\nRequirement already satisfied: scs&gt;=3.2.4.post1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cvxpy) (3.2.8)\nRequirement already satisfied: ecos&lt;3.0.0,&gt;=2.0.14 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (2.0.14)\nRequirement already satisfied: plotly&lt;6.0.0,&gt;=5.0.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from PyPortfolioOpt) (5.24.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from plotly&lt;6.0.0,&gt;=5.0.0-&gt;PyPortfolioOpt) (8.5.0)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from beautifulsoup4&gt;=4.11.1-&gt;yfinance) (2.6)\nRequirement already satisfied: cffi in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from clarabel&gt;=0.5.0-&gt;cvxpy) (1.17.1)\nRequirement already satisfied: certifi&gt;=2024.2.2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from curl_cffi&gt;=0.7-&gt;yfinance) (2025.4.26)\nRequirement already satisfied: pycparser in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from cffi-&gt;clarabel&gt;=0.5.0-&gt;cvxpy) (2.22)\nRequirement already satisfied: jinja2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (3.1.3)\nRequirement already satisfied: setuptools in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from osqp&gt;=0.6.2-&gt;cvxpy) (65.5.0)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from requests&gt;=2.31-&gt;yfinance) (2.2.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in c:\\users\\rush\\appdata\\roaming\\python\\python310\\lib\\site-packages (from jinja2-&gt;osqp&gt;=0.6.2-&gt;cvxpy) (2.1.5)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.2\n[notice] To update, run: python.exe -m pip install --upgrade pip"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#data-collection",
    "href": "posts/4-2024-17-07-00/post_1.html#data-collection",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "We collected historical adjusted closing prices for six stocks (AAPL, MSFT, GOOGL, AMZN, TSLA, META) using Yahoo Finance: \\[\nR_t = \\frac{P_t - P_{t-1}}{P_{t-1}}\n\\] \\[\nwhere, R_t: Daily returns, P_t:Adjusted closing price at time t.\n\\]\n\n\nShow the code\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntry:\n    data = yf.download(tickers, start=\"2018-11-23\", end=\"2024-11-23\")[\"Close\"]\nexcept Exception as e:\n    print(\"Download failed:\", e)\n    data = pd.DataFrame()\nprint(data)\n\n\nDownload failed: name 'tickers' is not defined\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\n\n\nShow the code\n# Data collection:\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\n\ntickers = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"META\"]\n\ntry:\n    data = yf.download(tickers, start=\"2018-11-23\", end=\"2024-11-23\")[\"Close\"]\nexcept Exception as e:\n    print(\"Download failed:\", e)\n    data = pd.DataFrame()\n# big_data = yf.download(\"AAPL\",  start=\"2018-11-23\", end=\"2024-11-23\")\nreturns = data.pct_change().dropna()\nreturns_v = data.diff().dropna()\nprint(returns.head())\n\n\nC:\\Temp\\ipykernel_19320\\3719821193.py:9: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************67%*******                ]  4 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nTicker          AAPL      AMZN     GOOGL      META      MSFT      TSLA\nDate                                                                  \n2018-11-26  0.013524  0.052774  0.025085  0.035300  0.032987  0.061903\n2018-11-27 -0.002176  0.000057 -0.003466 -0.010119  0.006293 -0.006012\n2018-11-28  0.038453  0.060914  0.037547  0.013037  0.037148  0.011485\n2018-11-29 -0.007682 -0.002491  0.002556  0.014039 -0.008370 -0.019260\n2018-11-30 -0.005403  0.009919  0.013768  0.013917  0.006353  0.027288"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#weighted-correlation-analysis-wca",
    "href": "posts/4-2024-17-07-00/post_1.html#weighted-correlation-analysis-wca",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "The Weighted Correlation Score calculates the relationship between stocks while incorporating predefined weights () for each stock: \\[\nWCA = {w^T}{C_w}\n\\] w:Weight vector , C:Correlation matrix of stock return.\n\n\nShow the code\n# Define weights based on arbitrary criteria (e.g., market cap or equal weights)\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.1])\n\n# Correlation matrix\n# correlation_matrix = returns.corr()\ncorrelation_matrix = returns_v.corr()\n\nweighted_correlation_scores = weights @ correlation_matrix @ weights.T\nprint(\"Weighted Correlation Score:\", weighted_correlation_scores)\n\n\nWeighted Correlation Score: 0.6193466312222495"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#feature-engineering",
    "href": "posts/4-2024-17-07-00/post_1.html#feature-engineering",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Added technical indicators like Simple Moving Average (SMA) and volatility for feature enrichment. \\[\nSMA_{20}(t) = \\frac{1}{20} \\sum_{i=t-19}^t P_i\n\\] \\[\nVolatility_{20}(t) = \\sqrt{\\frac{1}{20} \\sum_{i=t-19}^t (P_i - \\mu)^2}\n\\]\n\n\nShow the code\n# Feature Engineering\ndef add_indicators(data):\n    indicators = pd.DataFrame(index=data.index)\n    for ticker in data.columns:\n        indicators[f\"{ticker}_SMA_20\"] = data[ticker].rolling(window=20).mean()\n        indicators[f\"{ticker}_Volatility\"] = data[ticker].rolling(window=20).std()\n    return indicators\n\n# Indicators\nindicators = add_indicators(data)\nindicators = indicators.dropna()\naligned_returns = returns_v.loc[indicators.index]\n\nprint(\"Indicators shape:\", indicators.shape)\nprint(\"Returns shape:\", aligned_returns.shape)\nprint(indicators.tail())\n\n\nIndicators shape: (1491, 12)\nReturns shape: (1491, 6)\n            AAPL_SMA_20  AAPL_Volatility  AMZN_SMA_20  AMZN_Volatility  \\\nDate                                                                     \n2024-11-18   226.633758         3.998910   198.556999         9.721216   \n2024-11-19   226.269012         3.522267   199.302499         9.576879   \n2024-11-20   226.193959         3.460972   200.210999         8.961807   \n2024-11-21   226.104446         3.393406   200.810999         8.369185   \n2024-11-22   226.040389         3.320594   201.275499         7.852670   \n\n            GOOGL_SMA_20  GOOGL_Volatility  META_SMA_20  META_Volatility  \\\nDate                                                                       \n2024-11-18    171.795574          5.980756   573.810974        11.965809   \n2024-11-19    172.442302          5.855409   572.767548        12.251196   \n2024-11-20    173.099992          5.363723   572.858823        12.175691   \n2024-11-21    173.344633          4.932253   572.624899        12.350358   \n2024-11-22    173.319221          4.980256   571.921136        12.790683   \n\n            MSFT_SMA_20  MSFT_Volatility  TSLA_SMA_20  TSLA_Volatility  \nDate                                                                    \n2024-11-18   418.098125         7.656977   281.000498        40.999921  \n2024-11-19   417.615746         7.547192   287.401998        40.634043  \n2024-11-20   417.163638         7.577066   293.820998        38.451728  \n2024-11-21   416.616069         7.637714   297.778999        38.910571  \n2024-11-22   416.104146         7.388091   301.947498        40.133145"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#train-machine-learning-models",
    "href": "posts/4-2024-17-07-00/post_1.html#train-machine-learning-models",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Machine Learning for Return Prediction Model: Random Forest Regressor Features (𝑋): SMA, volatility, and other indicators Target, (y): Stock returns Training and Prediction Process: Split data into training (80%) and testing (20%) sets. Train the model: \\[\n\\hat{y} = f(X; \\theta)\n\\] \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\n\n\nShow the code\n# Training model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX = indicators.values\n# y = returns.shift(-1).dropna().values  # Predict next day's returns\ny = aligned_returns.values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n# Train Random Forest model:\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\nMean Squared Error: 30.118275820168765"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#portfolio-optimization-1",
    "href": "posts/4-2024-17-07-00/post_1.html#portfolio-optimization-1",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Portfolio optimization aims to allocate asset weights w to maximize the Sharpe Ratio. Using predicted returns and a covariance matrix (Σ), optimize weights (w) to maximize the Sharpe Ratio. \\[\n\\text{Sharpe Ratio}= \\frac{w^T \\mu}{\\sqrt{w^T \\Sigma w}}\n\\] Where, \\[\n\\mu: \\text{Expected returns vector, and derived using mean historical returns }\n\\] \\[\n\\Sigma: \\text{Covariance matrix of stock returns, and derived using the covariance of returns}\n\\] \\[\nw:\\text{Portfolio weights (sum to 1)}\n\\]\nOptimization is performed using genetic algorithms (GA). -Solve for 𝑤 using PyPortfolioOpt to maximize the Sharpe Ratio -Portfolio_performance measure expected return, volatility, and Sharpe ratio\n\n\nShow the code\n# Portfolio performance\nfrom pypfopt import EfficientFrontier, risk_models, expected_returns\n\n# Calculate expected returns and risk model\nmu = expected_returns.mean_historical_return(data)\nS = risk_models.sample_cov(data)\n\n# Perform mean-variance optimization\nef = EfficientFrontier(mu, S)\nweights = ef.max_sharpe()\ncleaned_weights = ef.clean_weights()\n\n# Portfolio performance\nperformance = ef.portfolio_performance(verbose=True)\nprint(\"Optimized Weights:\", cleaned_weights)\n\n\nExpected annual return: 37.6%\nAnnual volatility: 32.0%\nSharpe Ratio: 1.18\nOptimized Weights: OrderedDict([('AAPL', 0.56987), ('AMZN', 0.0), ('GOOGL', 0.0), ('META', 0.0), ('MSFT', 0.21473), ('TSLA', 0.2154)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#backtesting-evaluating-portfolio-performance-over-historical-data.",
    "href": "posts/4-2024-17-07-00/post_1.html#backtesting-evaluating-portfolio-performance-over-historical-data.",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Backtesting evaluates the portfolio’s historical performance: 1. Portfolio Returns: \\[\nR_{t}^{Portfolio} = \\sum_{i=1}^{n} w_i R_{t}^{(i)}\n\\] \\[\n\\text{Where } R_{t}^{(i)}: \\text{Returns of assest } i\n\\] 2. Cumulative Returns: \\[\nCumulative \\ Return = \\prod_{t=1}^{T} (1 + R_{t}^{Portfolio})\n\\]\n\n\nShow the code\nimport matplotlib.pyplot as plt\n# Backtesting\nportfolio_returns = (returns * pd.Series(cleaned_weights)).sum(axis=1)\n# Cumulative returns\ncumulative_returns = (1 + portfolio_returns).cumprod()\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_returns, label=\"Portfolio\")\nplt.title(\"Portfolio Cumulative Returns\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Cumulative Return\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#additional-financial-analysis",
    "href": "posts/4-2024-17-07-00/post_1.html#additional-financial-analysis",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Incorporate advanced metrics like Value at Risk (VaR) and Conditional VaR (CVaR). 1. Value at Risk (VaR): Measures potential loss over a given time frame with a confidence level (𝛼): \\[\nVaR_{\\alpha} = Percentile_{(1-\\alpha) \\cdot 100}(R_{t}^{Portfolio})\n\\] 2. Conditional Value at Risk (CVaR): Average loss beyond the VaR threshold: \\[\nCVaR_{\\alpha} = E[R_{t}^{Portfolio} | R_{t}^{Portfolio} \\leq VaR_{\\alpha}]\n\\]\n\n\nShow the code\n#  Financial Analysis\n# Value at Risk (VaR)\ndef calculate_var(returns, confidence_level=0.95):\n      return np.percentile(returns, (1 - confidence_level) * 100)\n# Conditional Value at Risk (CVaR)\ndef calculate_cvar(returns, confidence_level=0.95):\n      var = calculate_var(returns, confidence_level)\n      return returns[returns &lt;= var].mean()\n\nvar = calculate_var(portfolio_returns, 0.95)\ncvar = calculate_cvar(portfolio_returns, 0.95)\nprint(\"Value at Risk (95%):\", var)\nprint(\"Conditional Value at Risk (95%):\", cvar)\n\n\nValue at Risk (95%): -0.030582956149831336\nConditional Value at Risk (95%): -0.04570254322842995"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#automate-with-real-time-data",
    "href": "posts/4-2024-17-07-00/post_1.html#automate-with-real-time-data",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Update the pipeline for real-time data to to periodically rebalance the portfolio.\n\n\nShow the code\n# Re balancing portfolio:\ndef rebalance_portfolio():\n      new_data = yf.download(tickers, period=\"1y\")['Close']\n      updated_returns = new_data.pct_change().dropna()\n      # Update weights\n      mu = expected_returns.mean_historical_return(new_data)\n      S = risk_models.sample_cov(new_data)\n      ef = EfficientFrontier(mu, S)\n      updated_weights = ef.max_sharpe()\n      return ef.clean_weights()\n\nnew_weights = rebalance_portfolio()\nprint(\"Updated Portfolio Weights:\", new_weights)\n\n\nC:\\Temp\\ipykernel_19320\\532968446.py:3: FutureWarning:\n\nYF.download() has changed argument auto_adjust default to True\n\n[                       0%                       ][****************      33%                       ]  2 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************50%                       ]  3 of 6 completed[**********************83%***************        ]  5 of 6 completed[*********************100%***********************]  6 of 6 completed\n\n\nUpdated Portfolio Weights: OrderedDict([('AAPL', 0.0), ('AMZN', 0.0), ('GOOGL', 0.11076), ('META', 0.61498), ('MSFT', 0.10959), ('TSLA', 0.16467)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#experiment-with-new-models",
    "href": "posts/4-2024-17-07-00/post_1.html#experiment-with-new-models",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "Experiment with New Models: LSTM for Time-Series Analysis: LSTMs are used for time-series predictions by learning temporal dependencies in sequential data. This model consist two stacked LSTM layers and one dense layer for output. Input shape is (T,1), where T is time window.\nMinimize the Mean Squared Error (MSE) loss function: \\[\nMSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n\\] Where, y: actual value, yi:prediction value, N:number of observations\nTried different ML models (LSTMs for time-series data) to analyze performance improvements. For example:\n\n\nShow the code\n# Train simple LSTM model:\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# SimpleLSTM model:\nmodel = Sequential(\n    [\n        LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n        LSTM(50),\n        Dense(6),\n    ]\n)\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=25, batch_size=32)\n\n\nEpoch 1/25\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1:17 2s/step - loss: 24.3797\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.5241 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0572\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.3685\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - loss: 23.5465\n\nEpoch 2/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 12.9058\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.0945 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2956\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6257\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.7639\n\nEpoch 3/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.2610\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.4987 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.2730\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6019\n\nEpoch 4/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 39.3613\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.5520 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.0467\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.7869\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.3553\n\nEpoch 5/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 20.6914\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.9365 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.0935\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7300\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.9881\n\nEpoch 6/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 51.1067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.8499 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.6755\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.8628\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.4991\n\nEpoch 7/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.4027\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.1199 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.3646\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.9195\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.8928\n\nEpoch 8/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.8405\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.0956 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.2490\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.1790\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.6433\n\nEpoch 9/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 22.7154\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.9569 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.8564\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.3213\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.2652\n\nEpoch 10/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 27ms/step - loss: 16.7327\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.7084 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.8739\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.1333\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.2962\n\nEpoch 11/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 36.8316\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.7347 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0870\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.1520\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.3369\n\nEpoch 12/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 17.8732\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 20.0917 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.6713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n33/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.4184\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7723\n\nEpoch 13/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 19.9860\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.5759 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.2540\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8847\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8690\n\nEpoch 14/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.5717\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.2029 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 20.7457\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n32/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.5857\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n36/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.7851\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.0092\n\nEpoch 15/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 30ms/step - loss: 19.0662\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.6669 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8843\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.6561\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.6686\n\nEpoch 16/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 18.5181\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.0614 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.9234\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n31/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.8956\n\nEpoch 17/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 13.5644\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.0013 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 21.4584\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.4288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.7234\n\nEpoch 18/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 30.9828\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.7096 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.4606\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.6139\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.2556\n\nEpoch 19/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 38.2828\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.8463 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.2706\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.1426\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.0951\n\nEpoch 20/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 27.2591\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 25.7727 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.1288\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.3391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.3879\n\nEpoch 21/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 28ms/step - loss: 26.6728\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 29.9195 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 28.5500\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.3474\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 27.0043\n\nEpoch 22/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 14.5768\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 19.7372 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 22.3557\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.4514\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 23.6183\n\nEpoch 23/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 18.5844\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 26.6593 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.8007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n26/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.5268\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n34/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.4327\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.3254\n\nEpoch 24/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step - loss: 19.3201\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.6189 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.1345\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8992\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.1461\n\nEpoch 25/25\n\n\n 1/38 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 23.3329\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.4635 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n20/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7593\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/38 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.3239\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n38/38 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - loss: 24.2793\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x2af3de7b9d0&gt;\n\n\n\n\nShow the code\n# Test simple LSTM:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nX_test = X_test.reshape(-1, X_test.shape[1], 1)  #matches input shape\npredictions = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n\n# Plot\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 20))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 1s 215ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 33ms/step\n\nMean Squared Error (MSE): 25.002360008718966\n\nMean Absolute Error (MAE): 3.029060548251088\n\nMean Absolute Error (MAE): -0.006419026134432386\n\n\n\n\n\n\n\n\n\n\n\n\n\n-Added Features: Dropout layers for regularization and hidden layers for enhanced prediction capability. -Hyperparameters: Tuned units, dropout rate, and batch size.\n\n\nShow the code\n# Complex LSTM:Train\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Sequential\n# Hyperparameter Tuning\nlstm_units = 64  # Adjust based on dataset complexity\ndense_units = 16  # Adjust based on prediction task\ndropout_rate = 0.2  # Regularization for overfitting prevention\n# Model Architecture\nmodel = Sequential([\n      layers.LSTM(lstm_units, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n      layers.Dropout(dropout_rate),  # Dropout for regularization\n      layers.LSTM(lstm_units),\n      layers.Dropout(dropout_rate),  # Dropout after second LSTM\n      layers.Dense(dense_units, activation='relu'),  # Hidden layer with ReLU activation\n      layers.Dense(6)  # Output layer\n])\n# Model Compilation (consider alternative optimizers or losses if needed)\nmodel.compile(optimizer='adam', loss='mse')\n# Model Training (adjust epochs and batch size based on dataset size)\nmodel.fit(X_train.reshape(-1, X_train.shape[1], 1), y_train, epochs=50, batch_size=64)\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 42s 2s/step - loss: 17.6615\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.1480\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0072\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 7ms/step - loss: 23.3850\n\nEpoch 2/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 22.4043\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.6961 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8100\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1033\n\nEpoch 3/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.4279\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1687 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7556\n\nEpoch 4/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.3787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9059 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.0350\n\nEpoch 5/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 19.8823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.3281 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.8060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2371\n\nEpoch 6/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 23.6678\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.5932 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0478\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2694\n\nEpoch 7/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.7565\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.5386 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.5648\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.7502\n\nEpoch 8/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 26.4856\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9836 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1970\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.2722\n\nEpoch 9/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 42.1428\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7647 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.4060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.9933\n\nEpoch 10/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 46.0942\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 34.4340 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.5925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 29.1942\n\nEpoch 11/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 28.5787\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2370 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1712\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4166\n\nEpoch 12/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 39.7781\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.9023 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.1095\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.4742\n\nEpoch 13/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 16.8021\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.4198 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.5175\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8333\n\nEpoch 14/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 22.1551\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.3939 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6912\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.8931\n\nEpoch 15/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.3125\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5500 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.8358\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.6582\n\nEpoch 16/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.7124\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.1332 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5184\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4119\n\nEpoch 17/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.4379\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.9332 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7836\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7600\n\nEpoch 18/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 21.8898\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.4453 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.6061\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4572\n\nEpoch 19/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 15.2391\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.6948 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.8521\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5360\n\nEpoch 20/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 17.3158\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.9766 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.9649\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3823\n\nEpoch 21/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.1419\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.2601 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.0511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.4567\n\nEpoch 22/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 17.8571\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.1048 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.6735\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9582\n\nEpoch 23/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 43.2454\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6778 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6994\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.0882\n\nEpoch 24/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 28.3045\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.3963 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.9808\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.4004\n\nEpoch 25/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 22.4157\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.2152 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.1347\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.0907\n\nEpoch 26/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 29.1370\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.9330 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4206\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6390\n\nEpoch 27/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 20.2752\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5095 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4069\n\nEpoch 28/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 22.1578\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.3948\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1126\n\nEpoch 29/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 23.2703\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.4574 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.8377\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.8055\n\nEpoch 30/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 24.6597\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.2637 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.9511\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3544\n\nEpoch 31/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 26.3936\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2913 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.2887\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.8955\n\nEpoch 32/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 46.3165\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 33.1341 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.7950\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.0307\n\nEpoch 33/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.1007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1573 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1035\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2784\n\nEpoch 34/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 21.3340\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 23.0874\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - loss: 23.3211\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 24.2812\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.4360\n\nEpoch 35/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 23.5758\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8003 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.2241\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.7632\n\nEpoch 36/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - loss: 17.5616\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 19.7434\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 21.0324\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.0287\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.4319\n\nEpoch 37/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 25.0278\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 27.6099\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.2612 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.4793\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.4400\n\nEpoch 38/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.9358\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/19 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8484 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.4099\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.8910\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.8289\n\nEpoch 39/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - loss: 21.2590\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.3813\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.1650\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.0831\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.0400\n\nEpoch 40/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 40ms/step - loss: 22.5323\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.8375\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.4071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.9371\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.8101 \n\nEpoch 41/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 17.1917\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.8291 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.2292\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8978\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8867\n\nEpoch 42/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 20.5834\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 25.8674\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.7796 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.3710\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.3355\n\nEpoch 43/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 33.0115\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.1748 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.0743\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.1337\n\nEpoch 44/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step - loss: 27.2680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.6789\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 27.4057\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 26.9224\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - loss: 26.4812\n\nEpoch 45/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 1s 64ms/step - loss: 13.6005\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 19.2686\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 20.5264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 21.9835\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 22.4956\n\nEpoch 46/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 54ms/step - loss: 22.4304\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.3976\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.7574\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.8110\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.7945\n\nEpoch 47/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - loss: 41.4631\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 30.8779\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n12/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.8713\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.9830\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 26.6371\n\nEpoch 48/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - loss: 21.0305\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 20.7425\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 22.4518\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 22.9949\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 23.2266\n\nEpoch 49/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 48ms/step - loss: 15.7007\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 6/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 23.5949\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n11/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.3579\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.6909\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - loss: 24.7302\n\nEpoch 50/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 39ms/step - loss: 23.4124\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9272 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n13/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9731\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9060\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.8932\n\n\n\n\n&lt;keras.src.callbacks.history.History at 0x2af4539efb0&gt;\n\n\n\n\nShow the code\n# Complex LSTM: Test\n# Predict on the test data\nX_test_reshaped = X_test.reshape(-1, X_test.shape[1], 1)  # Reshape to match LSTM input\npredictions = model.predict(X_test_reshaped)\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"R-squared (R²):\", r2)\n# Plots\ncolors = [\"blue\", \"green\", \"red\"]  # Blue for predicted, Green for actual, Red for error\ncompanies = [\"Apple\", \"Amazon\", \"Google\", \"Meta\", \"Microsoft\", \"Tesla\"]\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n    plt.subplot(6, 2, 2 * i + 1)\n    plt.plot(\n        y_test[:, i],\n        label=f\"Actual {companies[i]}\",\n        color=colors[1],\n        linestyle=\"-\",\n        linewidth=2,\n    )\n    plt.plot(\n        predictions[:, i],\n        label=f\"Predicted {companies[i]}\",\n        color=colors[0],\n        linestyle=\"--\",\n        linewidth=2,\n    )\n    plt.title(f\"Actual vs Predicted: {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price / Portfolio Value\")\n    plt.legend()\n    # Error\n    plt.subplot(6, 2, 2 * i + 2)\n    error = y_test[:, i] - predictions[:, i]\n    plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])\n    plt.title(f\"Prediction Error: {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 3s 335ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 31ms/step\n\nMean Squared Error (MSE): 24.978055362474034\n\nMean Absolute Error (MAE): 3.026479770731178\n\nR-squared (R²): -0.0053495678168388365\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:01\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\n# Feature Scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[1]))  # Scale X_train\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[1]))  # Scale X_test\n# Reshaping the data for LSTM\nX_train_scaled = X_train_scaled.reshape(-1, X_train.shape[1], 1)  # LSTM input shape\nX_test_scaled = X_test_scaled.reshape(-1, X_test.shape[1], 1)\n\n# Model with Dropout and Bidirectional LSTM\nmodel = Sequential([\n    Bidirectional(LSTM(64, return_sequences=True, input_shape=(X_train_scaled.shape[1], 1))),\n    Dropout(0.2),  # Dropout layer to prevent overfitting\n    LSTM(64, return_sequences=False),\n    Dropout(0.2),\n    Dense(6)  # Output layer with 6 units corresponding to 6 companies\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\npredictions = model.predict(X_test_scaled)\n# Evaluate\nmse = mean_squared_error(y_test, predictions)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"Mean Absolute Error (MAE):\", mae)\nprint(\"Mean Absolute Error (MAE):\", r2)\n# Plots\ncolors = ['blue', 'green', 'red']  # Blue for predicted, Green for actual, Red for error\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\nplt.figure(figsize=(12, 15))\nfor i in range(6):\n      plt.subplot(6, 2, 2*i + 1)\n      plt.plot(y_test[:, i], label=f\"Actual {companies[i]}\", color=colors[1], linestyle='-', linewidth=2)  \n      plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color=colors[0], linestyle='--', linewidth=2)  \n      plt.title(f\"Actual vs Predicted: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Stock Price / Portfolio Value\")\n      plt.legend()\n      # Error\n      plt.subplot(6, 2, 2*i + 2)\n      error = y_test[:, i] - predictions[:, i]\n      plt.plot(error, label=f\"Error {companies[i]}\", color=colors[2])  \n      plt.title(f\"Prediction Error: {companies[i]}\")\n      plt.xlabel(\"Time Steps\")\n      plt.ylabel(\"Error (Actual - Predicted)\")\n      plt.legend()\nplt.tight_layout()\nplt.show()\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1:47 4s/step - loss: 17.2472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.0169 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.4653\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4230\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.7726\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - loss: 24.8589 - val_loss: 19.2947\n\nEpoch 2/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 25.4398\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.7412 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8654\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.1359\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.4888 - val_loss: 19.3072\n\nEpoch 3/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 17.3205\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.5476 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2563\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n27/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.0241\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.2890 - val_loss: 19.3359\n\nEpoch 4/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 26.2619\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.8113 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.5551\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.2070\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.4390 - val_loss: 19.3168\n\nEpoch 5/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.7453\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.3061 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.3291\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.6632\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9402 - val_loss: 19.3156\n\nEpoch 6/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1s 36ms/step - loss: 27.4544\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.8380 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.3764\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n22/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.3153\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.7576\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 24.8417 - val_loss: 19.3423\n\nEpoch 7/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.0636\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.0314 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6311\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.6199\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 24.9801 - val_loss: 19.3425\n\nEpoch 8/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 26.6717\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.0107 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9826\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.3354\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - loss: 27.7882 - val_loss: 19.3204\n\nEpoch 9/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step - loss: 17.4348\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.3534 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7493\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.3760\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.3997 - val_loss: 19.3414\n\nEpoch 10/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 64.7895\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 40.1225 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.7579\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3330\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 31.1952 - val_loss: 19.3181\n\nEpoch 11/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.7807\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.2694 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.5541\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.5185\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 23.2786 - val_loss: 19.3371\n\nEpoch 12/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 26.2417\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.6123 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9911\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.2342\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.4676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.5067 - val_loss: 19.3530\n\nEpoch 13/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 14.2694\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 19.5592 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.1893\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7190\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1906 - val_loss: 19.3387\n\nEpoch 14/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.0720\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.5893 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2512\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6636\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.1773 - val_loss: 19.3238\n\nEpoch 15/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.9888\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 37.2425 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.1837\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.0728\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 30.9890 - val_loss: 19.3334\n\nEpoch 16/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.8714\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.1570 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1583\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.6509\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.6954 - val_loss: 19.3560\n\nEpoch 17/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 17.2823\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3187 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.2485\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.7034\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.9724 - val_loss: 19.3392\n\nEpoch 18/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.5325\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.3507 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.1400\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.6932\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0090 - val_loss: 19.3441\n\nEpoch 19/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step - loss: 29.0817\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.2049 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 24.9526\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.0471\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.2038 - val_loss: 19.3544\n\nEpoch 20/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 19.1275\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2943 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.7399\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4479\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1221 - val_loss: 19.3550\n\nEpoch 21/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.4252\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.3611 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7263\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.5548\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.5521 - val_loss: 19.3392\n\nEpoch 22/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 36.2122\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 28.3619 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 27.6676\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.9928\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.8096 - val_loss: 19.3558\n\nEpoch 23/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 18.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.2147 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8394\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.4218\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0859 - val_loss: 19.3555\n\nEpoch 24/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 16.6957\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.2797 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.1708\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.5716\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.9684 - val_loss: 19.3485\n\nEpoch 25/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 15.9916\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 19.9071 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.7702\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.6888\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 22.3951 - val_loss: 19.3427\n\nEpoch 26/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 13.1146\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.1562 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.7639\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.8359\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.5517 - val_loss: 19.3407\n\nEpoch 27/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 15.3232\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.5427 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8192\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.7798\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.2430 - val_loss: 19.3456\n\nEpoch 28/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 33.1680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 30.9568 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.7914\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 29.1265\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.5709 - val_loss: 19.3444\n\nEpoch 29/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 28ms/step - loss: 18.5323\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9770 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n15/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.2770\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.2833\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n29/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.7721\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 23.9187 - val_loss: 19.3402\n\nEpoch 30/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 34.0528\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 32.6259 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.1978\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.0654\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 28.5250 - val_loss: 19.3430\n\nEpoch 31/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 20.6591\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.7177 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.2624\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.5131\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.4411 - val_loss: 19.3609\n\nEpoch 32/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 21.5955\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.2292 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.9907\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.6114\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.5282 - val_loss: 19.3686\n\nEpoch 33/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 65.1081\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 44.1042 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 37.8204\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 34.5450\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 32.3578 - val_loss: 19.3728\n\nEpoch 34/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 27.8631\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2318 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7661\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 25.6547\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.7644 - val_loss: 19.3836\n\nEpoch 35/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 19.9032\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.8187 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.2834\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.9913\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4137 - val_loss: 19.3540\n\nEpoch 36/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 11.4795\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 8/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 20.9734 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.0948\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.6574 - val_loss: 19.3902\n\nEpoch 37/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 36.1415\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 29.6248 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9660\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n23/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2228\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 27.7399 - val_loss: 19.4040\n\nEpoch 38/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 17.7340\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 21.5582 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.0746\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.7826\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.0293 - val_loss: 19.4655\n\nEpoch 39/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 56.4279\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 36.8252 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 32.3398\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 30.6403\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 29.7989 - val_loss: 19.3824\n\nEpoch 40/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 16.2450\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.3838 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 26.1215\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 26.4408\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.3853 - val_loss: 19.3980\n\nEpoch 41/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 14.2457\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.9945 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.8925\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 24.9801\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.2418 - val_loss: 19.3694\n\nEpoch 42/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.1206\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.0493 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0413\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.9380\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.4330 - val_loss: 19.3871\n\nEpoch 43/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 25ms/step - loss: 25.7176\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.0048 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.9222\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.6528\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1326 - val_loss: 19.4147\n\nEpoch 44/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 22.8038\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.2068 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n18/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0183\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n26/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.0625\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.5169 - val_loss: 19.3905\n\nEpoch 45/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 21.8415\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 20.9044 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 22.1423\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 23.6610\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.1861 - val_loss: 19.3989\n\nEpoch 46/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 34ms/step - loss: 25.1074\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 7/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 26.6888 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n14/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 26.0508\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n21/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.7555\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n28/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 25.9111\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.9185 - val_loss: 19.4130\n\nEpoch 47/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 33.9264\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7908 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n16/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.7225\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n24/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 25.9671\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 25.9415 - val_loss: 19.4470\n\nEpoch 48/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 19.5680\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 19.9904 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 21.5977\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.3469\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 23.0098 - val_loss: 19.4161\n\nEpoch 49/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step - loss: 17.1747\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8232 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 22.8352\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 23.5322\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 24.0362 - val_loss: 19.4772\n\nEpoch 50/50\n\n\n 1/30 ━━━━━━━━━━━━━━━━━━━━ 0s 27ms/step - loss: 29.8088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.9395 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n17/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 28.2321\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n25/30 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 27.6306\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - loss: 27.3645 - val_loss: 19.4442\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 2s 309ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 35ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 35ms/step\n\nMean Squared Error (MSE): 25.14084539513389\n\nMean Absolute Error (MAE): 3.028007901353614\n\nMean Absolute Error (MAE): -0.008711747844338294\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# scaled LSTM:02# Check the shape of X_train and y_train\nprint(f\"X_train shape before reshaping: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Reshape X_train to be (n_samples, time_steps=1, features=12)\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])  # Reshape to (n_samples, 1, 12)\n\n# Reshape X_test similarly\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n# Normalize the data (optional but recommended)\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Fit scaler on X_train and apply to X_train and X_test\nscaler_X = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler_X.fit_transform(X_train.reshape(-1, X_train.shape[2]))  # Reshape for scaling\nX_test_scaled = scaler_X.transform(X_test.reshape(-1, X_test.shape[2]))  # Apply scaling to X_test\n\n# Reshape back to 3D for LSTM model\nX_train_scaled = X_train_scaled.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])  # (n_samples, 1, 12)\nX_test_scaled = X_test_scaled.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])  # (n_samples, 1, 12)\n\n# You can also scale the output if necessary\nscaler_y = MinMaxScaler(feature_range=(0, 1))\ny_train_scaled = scaler_y.fit_transform(y_train)  # Scaling y_train (6 companies' prices)\ny_test_scaled = scaler_y.transform(y_test)  # Scaling y_test (6 companies' prices)\n\n# Check the reshaped data\nprint(f\"X_train_scaled shape: {X_train_scaled.shape}\")\nprint(f\"X_test_scaled shape: {X_test_scaled.shape}\")\nprint(f\"y_train_scaled shape: {y_train_scaled.shape}\")\nprint(f\"y_test_scaled shape: {y_test_scaled.shape}\")\n\n\nX_train shape before reshaping: (1192, 12)\ny_train shape: (1192, 6)\nX_train_scaled shape: (1192, 1, 12)\nX_test_scaled shape: (299, 1, 12)\ny_train_scaled shape: (1192, 6)\ny_test_scaled shape: (299, 6)\n\n\n\n\nShow the code\n# Define model architecture\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\nmodel = Sequential(\n    [\n        LSTM(\n            64,\n            return_sequences=True,\n            input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]),\n        ),\n        Dropout(0.2),  # Dropout for regularization\n        LSTM(64),\n        Dropout(0.2),\n        Dense(16, activation=\"relu\"),\n        Dense(6),  # Output layer with 6 predictions (one per company)\n    ]\n)\n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss=\"mse\")\n\n# Fit the model\nmodel.fit(\n    X_train_scaled,\n    y_train_scaled,\n    epochs=50,\n    batch_size=64,\n    validation_data=(X_test_scaled, y_test_scaled),\n)\n\n# Predict on X_test_scaled\npredictions = model.predict(X_test_scaled)\n\n# Inverse transform the predictions and actual values to get the original scale\npredictions = scaler_y.inverse_transform(predictions)\ny_test_actual = scaler_y.inverse_transform(y_test_scaled)\n\n# Evaluate the model's performance\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmse = mean_squared_error(y_test_actual, predictions)\nmae = mean_absolute_error(y_test_actual, predictions)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\n\nEpoch 1/50\n\n\nC:\\Users\\Rush\\AppData\\Roaming\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 41s 2s/step - loss: 0.2472\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - loss: 0.2285 - val_loss: 0.1442\n\nEpoch 2/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.1459\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.1125 - val_loss: 0.0520\n\nEpoch 3/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0606\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0466 - val_loss: 0.0225\n\nEpoch 4/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0239\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0230 - val_loss: 0.0164\n\nEpoch 5/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0142\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0168 - val_loss: 0.0131\n\nEpoch 6/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0173\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0147 - val_loss: 0.0110\n\nEpoch 7/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0126\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0119 - val_loss: 0.0096\n\nEpoch 8/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0097\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0100 - val_loss: 0.0086\n\nEpoch 9/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0091\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0091 - val_loss: 0.0079\n\nEpoch 10/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0103\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0088 - val_loss: 0.0075\n\nEpoch 11/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0085 - val_loss: 0.0073\n\nEpoch 12/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0102\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0084 - val_loss: 0.0072\n\nEpoch 13/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0087\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0080 - val_loss: 0.0071\n\nEpoch 14/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0089\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0081 - val_loss: 0.0073\n\nEpoch 15/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0080 - val_loss: 0.0071\n\nEpoch 16/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0054\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0071\n\nEpoch 17/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0086\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0079 - val_loss: 0.0070\n\nEpoch 18/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0076\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 19/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0056\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 20/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n 9/19 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - loss: 0.0076 \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 21/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0072\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 22/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0065\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 23/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step - loss: 0.0067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 24/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 25/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0074\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 26/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 27/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0069\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0073 - val_loss: 0.0070\n\nEpoch 28/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0058\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 29/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0075\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0076 - val_loss: 0.0070\n\nEpoch 30/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0092\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0077 - val_loss: 0.0070\n\nEpoch 31/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0054\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0069 - val_loss: 0.0070\n\nEpoch 32/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step - loss: 0.0082\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 33/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0059\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 34/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0084\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 35/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0067\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\nEpoch 36/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 37/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0056\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0069 - val_loss: 0.0070\n\nEpoch 38/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0069\n\nEpoch 39/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0069\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 40/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 24ms/step - loss: 0.0085\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 41/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0088\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0074 - val_loss: 0.0070\n\nEpoch 42/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 43/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0078\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\nEpoch 44/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0051\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0068 - val_loss: 0.0070\n\nEpoch 45/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0084\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0071 - val_loss: 0.0070\n\nEpoch 46/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0077\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0070\n\nEpoch 47/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.0079\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0068 - val_loss: 0.0070\n\nEpoch 48/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0081\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0072 - val_loss: 0.0069\n\nEpoch 49/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 20ms/step - loss: 0.0071\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0075 - val_loss: 0.0070\n\nEpoch 50/50\n\n\n 1/19 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step - loss: 0.0080\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n19/19 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step - loss: 0.0070 - val_loss: 0.0070\n\n\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 2s 271ms/step\n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 29ms/step \n\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n10/10 ━━━━━━━━━━━━━━━━━━━━ 1s 29ms/step\n\nMean Squared Error (MSE): 24.98155140126504\n\nMean Absolute Error (MAE): 3.0311981552411438\n\n\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Plot Actual vs Predicted values for each company\ncompanies = ['Apple', 'Amazon', 'Google', 'Meta', 'Microsoft', 'Tesla']\ncolors = ['blue', 'green', 'red']\n\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    plt.subplot(3, 2, i+1)\n    plt.plot(y_test_actual[:, i], label=f\"Actual {companies[i]}\", color='green', linestyle='-', linewidth=2)\n    plt.plot(predictions[:, i], label=f\"Predicted {companies[i]}\", color='blue', linestyle='--', linewidth=2)\n    plt.title(f\"Actual vs Predicted for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Stock Price\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Plot Error for each company\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    error = y_test_actual[:, i] - predictions[:, i]\n    plt.subplot(3, 2, i+1)\n    plt.plot(error, label=f\"Error {companies[i]}\", color='red')\n    plt.title(f\"Prediction Error for {companies[i]}\")\n    plt.xlabel(\"Time Steps\")\n    plt.ylabel(\"Error (Actual - Predicted)\")\n    plt.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#evaluate-and-enhance-portfolio-strategies",
    "href": "posts/4-2024-17-07-00/post_1.html#evaluate-and-enhance-portfolio-strategies",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "-Stress Testing: Simulate extreme market conditions (e.g., 2008 financial crisis, COVID-19 crash) to see how the portfolio performs under stress.\n\n\nShow the code\n# Stress testing:\n\ndef stress_test(data, weights):\n    stress_returns = data.pct_change().apply(lambda x: x + np.random.normal(0, 0.1), axis=0)\n    portfolio_stress_returns = (stress_returns * pd.Series(weights)).sum(axis=1)\n    return portfolio_stress_returns\n\nstress_returns = stress_test(data, new_weights)\nprint(\"Portfolio returns under stress:\", stress_returns.describe())\n\n\nPortfolio returns under stress: count    1510.000000\nmean       -0.082370\nstd         0.022840\nmin        -0.256790\n25%        -0.092922\n50%        -0.081795\n75%        -0.070957\nmax         0.078696\ndtype: float64"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#alternative-optimization-methods",
    "href": "posts/4-2024-17-07-00/post_1.html#alternative-optimization-methods",
    "title": "portfolio optimization using WCA",
    "section": "",
    "text": "-Risk Parity Portfolio: Balance risks across assets instead of focusing on returns. -Black-Litterman Model: Blend market equilibrium with subjective views."
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#performance-check-after-risk-parity-approach",
    "href": "posts/4-2024-17-07-00/post_1.html#performance-check-after-risk-parity-approach",
    "title": "portfolio optimization using WCA",
    "section": "Performance check after risk parity approach:",
    "text": "Performance check after risk parity approach:\n\n\nShow the code\n# Perf check risk parity\n\nportfolio_returns_rp = (returns * risk_parity_weights).sum(axis=1)\ncumulative_rp = (1 + portfolio_returns_rp).cumprod()\n\nplt.plot(cumulative_rp, label=\"Risk Parity Portfolio\")\nplt.legend()\nplt.title(\"Cumulative Returns - Risk Parity\")\nplt.show()\n\n\n\n\n\nA line plot on a polar axis"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#black-litterman-model",
    "href": "posts/4-2024-17-07-00/post_1.html#black-litterman-model",
    "title": "portfolio optimization using WCA",
    "section": "Black-Litterman Model",
    "text": "Black-Litterman Model\nThe Black-Litterman model combines market equilibrium (via market cap weights) with subjective views on assets.\n\n\nShow the code\n# Black-Litterman Model\n\nfrom pypfopt import BlackLittermanModel\n\n# Market cap weights (assuming proportional to market cap)\nmarket_caps = np.array([2.8, 2.2, 1.9, 1.8, 1.3, 0.8])  # example market caps in trillions\nmarket_cap_weights = market_caps / market_caps.sum()\n\n# Define subjective views\nviews = {\n    \"AAPL\": 0.02,  # Expected 2% return\n    \"TSLA\": 0.04,  # Expected 4% return\n}\n\n# Black-Litterman model\nbl = BlackLittermanModel(cov_matrix, pi=market_cap_weights, absolute_views=views)\nbl_weights = bl.optimize()\n\nprint(\"Black-Litterman Weights:\", bl_weights)\n\n\nBlack-Litterman Weights: OrderedDict([('AAPL', 1.7699073524978122), ('AMZN', 0.8960410721351605), ('GOOGL', 0.6998671206181221), ('META', -0.27160895459209766), ('MSFT', -1.7597132250785943), ('TSLA', -0.3344933655804028)])"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#performance-check",
    "href": "posts/4-2024-17-07-00/post_1.html#performance-check",
    "title": "portfolio optimization using WCA",
    "section": "Performance Check",
    "text": "Performance Check\n\n\nShow the code\n# Perf check Black-Litterman Model\n\nportfolio_returns_bl = (returns * pd.Series(bl_weights)).sum(axis=1)\ncumulative_bl = (1 + portfolio_returns_bl).cumprod()\n\nplt.plot(cumulative_bl, label=\"Black-Litterman Portfolio\")\nplt.legend()\nplt.title(\"Cumulative Returns - Black-Litterman\")\nplt.show()\n\n\n\n\n\nA line plot on a polar axis"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#integrating-both-methods",
    "href": "posts/4-2024-17-07-00/post_1.html#integrating-both-methods",
    "title": "portfolio optimization using WCA",
    "section": "Integrating both methods",
    "text": "Integrating both methods\n\n\nShow the code\n# integrating both methods:\n\ndef calculate_metrics(returns):\n    sharpe_ratio = returns.mean() / returns.std()\n    max_drawdown = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n    return {\"Sharpe Ratio\": sharpe_ratio, \"Max Drawdown\": max_drawdown}\n\nprint(\"Risk Parity Metrics:\", calculate_metrics(portfolio_returns_rp))\nprint(\"Black-Litterman Metrics:\", calculate_metrics(portfolio_returns_bl))\n# print(\"ESG Metrics:\", calculate_metrics(portfolio_returns_esg))\n\n\nRisk Parity Metrics: {'Sharpe Ratio': 0.07042245244253975, 'Max Drawdown': -0.42126756100966034}\nBlack-Litterman Metrics: {'Sharpe Ratio': 0.018049273107970144, 'Max Drawdown': -0.42126756100966034}"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#alternative-data",
    "href": "posts/4-2024-17-07-00/post_1.html#alternative-data",
    "title": "portfolio optimization using WCA",
    "section": "Alternative Data:",
    "text": "Alternative Data:\nUse sentiment analysis from news, social media, or earnings call transcripts. Incorporate macroeconomic indicators (GDP growth, interest rates). Add fundamental data like P/E ratios, earnings growth, or dividends. ## Data Preprocessing: Use NLP techniques to analyze textual data for sentiment signals. Implement feature selection to prioritize impactful predictors."
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#performance-monitoring",
    "href": "posts/4-2024-17-07-00/post_1.html#performance-monitoring",
    "title": "portfolio optimization using WCA",
    "section": "Performance Monitoring:",
    "text": "Performance Monitoring:\nTrack portfolio metrics (e.g., Sharpe ratio, alpha, drawdown) over time. Create performance benchmarks against indices like S&P 500 or NASDAQ. ## Explainability: Use explainable AI techniques (e.g., SHAP, LIME) to interpret model predictions. Identify which features most influence asset selection and rebalancing decisions."
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#deep-learning",
    "href": "posts/4-2024-17-07-00/post_1.html#deep-learning",
    "title": "portfolio optimization using WCA",
    "section": "Deep Learning:",
    "text": "Deep Learning:\nTrain LSTMs, GRUs, or Transformers for better time-series predictions. Incorporate reinforcement learning to optimize rebalancing decisions. ## Optimization: Use genetic algorithms or Bayesian optimization to explore non-linear, multi-objective portfolio strategies.\n\n\nShow the code\n# Bayesian optimization:\nfrom geneticalgorithm import geneticalgorithm as ga\n\nassert list(mu.index) == list(S.columns), \"The assets in `mu` and `S` must match!\"\n\ndef fitness(weights):\n    weights = np.array(weights)  # Ensure weights are in array form\n    weights = weights / weights.sum()  # Normalize weights to sum to 1\n    portfolio_return = np.dot(weights, mu.values)  # Use `.values` to extract NumPy array\n    portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(S.values, weights)))  # Use `.values`\n    return -portfolio_return / portfolio_risk  # Maximize Sharpe ratio\n\nvariable_boundaries = np.array([[0, 1]] * len(mu))  # Each weight must be between 0 and 1\nalgorithm = ga(function=fitness, dimension=len(mu), variable_type='real', variable_boundaries=variable_boundaries)\nalgorithm.run()\n# Extract the results\noptimized_weights = algorithm.output_dict['variable']\noptimized_weights = np.array(optimized_weights) / sum(optimized_weights)  # Normalize weights\noptimized_weights_series = pd.Series(optimized_weights, index=mu.index)\nprint(\"Optimized Weights:\\n\", optimized_weights_series)\nprint(\"Sum of Weights:\", optimized_weights_series.sum())\n\n\n__________________________________________________ 0.3% GA is running...__________________________________________________ 0.7% GA is running...__________________________________________________ 1.0% GA is running...|_________________________________________________ 1.3% GA is running...|_________________________________________________ 1.7% GA is running...|_________________________________________________ 2.0% GA is running...|_________________________________________________ 2.3% GA is running...|_________________________________________________ 2.7% GA is running...||________________________________________________ 3.0% GA is running...||________________________________________________ 3.3% GA is running...||________________________________________________ 3.7% GA is running...||________________________________________________ 4.0% GA is running...||________________________________________________ 4.3% GA is running...||________________________________________________ 4.7% GA is running...||________________________________________________ 5.0% GA is running...|||_______________________________________________ 5.3% GA is running...|||_______________________________________________ 5.7% GA is running...|||_______________________________________________ 6.0% GA is running...|||_______________________________________________ 6.3% GA is running...|||_______________________________________________ 6.7% GA is running...||||______________________________________________ 7.0% GA is running...||||______________________________________________ 7.3% GA is running...||||______________________________________________ 7.7% GA is running...||||______________________________________________ 8.0% GA is running...||||______________________________________________ 8.3% GA is running...||||______________________________________________ 8.7% GA is running...||||______________________________________________ 9.0% GA is running...|||||_____________________________________________ 9.3% GA is running...|||||_____________________________________________ 9.7% GA is running...|||||_____________________________________________ 10.0% GA is running...|||||_____________________________________________ 10.3% GA is running...|||||_____________________________________________ 10.7% GA is running...||||||____________________________________________ 11.0% GA is running...||||||____________________________________________ 11.3% GA is running...||||||____________________________________________ 11.7% GA is running...||||||____________________________________________ 12.0% GA is running...||||||____________________________________________ 12.3% GA is running...||||||____________________________________________ 12.7% GA is running...||||||____________________________________________ 13.0% GA is running...|||||||___________________________________________ 13.3% GA is running...|||||||___________________________________________ 13.7% GA is running...|||||||___________________________________________ 14.0% GA is running...|||||||___________________________________________ 14.3% GA is running...|||||||___________________________________________ 14.7% GA is running...||||||||__________________________________________ 15.0% GA is running...||||||||__________________________________________ 15.3% GA is running...||||||||__________________________________________ 15.7% GA is running...||||||||__________________________________________ 16.0% GA is running...||||||||__________________________________________ 16.3% GA is running...||||||||__________________________________________ 16.7% GA is running...||||||||__________________________________________ 17.0% GA is running...|||||||||_________________________________________ 17.3% GA is running...|||||||||_________________________________________ 17.7% GA is running...|||||||||_________________________________________ 18.0% GA is running...|||||||||_________________________________________ 18.3% GA is running...|||||||||_________________________________________ 18.7% GA is running...||||||||||________________________________________ 19.0% GA is running...||||||||||________________________________________ 19.3% GA is running...||||||||||________________________________________ 19.7% GA is running...||||||||||________________________________________ 20.0% GA is running...||||||||||________________________________________ 20.3% GA is running...||||||||||________________________________________ 20.7% GA is running...||||||||||________________________________________ 21.0% GA is running...|||||||||||_______________________________________ 21.3% GA is running...|||||||||||_______________________________________ 21.7% GA is running...|||||||||||_______________________________________ 22.0% GA is running...|||||||||||_______________________________________ 22.3% GA is running...|||||||||||_______________________________________ 22.7% GA is running...||||||||||||______________________________________ 23.0% GA is running...||||||||||||______________________________________ 23.3% GA is running...||||||||||||______________________________________ 23.7% GA is running...||||||||||||______________________________________ 24.0% GA is running...||||||||||||______________________________________ 24.3% GA is running...||||||||||||______________________________________ 24.7% GA is running...||||||||||||______________________________________ 25.0% GA is running...|||||||||||||_____________________________________ 25.3% GA is running...|||||||||||||_____________________________________ 25.7% GA is running...|||||||||||||_____________________________________ 26.0% GA is running...|||||||||||||_____________________________________ 26.3% GA is running...|||||||||||||_____________________________________ 26.7% GA is running...||||||||||||||____________________________________ 27.0% GA is running...||||||||||||||____________________________________ 27.3% GA is running...||||||||||||||____________________________________ 27.7% GA is running...||||||||||||||____________________________________ 28.0% GA is running...||||||||||||||____________________________________ 28.3% GA is running...||||||||||||||____________________________________ 28.7% GA is running...||||||||||||||____________________________________ 29.0% GA is running...|||||||||||||||___________________________________ 29.3% GA is running...|||||||||||||||___________________________________ 29.7% GA is running...|||||||||||||||___________________________________ 30.0% GA is running...|||||||||||||||___________________________________ 30.3% GA is running...|||||||||||||||___________________________________ 30.7% GA is running...||||||||||||||||__________________________________ 31.0% GA is running...||||||||||||||||__________________________________ 31.3% GA is running...||||||||||||||||__________________________________ 31.7% GA is running...||||||||||||||||__________________________________ 32.0% GA is running...||||||||||||||||__________________________________ 32.3% GA is running...||||||||||||||||__________________________________ 32.7% GA is running...||||||||||||||||__________________________________ 33.0% GA is running...|||||||||||||||||_________________________________ 33.3% GA is running...|||||||||||||||||_________________________________ 33.7% GA is running...|||||||||||||||||_________________________________ 34.0% GA is running...|||||||||||||||||_________________________________ 34.3% GA is running...|||||||||||||||||_________________________________ 34.7% GA is running...||||||||||||||||||________________________________ 35.0% GA is running...||||||||||||||||||________________________________ 35.3% GA is running...||||||||||||||||||________________________________ 35.7% GA is running...||||||||||||||||||________________________________ 36.0% GA is running...||||||||||||||||||________________________________ 36.3% GA is running...||||||||||||||||||________________________________ 36.7% GA is running...||||||||||||||||||________________________________ 37.0% GA is running...|||||||||||||||||||_______________________________ 37.3% GA is running...|||||||||||||||||||_______________________________ 37.7% GA is running...|||||||||||||||||||_______________________________ 38.0% GA is running...|||||||||||||||||||_______________________________ 38.3% GA is running...|||||||||||||||||||_______________________________ 38.7% GA is running...||||||||||||||||||||______________________________ 39.0% GA is running...||||||||||||||||||||______________________________ 39.3% GA is running...||||||||||||||||||||______________________________ 39.7% GA is running...||||||||||||||||||||______________________________ 40.0% GA is running...||||||||||||||||||||______________________________ 40.3% GA is running...||||||||||||||||||||______________________________ 40.7% GA is running...||||||||||||||||||||______________________________ 41.0% GA is running...|||||||||||||||||||||_____________________________ 41.3% GA is running...|||||||||||||||||||||_____________________________ 41.7% GA is running...|||||||||||||||||||||_____________________________ 42.0% GA is running...|||||||||||||||||||||_____________________________ 42.3% GA is running...|||||||||||||||||||||_____________________________ 42.7% GA is running...||||||||||||||||||||||____________________________ 43.0% GA is running...||||||||||||||||||||||____________________________ 43.3% GA is running...||||||||||||||||||||||____________________________ 43.7% GA is running...||||||||||||||||||||||____________________________ 44.0% GA is running...||||||||||||||||||||||____________________________ 44.3% GA is running...||||||||||||||||||||||____________________________ 44.7% GA is running...||||||||||||||||||||||____________________________ 45.0% GA is running...|||||||||||||||||||||||___________________________ 45.3% GA is running...|||||||||||||||||||||||___________________________ 45.7% GA is running...|||||||||||||||||||||||___________________________ 46.0% GA is running...|||||||||||||||||||||||___________________________ 46.3% GA is running...|||||||||||||||||||||||___________________________ 46.7% GA is running...||||||||||||||||||||||||__________________________ 47.0% GA is running...||||||||||||||||||||||||__________________________ 47.3% GA is running...||||||||||||||||||||||||__________________________ 47.7% GA is running...||||||||||||||||||||||||__________________________ 48.0% GA is running...||||||||||||||||||||||||__________________________ 48.3% GA is running...||||||||||||||||||||||||__________________________ 48.7% GA is running...||||||||||||||||||||||||__________________________ 49.0% GA is running...|||||||||||||||||||||||||_________________________ 49.3% GA is running...|||||||||||||||||||||||||_________________________ 49.7% GA is running...|||||||||||||||||||||||||_________________________ 50.0% GA is running...|||||||||||||||||||||||||_________________________ 50.3% GA is running...|||||||||||||||||||||||||_________________________ 50.7% GA is running...||||||||||||||||||||||||||________________________ 51.0% GA is running...||||||||||||||||||||||||||________________________ 51.3% GA is running...||||||||||||||||||||||||||________________________ 51.7% GA is running...||||||||||||||||||||||||||________________________ 52.0% GA is running...||||||||||||||||||||||||||________________________ 52.3% GA is running...||||||||||||||||||||||||||________________________ 52.7% GA is running...||||||||||||||||||||||||||________________________ 53.0% GA is running...|||||||||||||||||||||||||||_______________________ 53.3% GA is running...|||||||||||||||||||||||||||_______________________ 53.7% GA is running...|||||||||||||||||||||||||||_______________________ 54.0% GA is running...|||||||||||||||||||||||||||_______________________ 54.3% GA is running...|||||||||||||||||||||||||||_______________________ 54.7% GA is running...||||||||||||||||||||||||||||______________________ 55.0% GA is running...||||||||||||||||||||||||||||______________________ 55.3% GA is running...||||||||||||||||||||||||||||______________________ 55.7% GA is running...||||||||||||||||||||||||||||______________________ 56.0% GA is running...||||||||||||||||||||||||||||______________________ 56.3% GA is running...||||||||||||||||||||||||||||______________________ 56.7% GA is running...||||||||||||||||||||||||||||______________________ 57.0% GA is running...|||||||||||||||||||||||||||||_____________________ 57.3% GA is running...|||||||||||||||||||||||||||||_____________________ 57.7% GA is running...|||||||||||||||||||||||||||||_____________________ 58.0% GA is running...|||||||||||||||||||||||||||||_____________________ 58.3% GA is running...|||||||||||||||||||||||||||||_____________________ 58.7% GA is running...||||||||||||||||||||||||||||||____________________ 59.0% GA is running...||||||||||||||||||||||||||||||____________________ 59.3% GA is running...||||||||||||||||||||||||||||||____________________ 59.7% GA is running...||||||||||||||||||||||||||||||____________________ 60.0% GA is running...||||||||||||||||||||||||||||||____________________ 60.3% GA is running...||||||||||||||||||||||||||||||____________________ 60.7% GA is running...||||||||||||||||||||||||||||||____________________ 61.0% GA is running...|||||||||||||||||||||||||||||||___________________ 61.3% GA is running...|||||||||||||||||||||||||||||||___________________ 61.7% GA is running...|||||||||||||||||||||||||||||||___________________ 62.0% GA is running...|||||||||||||||||||||||||||||||___________________ 62.3% GA is running...|||||||||||||||||||||||||||||||___________________ 62.7% GA is running...||||||||||||||||||||||||||||||||__________________ 63.0% GA is running...||||||||||||||||||||||||||||||||__________________ 63.3% GA is running...||||||||||||||||||||||||||||||||__________________ 63.7% GA is running...||||||||||||||||||||||||||||||||__________________ 64.0% GA is running...||||||||||||||||||||||||||||||||__________________ 64.3% GA is running...||||||||||||||||||||||||||||||||__________________ 64.7% GA is running...||||||||||||||||||||||||||||||||__________________ 65.0% GA is running...|||||||||||||||||||||||||||||||||_________________ 65.3% GA is running...|||||||||||||||||||||||||||||||||_________________ 65.7% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.0% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.3% GA is running...|||||||||||||||||||||||||||||||||_________________ 66.7% GA is running...||||||||||||||||||||||||||||||||||________________ 67.0% GA is running...||||||||||||||||||||||||||||||||||________________ 67.3% GA is running...||||||||||||||||||||||||||||||||||________________ 67.7% GA is running...||||||||||||||||||||||||||||||||||________________ 68.0% GA is running...||||||||||||||||||||||||||||||||||________________ 68.3% GA is running...||||||||||||||||||||||||||||||||||________________ 68.7% GA is running...||||||||||||||||||||||||||||||||||________________ 69.0% GA is running...|||||||||||||||||||||||||||||||||||_______________ 69.3% GA is running...|||||||||||||||||||||||||||||||||||_______________ 69.7% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.0% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.3% GA is running...|||||||||||||||||||||||||||||||||||_______________ 70.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.0% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.3% GA is running...||||||||||||||||||||||||||||||||||||______________ 71.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.0% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.3% GA is running...||||||||||||||||||||||||||||||||||||______________ 72.7% GA is running...||||||||||||||||||||||||||||||||||||______________ 73.0% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 73.3% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 73.7% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.0% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.3% GA is running...|||||||||||||||||||||||||||||||||||||_____________ 74.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.0% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.3% GA is running...||||||||||||||||||||||||||||||||||||||____________ 75.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.0% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.3% GA is running...||||||||||||||||||||||||||||||||||||||____________ 76.7% GA is running...||||||||||||||||||||||||||||||||||||||____________ 77.0% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 77.3% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 77.7% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.0% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.3% GA is running...|||||||||||||||||||||||||||||||||||||||___________ 78.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.0% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.3% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 79.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.0% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.3% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 80.7% GA is running...||||||||||||||||||||||||||||||||||||||||__________ 81.0% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 81.3% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 81.7% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.0% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.3% GA is running...|||||||||||||||||||||||||||||||||||||||||_________ 82.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.0% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.3% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 83.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.0% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.3% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 84.7% GA is running...||||||||||||||||||||||||||||||||||||||||||________ 85.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 85.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 85.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||_______ 86.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 87.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 88.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||______ 89.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 89.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 89.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||_____ 90.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 91.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 92.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||____ 93.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 93.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 93.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||___ 94.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 95.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.0% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.3% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 96.7% GA is running...||||||||||||||||||||||||||||||||||||||||||||||||__ 97.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 97.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 97.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||_ 98.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.0% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.3% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 99.7% GA is running...|||||||||||||||||||||||||||||||||||||||||||||||||| 100.0% GA is running...                                                                                                     The best solution found:\n [0.99838705 0.01187851 0.00576266 0.02412417 0.39956896 0.40654724]\n\n Objective function:\n -1.173701232939604\n\n\n\n\n\n\n\n\n\nOptimized Weights:\n Ticker\nAAPL     0.540759\nAMZN     0.006434\nGOOGL    0.003121\nMETA     0.013066\nMSFT     0.216420\nTSLA     0.220199\ndtype: float64\nSum of Weights: 0.9999999999999998"
  },
  {
    "objectID": "posts/4-2024-17-07-00/post_1.html#equation",
    "href": "posts/4-2024-17-07-00/post_1.html#equation",
    "title": "portfolio optimization using WCA",
    "section": "Equation",
    "text": "Equation\nUse LaTeX to write equations:\n\\[\n\\chi' = \\sum_{i=1}^n k_i s_i^2\n\\]"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\nportfolio optimization using WCA\n\n\n\n\n\n\n\n16 min\n\n\n\n\n\n\nRAG-Enhanced Multimodal Risk Intelligence for Real-Time Banking Decisions\n\n\n\nMachine Learning\n\nGenerative AI\n\nBanking Risk\n\nRAG\n\nLLM\n\n\n\nRetrieval-Augmented Generation (RAG)-enhanced hybrid model that integrates structured financial…\n\n\n\n\n\nAug 13, 2025\n\n5 min\n\n\n\n\n\n\nFinancial Analysis of a Portfolio Trading Strategy: A Tryout Version\n\n\n\nFinance\n\nPortfolio\n\nMachine Learning\n\n\n\nExploration of systematic trading strategy to achieve higher risk-adjusted returns\n\n\n\n\n\nNov 26, 2024\n\n17 min\n\n\n\n\n\n\nRiskRAG: Real-Time Credit Risk Assessment\n\n\n\nGenerative AI\n\nNLP\n\nFinance\n\nRisk Modeling\n\nRAG\n\n\n\nFocused on exploring credit risk evaluation with trial and error with RAG and Multimodal Learning\n\n\n\n\n\nAug 13, 2024\n\n7 min\n\n\n\n\n\n\nportfolio optimization using WCA\n\n\n\nBacktesting\n\nrisk\n\nMachine Learning\n\nPortfolio Optimization\n\nRAG\n\n\n\ndemonstrates portfolio optimization with Machine Learning and Risk management\n\n\n\n\n\nJul 17, 2024\n\n15 min\n\n\n\n\n\n\nFinancial Analysis of a Portfolio Trading Strategy: A Tryout Version\n\n\n\nBacktesting\n\nReturns\n\nPortfolio Optimization\n\n\n\nAn initial exploration of a systematic trading strategy tailored to portfolio performance\n\n\n\n\n\nJul 17, 2024\n\n15 min\n\n\n\n\nNo matching items\n\n…"
  }
]